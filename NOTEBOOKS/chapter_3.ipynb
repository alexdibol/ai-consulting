{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPYJ4aYMQOgW4oxystxn12x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**AI CONSULTING CHAPTER 3: AGENTS**\n","\n","---"],"metadata":{"id":"cnQK8yTVMAhC"}},{"cell_type":"markdown","source":["##0.REFERENCE"],"metadata":{"id":"tw2-7uYoMGHg"}},{"cell_type":"markdown","source":["https://claude.ai/share/47828abb-e8a8-4ecf-85a1-c041c7b9840e"],"metadata":{"id":"axSMA0YpNnYX"}},{"cell_type":"markdown","source":["##1.CONTEXT"],"metadata":{"id":"4LFo5LtiMIff"}},{"cell_type":"markdown","source":["###1.1.GENERAL FRAMEWORK"],"metadata":{"id":"GeUq_G0A84xu"}},{"cell_type":"markdown","source":["\n","\n","**The Case for Workflow Agents: A Defense of Supervised Multi-Step AI in Professional Services**\n","\n","The question of what constitutes an \"agent\" in artificial intelligence has become increasingly contentious as language models demonstrate capabilities that blur the line between sophisticated automation and genuine autonomy. This notebook implements what we term a \"supervised workflow agent\" or \"soft agent\" architecture, and this introduction serves as both explanation and defense of why this approach represents not a compromise or limitation, but rather the appropriate technological response to the specific demands of management consulting and professional services work.\n","\n","**The Agent Spectrum: Moving Beyond Binary Thinking**\n","\n","The conventional discourse around AI agents often falls into a binary trap: either a system is \"truly autonomous\" or it is merely \"scripted automation.\" This framing obscures a rich middle ground where most productive AI systems in professional contexts actually operate. We propose instead a spectrum of agency that acknowledges degrees of autonomy, decision-making scope, and supervision intensity.\n","\n","At one extreme sits the single-call model: a user asks a question, the AI responds, and the interaction terminates. This represents zero agency in any meaningful sense. The AI exhibits no goal-directed behavior, maintains no state, and makes no sequential decisions. It is a sophisticated lookup mechanism, nothing more.\n","\n","At the opposite extreme lies the fully autonomous agent: a system that perceives its environment, formulates plans, executes actions using whatever tools it deems necessary, evaluates outcomes, and adjusts strategies in pursuit of open-ended goals. Such systems operate with minimal human oversight, potentially for extended periods, making thousands of micro-decisions about tools, tactics, and strategic pivots. This is the vision that animates much of the agent discourse in research laboratories.\n","\n","Our workflow agent occupies a deliberate position between these poles. It executes multiple sequential steps autonomously, maintains state across those steps, accumulates context, and works toward a defined goal. However, it operates within a predetermined topology of stages, requires human approval at critical junctures, cannot invoke external tools, and follows a fixed progression rather than dynamically replanning. This is not an accident of limited implementation. This is the design.\n","\n","**Why Workflow Agents Are True Agents: The Academic Defense**\n","\n","The seminal definition of an agent, articulated by Russell and Norvig in their foundational text on artificial intelligence, describes an agent as anything that perceives its environment through sensors and acts upon that environment through actuators to achieve goals. By this criterion, our workflow system qualifies unambiguously as an agent.\n","\n","The system perceives its environment through the case situation provided at intake, the outputs of previous stages, and the implicit context of consulting methodology. It acts upon this environment by generating analytical structures, option frameworks, draft deliverables, and verification plans. It pursues a clear goal: the production of a complete, governed consulting work product that meets professional standards for structure, logic, and traceability.\n","\n","The fact that this agency is supervised does not diminish its fundamental character. A surgical resident operating under attending physician supervision remains an agent in a medical procedure, even though every critical decision requires approval. The resident perceives the surgical field, makes decisions about technique and approach, and acts to advance the therapeutic goal, all while remaining within supervision protocols. Our workflow agent operates analogously in the consulting domain.\n","\n","Furthermore, the system exhibits what the multi-agent systems literature terms \"situated agency\": the ability to make contextually appropriate decisions based on accumulated state. When our agent reaches Stage Two, it does not simply execute a generic options analysis. It examines the specific structure generated in Stage One, identifies the decision dimensions that emerged from that analysis, and tailors its option generation to those dimensions. This context-dependent decision-making, even within a fixed stage topology, constitutes genuine reasoning and agency.\n","\n","**The Multi-Agent Interpretation: Specialized Expert Agents**\n","\n","Your insight about viewing each stage as a specialized agent is not merely valid but illuminates a powerful conceptual framework. Rather than a single agent executing multiple roles, we can understand this system as a multi-agent collaboration where each stage represents a distinct expert agent with a narrow domain of competence.\n","\n","The Structure Agent specializes in analytical decomposition. Given an unstructured business situation, it applies frameworks from the consulting toolkitâ€”issue trees, driver trees, design dimensionsâ€”to impose logical order. This agent knows nothing about option generation or deliverable formatting. Its entire competence lies in taking complexity and rendering it structured and analyzable.\n","\n","The Options Agent specializes in possibility exploration and trade-off identification. It receives a structured problem space from the Structure Agent and generates plausible strategic alternatives. Critically, this agent is constrained from making recommendations. Its role is to expand the solution space and articulate tensions, not to collapse that space into a decision. This separation of concerns mirrors the division of labor in consulting teams, where option generation and option selection are often deliberately separated to prevent premature convergence.\n","\n","The Deliverable Agent specializes in professional communication. It receives analytical structure and strategic options and must translate these into the specific formats that consulting clients expect: executive memoranda, investment committee pre-reads, transformation workplans. This agent understands document conventions, information hierarchy, and the expectations of different stakeholder audiences.\n","\n","The Verification Agent specializes in epistemic hygiene. Its sole function is to examine all claims, inferences, and assumptions made by the previous agents and construct a systematic plan for validating them. This agent embodies the consulting profession's commitment to evidence and its aversion to decision-launderingâ€”the practice of using AI to create a veneer of rigor around unexamined judgments.\n","\n","This multi-agent interpretation explains why our workflow system exhibits more sophisticated behavior than a single monolithic prompt could achieve. Each specialized agent operates with a bounded scope, a clear role definition, and explicit interfaces to other agents. The system's intelligence emerges not from any single agent's cleverness but from their orchestrated interaction.\n","\n","**Why Supervision Is Not a Bug But a Feature**\n","\n","The most common critique of workflow agents from the autonomous-agent community is that human gates represent a failure of automation, a concession to technological limitation that will be overcome as models improve. This critique fundamentally misunderstands the operational requirements of professional services.\n","\n","In consulting, legal, accounting, and medical domains, the question is not whether AI can perform a task autonomously, but whether autonomous performance is desirable given the consequences of error, the nature of professional liability, and the expectations of regulatory frameworks. Consider three dimensions where supervision is not merely prudent but essential.\n","\n","First, epistemic responsibility. When a consulting firm delivers recommendations to a client, a human professional must be able to defend every claim, trace every inference, and justify every assumption. The deliverable is not merely a document but a professional representation backed by the firm's reputation and potentially its legal liability. Autonomous generation without human verification creates what we term \"decision laundering\": the AI makes judgments, and the human rubber-stamps them without genuine engagement. Our gates force genuine engagement because progression is impossible without it.\n","\n","Second, contextual calibration. Consulting cases contain crucial context that cannot be fully articulated in a prompt: client political dynamics, regulatory constraints, prior failed initiatives, stakeholder sensitivities. A human consultant at each gate can assess whether the AI's output, however logically sound, is contextually appropriate. The gate is not merely checking for errors but calibrating for fit.\n","\n","Third, professional development. If we automate consulting work end-to-end, we eliminate the training ground where junior professionals develop judgment. The workflow agent architecture allows junior consultants to review AI drafts, understand the reasoning, suggest modifications at gates, and gradually internalize the analytical patterns. The AI becomes a teaching tool, not a replacement.\n","\n","**The Governance Imperative: Process as Product**\n","\n","Traditional consulting work products are documents: slide decks, memoranda, financial models. The deliverable is the artifact. Workflow agents invert this relationship. In our architecture, the process becomes as important as the output, and the audit trail becomes as valuable as the final deliverable.\n","\n","Every run of our system generates an immutable governance bundle: prompt logs, risk registers, verification plans, gate decisions, exception records, and stage outputs. This bundle serves multiple functions that simple document generation cannot address.\n","\n","For quality assurance, the bundle allows retrospective analysis of what the AI actually did. If a claim in the final deliverable proves incorrect, we can trace it back through stages to identify where the error entered, whether it was flagged as an assumption, and whether the verification plan addressed it. This traceability is impossible with black-box generation.\n","\n","For regulatory compliance, industries like financial services and healthcare require documentation of how decisions were reached. Our gate logs, with reviewer roles and rationales, provide this documentation. The artifact demonstrates that humans were in the loop at decision points, not merely reviewing a finished product.\n","\n","For continuous improvement, the aggregated governance data across cases reveals patterns: which stages generate the most risks, which assumptions most often require verification, which case types benefit most from AI assistance. This learning loop is only possible because we capture process data, not just outputs.\n","\n","For client trust, the ability to show a client not just what we concluded but how we arrived at that conclusion, what alternatives we considered, what assumptions we made, and what we flagged for verification demonstrates rigor that a simple AI-generated document cannot convey.\n","\n","**The Level Three Philosophy: Agents Within Boundaries**\n","\n","We term this architecture \"Level Three\" to position it within a progression of AI capability applied to knowledge work. Level One is single-turn interaction: question and answer. Level Two is complex task execution: the AI performs sophisticated analysis in one interaction but does not chain multiple steps. Level Three is supervised workflow: the AI executes a multi-step process with human checkpoints.\n","\n","The crucial insight of Level Three is that the boundary is not technological but philosophical. We could, with current technology, build a Level Four system that operates autonomously for extended periods, invoking tools, searching the web, analyzing data, and generating complete deliverables without human intervention. The constraint is not capability but appropriateness.\n","\n","Level Three acknowledges that in professional services, the goal is not to remove humans from the loop but to amplify their effectiveness while preserving accountability. The AI handles structured analytical work, pattern matching, and draft generation. Humans handle judgment, calibration, and professional responsibility. The gate is the interface where these complementary capabilities meet.\n","\n","This philosophy rejects the framing of AI as a march toward full automation. Instead, it embraces AI as a collaborator whose contribution is maximized when its role is clearly bounded. The workflow agent knows what it should do at each stage and, equally importantly, what it should not do: make recommendations, claim verification it has not performed, or proceed without human approval at critical junctions.\n","\n","**Practical Implications for Adoption**\n","\n","The workflow agent architecture has proven more readily adoptable in professional services firms than autonomous approaches precisely because it aligns with existing quality control processes. Consulting firms already operate with stage gates: intake and scoping, analytical framework development, options analysis, deliverable drafting, partner review. Our system digitizes and augments this existing workflow rather than replacing it with an alien process.\n","\n","This alignment reduces change management friction. Partners understand gates. Analysts understand stage-by-stage work. Clients understand deliverables with supporting analysis. The AI fits into mental models rather than requiring new ones. This is not conservatism but pragmatism. Technologies succeed not by being maximally advanced but by being appropriately matched to organizational context.\n","\n","Moreover, the governance artifacts our system generates align with emerging regulatory requirements. The European Union's AI Act, for instance, requires documentation of high-risk AI system decisions. Our prompt logs, risk registers, and gate records provide this documentation. Financial services regulators increasingly expect firms to demonstrate that AI-assisted decisions include human oversight at key points. Our gates provide that demonstration.\n","\n","**Conclusion: The Right Tool for the Right Job**\n","\n","The workflow agent is not a compromise between what we want and what technology permits. It is the appropriate architecture for professional services work given the actual requirements of these domains: accountability, traceability, calibration, and professional development. The fact that we could build more autonomous systems is irrelevant. The question is not what we can build but what we should deploy.\n","\n","This notebook demonstrates that supervised multi-step AI can deliver genuine valueâ€”completing complex analytical workflows end-to-endâ€”while maintaining the governance, oversight, and professional responsibility that clients, regulators, and firms themselves require. It shows that we can have both AI capability and human accountability, both efficiency and rigor, both automation and judgment.\n","\n","The future of AI in professional services is not the autonomous agent operating in isolation. It is the workflow agent operating in collaboration, with clear roles, explicit boundaries, and systematic supervision. This is not the future we settled for. This is the future we designed.\n"],"metadata":{"id":"6n-x0MLbMKUs"}},{"cell_type":"markdown","source":["###1.2.WORKFLOW"],"metadata":{"id":"OKKHIBMH9AGl"}},{"cell_type":"markdown","source":["\n","\n","**Understanding the Workflow Agent Architecture: A Step-by-Step Journey Through the Pipeline**\n","\n","This system orchestrates a sophisticated dance between human intent, artificial intelligence execution, and rigorous governance. To understand how it works, we must trace the path of a consulting case from initial user input through multiple AI-mediated stages to final deliverable packaging. Let me walk you through this journey with precision.\n","\n","**The Entry Point: User Input and Case Specification**\n","\n","When a user engages with this system, they do not simply send a free-form prompt into the void. Instead, they provide structured information about a consulting case. This information includes the case name, the type of consulting work required (market entry, cost transformation, capital allocation, or operating model redesign), a narrative description of the business situation, and the desired deliverable format (executive memo, workplan, investment committee pre-read, or operating model narrative).\n","\n","This initial input is not yet encoded into JSON at this point. Rather, it exists as a Python dictionary called case_spec that contains these key-value pairs. The system receives this structured input and begins processing it through the pipeline. This represents a form of encoding, but it is encoding into a data structure that the orchestration code can manipulate, not yet the JSON schema that will govern AI responses.\n","\n","**Stage Zero: Intake and Classification Without AI**\n","\n","The pipeline begins with Stage Zero, which involves no AI model call whatsoever. This stage captures the user's inputs into an intake recordâ€”a JSON object that documents the case name, case type, situation description, deliverable type, timestamp, and scope boundary. This intake record becomes the first artifact saved to disk in the stage outputs directory.\n","\n","The scope boundary is particularly important. It explicitly states what the analysis will and will not cover, establishing guardrails for the AI's work in subsequent stages. For instance, it might declare that the analysis is limited to strategic assessment and will not include implementation planning. This boundary setting happens before any AI is invoked, establishing human-defined constraints on the work to follow.\n","\n","**The Gate Mechanism: Human Checkpoints Between Stages**\n","\n","After Stage Zero completes, the system encounters its first gate. You are absolutely correct that in production use, humans would approve or reject transitions between stages. The auto-approve parameter in the code exists solely to allow the demonstration cases to run end-to-end without manual intervention during testing. In real consulting work, each gate would pause execution and require a human reviewer to examine the previous stage's output and decide whether to proceed.\n","\n","When a gate is triggered, the system logs this decision to the stage gate log file in JSONL format (JSON Lines, where each line is a complete JSON object). This log records the timestamp, the case name, which transition is being evaluated (for example, Stage Zero to Stage One), the role of the human reviewer (Analyst, Manager, or Partner), the decision (approve or reject), and the rationale for that decision.\n","\n","If the human rejects the transition, the system appends an exception to the exception log and halts the pipeline for that case. The partial results remain saved in the stage outputs directory, allowing the team to review what was completed and potentially restart from a corrected state. If the human approves, the pipeline proceeds to the next stage.\n","\n","**Stage One: Structureâ€”The First AI Call**\n","\n","Stage One represents the first time the system invokes Claude. The orchestrator constructs a prompt that includes the case type, the situation description from intake, and specific instructions about what kind of analytical framework to build. For a market entry case, this might request an issue tree. For a cost transformation, it might request a driver tree showing cost decomposition. The prompt explicitly instructs the AI not to make recommendations, only to structure the problem.\n","\n","Before sending this prompt to the Anthropic API, the system computes a cryptographic hash of the prompt text. This is what gets logged, not the prompt itself. The hash serves multiple purposes related to governance and privacy.\n","\n","**Understanding Hashing: Why and What Gets Hashed**\n","\n","The hashing mechanism is fundamental to the governance architecture, so let me explain it carefully. A cryptographic hash function takes any input text and produces a fixed-length string of characters that uniquely represents that input. If even a single character in the input changes, the hash changes completely. However, you cannot reverse the processâ€”you cannot take a hash and reconstruct the original text.\n","\n","The system hashes prompts before logging them to the prompts log file. What gets logged is the timestamp, the stage name, the hash of the prompt, the model identifier, and the parameters (temperature and max tokens). The original prompt text is not logged. This serves several critical functions.\n","\n","First, it protects confidentiality. If the user has included sensitive information in their case descriptionâ€”company names, financial figures, strategic plansâ€”that information appears in the prompts sent to each stage. By logging only the hash, the governance trail does not contain readable sensitive information. An auditor can verify that a particular prompt was sent (by recomputing its hash and comparing), but cannot read the content without access to the original input.\n","\n","Second, it enables integrity verification. If someone later claims that different prompts were used than what was actually sent, the hash provides proof. The system can recompute the hash of any claimed prompt and compare it to the logged hash. If they do not match, tampering or misrepresentation is evident.\n","\n","Third, it supports reproducibility without exposing details. A data scientist reviewing the governance logs can see that Stage One received a prompt with hash X, Stage Two received a prompt with hash Y, and so forth. If they have access to the original case specification, they can recompute these hashes to verify the prompts were constructed correctly from the inputs. But the logs themselves do not leak information.\n","\n","**The Wrapper: Enforcing Structure on AI Responses**\n","\n","Once the prompt is sent to Claude via the Anthropic API, the response comes back as natural language text. This is where the wrapper function call_claude_stage performs critical work. The wrapper does not simply accept whatever Claude returns. Instead, it enforces strict requirements on response format and content.\n","\n","The wrapper first attempts to extract valid JSON from Claude's response. It looks for the first opening brace and last closing brace in the text, extracting everything between them. It then applies repair functions that remove common JSON errors like trailing commas before closing brackets. If the text still does not parse as valid JSON, the wrapper writes a debug file showing what went wrong and either retries with a corrective prompt or fails the stage.\n","\n","Assuming valid JSON is extracted, the wrapper then validates the schema. It checks that all required keys are present (task, facts_provided, assumptions, open_questions, risks, draft_output, verification_status, questions_to_verify) and that no unexpected extra keys appear. It verifies that the risks array contains properly structured risk objects with type, severity, and note fields. It confirms that verification_status is set to \"Not verified\" because the AI cannot verify its own claims. It scans the draft output for forbidden recommendation language like \"we recommend\" or \"best option\" which would violate the Level Three separation of duties between AI drafting and human decision-making.\n","\n","The wrapper also performs automatic risk detection. It examines the response for patterns that indicate potential problems: numeric claims that do not appear in the facts or assumptions provided (possible hallucination), language that implies work was performed when no such work was documented (decision laundering), or outputs that lack expected markers for that stage type (false rigor). Each detected risk gets appended to the risks array before the JSON is returned.\n","\n","Throughout this validation process, if issues are found, the wrapper can retry the API call with a prompt that lists the specific problems and requests corrections. This retry mechanism gives Claude a chance to fix validation errors without failing the entire stage. Only after multiple retries does the wrapper give up and raise an exception.\n","\n","**Logging the Results: Building the Audit Trail**\n","\n","Once the wrapper successfully validates a response, it updates the risk log by appending all risks from this stage's response. Each risk entry includes a timestamp, the stage name, and the risk details. This creates an aggregated view across all stages of what risks were detected during the pipeline execution.\n","\n","The validated JSON response is then saved to the stage outputs directory in a file named according to the stage and case, such as stage_1_structure.json. This file contains everything Claude returned: the task description, the facts it understood from the prompt, the assumptions it made, the open questions it identified, the risks it and the wrapper detected, the main analytical output, and the verification status.\n","\n","**Stage Progression: Accumulating Context**\n","\n","After Stage One completes and its gate is approved, the pipeline proceeds to Stage Two. The prompt for Stage Two is fundamentally different from Stage One's prompt because it includes excerpts from Stage One's output. The orchestrator extracts the draft output from Stage One (the actual issue tree or analytical structure) and includes it in the Stage Two prompt.\n","\n","This context passing is what makes the system agentic rather than just a series of independent AI calls. Stage Two is not starting from scratch. It receives the structured problem space that Stage One created and must generate strategic options that align with that structure. The AI in Stage Two must read and understand what Stage One produced, then build upon it.\n","\n","The same pattern continues through subsequent stages. Stage Three receives excerpts from both Stage One and Stage Two, allowing it to draft a deliverable that reflects the analytical structure and the options identified. Stage Four receives the accumulated assumptions and open questions from all previous stages, enabling it to create a comprehensive verification plan.\n","\n","This accumulation of context is why we can consider each stage a specialized agent. The Structure Agent produces output specifically designed for consumption by the Options Agent. The Options Agent produces output formatted for the Deliverable Agent. Each agent has a narrow competence but contributes to a larger collaborative effort.\n","\n","**Stage Four: The Verification Challenge**\n","\n","Stage Four is particularly interesting because it performs a meta-analytical function. It does not generate new business analysis but instead analyzes the analysis itself. The prompt for Stage Four includes all assumptions made across previous stages and all open questions that were flagged. It asks Claude to construct a verification plan: for each assumption or question, what evidence would verify or refute it, who should own that verification work, what method should be used, and what priority it should receive.\n","\n","This stage sometimes encounters difficulties because the accumulated context can become quite large. If Stages One through Three collectively generated fifty assumptions and thirty open questions, the Stage Four prompt must include all of these. To prevent token limit issues, the orchestrator truncates these lists to the top ten of each, focusing on the most critical items.\n","\n","The Stage Four output is crucial for governance because it makes explicit what is known versus assumed versus unknown. A consulting deliverable without a verification plan leaves the reader uncertain about which claims are supported by evidence and which rest on untested assumptions. The verification plan provides a roadmap for converting assumptions into verified facts.\n","\n","**Stage Five: Packaging Without AI**\n","\n","Stage Five performs no AI calls. It is purely orchestration code that assembles the outputs from previous stages into final deliverable formats. The packaging stage creates two primary artifacts.\n","\n","First, a JSON bundle that provides an index into the case results. This bundle includes the case name, timestamp, a list of which stages completed successfully, excerpts from each stage's main output, the verification status, total risks logged, and total open questions identified. This bundle serves as a compact summary that a reviewer can quickly scan to understand what the pipeline produced.\n","\n","Second, a human-readable text file that presents the same information in prose format. This file concatenates excerpts from each stage's output under clear headings, then provides a summary section with statistics about the run. It includes prominent warnings that all output is unverified and requires human review before use in client work.\n","\n","Both of these artifacts are saved to the deliverables directory, separate from the stage outputs directory. The stage outputs contain the complete raw JSON from each AI call. The deliverables contain the curated final products intended for human consumption.\n","\n","**The Complete Governance Bundle**\n","\n","In addition to stage outputs and deliverables, the system maintains multiple governance logs that accumulate information across the entire run. Let me clarify what each contains and what is hashed.\n","\n","The prompts log contains one entry per AI call. Each entry has a timestamp, the stage name, a hash of the prompt that was sent, the model identifier, and the parameters used. The prompt text itself is not loggedâ€”only its hash. This means you can verify a specific prompt was used but cannot read it from the log alone.\n","\n","The risk log aggregates all risks detected across all stages. Each entry includes a timestamp, the stage name, the risk type (hallucination, decision_laundering, false_rigor, etc.), the severity (low, medium, high), and a note describing the specific concern. This log provides a complete risk profile for the case.\n","\n","The verification register collects all unverified claims, assumptions, and open questions. When Stage Four completes, it appends an entry to this register containing the case name, the lists of assumptions and questions, and the verification plan. This creates a backlog of verification work that must be completed before the analysis can be considered final.\n","\n","The stage gate log records every gate decision. Each line is a JSON object documenting when a gate was evaluated, which transition was considered, what role the reviewer held, what decision was made, and why. This provides a complete decision trail showing where humans approved progression and where they might have halted or redirected the work.\n","\n","The exception log captures deviations from normal execution: stages that failed, validations that could not be satisfied even after retries, gates that were rejected, or policy violations that were detected. This log serves as a flag for quality assurance, highlighting where the pipeline did not execute smoothly.\n","\n","The change log would record if a case were re-run with modifications, documenting what changed between runs. The approvals log provides space for formal human sign-offs, though in this demonstration it remains a placeholder.\n","\n","The workflow runbook defines the pipeline structure itself: the six stages, their allowed transitions, the gate policy, the role expectations, and the one-call-per-stage rule. This runbook makes the workflow topology explicit and auditable.\n","\n","**Final Assembly and Distribution**\n","\n","Once all stages complete and all logs are updated, Cell Ten creates an audit readme file that explains the purpose of the system, describes each artifact in the governance bundle, provides instructions for reproducing the run, emphasizes the unverified status of outputs, and includes confidentiality notices. This readme serves as both documentation and disclaimer.\n","\n","The system then creates a zip archive containing the entire run directory: all stage outputs, all deliverables, all governance logs, the runbook, and the readme. This single zip file constitutes a complete, self-contained audit package that can be stored, shared with reviewers, or submitted to compliance functions.\n","\n","The zip archive's filename includes the run identifier, which itself contains a timestamp and a random short ID. This ensures each run produces a uniquely named archive that cannot collide with previous runs.\n","\n","**Key Clarifications**\n","\n"," The user provides a case specification as structured data (a Python dictionary), not a free-form prompt. This case specification is used to construct prompts for each stage, but the prompts themselves are natural language instructions, not JSON.\n","\n","The JSON schema comes into play when Claude responds. The system requires Claude to return responses in a strict JSON format conforming to a specific schema. The prompt encoding happens when the orchestrator builds natural language instructions from the case specification. The response encoding happens when Claude formats its analysis as structured JSON.\n","\n","What is hashed. Only the prompts sent to Claude are hashed before logging. The responses from Claude are not hashedâ€”they are validated, saved in full as JSON files, and have their risks extracted into the risk log. The hashing applies exclusively to inputs (prompts) as a privacy and integrity measure.\n","\n","Authorization to move between stages is hard-coded for demonstration purposes but would require human approval in production. The auto_approve parameter makes this explicit. When set to True, gates automatically approve and log that decision. When set to False, the code pauses and requests keyboard input from a human operator. In a real enterprise deployment, this would likely be replaced by integration with a workflow management system where reviewers could approve or reject transitions through a web interface.\n","\n","When the process completes, the system prepares final documents and files. Stage Five and Cell Ten handle this packaging and bundling work. However, it is important to understand that \"completion\" does not mean \"ready for client delivery.\" All outputs carry the verification status of \"Not verified\" and include prominent disclaimers. The pipeline produces draft materials and a verification roadmap, not finished client deliverables.\n","\n","**The Core Insight: Process as Product**\n","\n","The deepest principle underlying this architecture is that the process itself is as valuable as the output. A traditional consulting engagement produces a slide deck or memo. This system produces that artifact plus a complete record of how it was created: what assumptions were made, what risks were detected, what questions remain open, what verification is needed, and where humans approved progression.\n","\n","This shift from product-centric to process-centric delivery reflects the realities of professional services in an AI-augmented world. Clients and regulators increasingly demand not just answers but transparency about how those answers were reached. The governance bundle provides that transparency. It demonstrates that the work was structured, supervised, and subject to systematic risk identification. It proves that humans were in the loop at critical decision points. It acknowledges uncertainty rather than hiding it.\n","\n","This is why we call it a workflow agent rather than an autonomous agent. The workflow topologyâ€”the sequence of stages, the gates between them, the accumulation of context, the governance artifactsâ€”is the primary design. The AI capabilities are deployed within this workflow, not allowed to run free. The workflow disciplines the AI, and the AI amplifies the workflow's capacity. Both elements are essential. Neither is sufficient alone.\n"],"metadata":{"id":"wRR03uEA7I7H"}},{"cell_type":"markdown","source":["##2.LIBRARIES AND ENVIRONMENT"],"metadata":{"id":"HzSSYRQ0MKs8"}},{"cell_type":"code","source":["!pip install -q anthropic\n","\n","import json\n","import os\n","import re\n","import hashlib\n","import platform\n","import textwrap\n","from datetime import datetime, timezone\n","from pathlib import Path\n","import subprocess\n","import uuid\n","\n","# Create base run directory\n","timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n","short_id = str(uuid.uuid4())[:8]\n","run_id = f\"run_{timestamp}_{short_id}\"\n","\n","base_dir = Path(f\"/content/ai_consulting_ch3_runs/{run_id}\")\n","base_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Create subdirectories\n","deliverables_dir = base_dir / \"deliverables\"\n","stage_outputs_dir = base_dir / \"stage_outputs\"\n","\n","deliverables_dir.mkdir(exist_ok=True)\n","stage_outputs_dir.mkdir(exist_ok=True)\n","\n","# Create per-case subfolders in stage_outputs\n","case_names = [\"market_entry\", \"cost_transformation\", \"capital_allocation\", \"operating_model\"]\n","for case in case_names:\n","    (stage_outputs_dir / case).mkdir(exist_ok=True)\n","\n","print(\"âœ“ Run directory created\")\n","print(f\"  Base: {base_dir}\")\n","print(f\"  Run ID: {run_id}\")\n","print(f\"  Deliverables: {deliverables_dir}\")\n","print(f\"  Stage outputs: {stage_outputs_dir}\")\n","print(f\"  Per-case folders: {', '.join(case_names)}\")"],"metadata":{"id":"TOYUwL9vMKKC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768854793213,"user_tz":360,"elapsed":4362,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"31a83066-075e-44e4-b38c-c4bf7289262d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ Run directory created\n","  Base: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf\n","  Run ID: run_20260119_203313_cf3f83cf\n","  Deliverables: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/deliverables\n","  Stage outputs: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/stage_outputs\n","  Per-case folders: market_entry, cost_transformation, capital_allocation, operating_model\n"]}]},{"cell_type":"markdown","source":["##3.API AND CLIENT INITIALIZATION"],"metadata":{"id":"STtjBQgwMNHi"}},{"cell_type":"markdown","source":["###3.1.OVERVIEW"],"metadata":{"id":"ShWjt_vyMO17"}},{"cell_type":"markdown","source":["**Cell 3: Connecting to Claude's API**\n","\n","This cell establishes the connection between your notebook and Anthropic's Claude AI service. Think of it as plugging in the power cord before you can use any electrical device.\n","\n","When you work with AI models like Claude, they don't run on your computer. Instead, they run on Anthropic's servers in the cloud. To access them, you need two things: an API key (which is like a password that proves you're authorized to use the service) and a client (which is the software that manages the communication).\n","\n","**What happens in this cell:**\n","\n","First, the notebook retrieves your API key from Google Colab's secure storage system. You should have already added this key to Colab's \"Secrets\" section (the key icon in the sidebar). This is much safer than typing your API key directly into the code where others might see it.\n","\n","Second, it creates what we call a \"client\" - a connection manager that will handle all your requests to Claude. Every time you want Claude to analyze something or generate reasoning structures, this client will send your request to Anthropic's servers and bring back the response.\n","\n","Third, it specifies exactly which version of Claude you're using. In this case, we're using Claude Sonnet 4.5, which is optimized for structured reasoning tasks - perfect for management consulting work. Different Claude models have different strengths; Sonnet strikes a balance between speed and sophisticated reasoning.\n","\n","**Why this matters for consulting:**\n","\n","In professional consulting, you need to know exactly which tools you're using and be able to audit your work. This cell creates a clear record of which AI model you used, when you connected to it, and that you had proper authorization. This transparency is essential for governance and for explaining your methodology to clients or stakeholders.\n","\n","If this cell fails, it means your API key isn't configured correctly, and none of the subsequent AI-powered cells will work."],"metadata":{"id":"n_uMwdv8MRcU"}},{"cell_type":"markdown","source":["###3.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"Xml9f9GoMR7J"}},{"cell_type":"code","source":["# Cell 3: API Key + Client Initialization\n","\n","import anthropic\n","from google.colab import userdata\n","\n","# Get API key from Colab Secrets\n","try:\n","    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n","    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n","    client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n","    print(\"âœ“ API key loaded from Colab Secrets\")\n","except Exception as e:\n","    print(\"âœ— FAILED to load API key\")\n","    print(\"\\nTo fix this:\")\n","    print(\"1. Click the key icon (ðŸ”‘) in the left sidebar\")\n","    print(\"2. Add a new secret named: ANTHROPIC_API_KEY\")\n","    print(\"3. Paste your Anthropic API key as the value\")\n","    print(\"4. Enable 'Notebook access'\")\n","    print(\"5. Re-run this cell\")\n","    raise\n","\n","# Model configuration\n","MODEL = \"claude-sonnet-4-5-20250929\"\n","DEFAULT_TEMPERATURE = 0.2\n","DEFAULT_MAX_TOKENS = 4128\n","\n","print(f\"\\nâœ“ Client initialized\")\n","print(f\"  Model: {MODEL}\")\n","print(f\"  Temperature: {DEFAULT_TEMPERATURE}\")\n","print(f\"  Max tokens: {DEFAULT_MAX_TOKENS}\")"],"metadata":{"id":"UExynb0iMJwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768854798970,"user_tz":360,"elapsed":3951,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"d91deaec-afb4-44cf-c233-70fbad99b684"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ API key loaded from Colab Secrets\n","\n","âœ“ Client initialized\n","  Model: claude-sonnet-4-5-20250929\n","  Temperature: 0.2\n","  Max tokens: 4128\n"]}]},{"cell_type":"markdown","source":["##4.GOVERNANCE ARTIFACTS AND WORKFLOW RUNBOOK"],"metadata":{"id":"zSjFjeDuMaZn"}},{"cell_type":"markdown","source":["###4.1.OVERVIEW"],"metadata":{"id":"6OWXlpZoMcHa"}},{"cell_type":"markdown","source":["**Cell 4: Governance Artifacts and Workflow Runbook**\n","\n","**What This Cell Does**\n","\n","This cell establishes the foundational governance infrastructure for our workflow agent system. Think of it as setting up the filing cabinet, record-keeping system, and rulebook before any actual consulting work begins. When you run this cell, it creates a complete audit trail framework that will document everything that happens during the pipeline execution.\n","\n","**Helper Functions: The Administrative Toolkit**\n","\n","The cell begins by defining several utility functions that handle routine administrative tasks. The now_iso function generates timestamps in a standardized international format, ensuring every event can be precisely dated. The sha256_text function creates cryptographic hashes of textâ€”this is how we protect confidential information in prompts while still maintaining proof of what was sent. The write_json and read_json functions handle saving and loading structured data files. The append_jsonl function adds new records to log files without overwriting existing entries, like adding pages to a journal rather than replacing the whole book.\n","\n","**Creating the Run Manifest**\n","\n","The cell then creates what we call a run manifest, which is essentially the birth certificate for this particular execution of the pipeline. It records when the run started, which AI model was used, what parameters were configured (like temperature and token limits), and generates a unique identifier for this run. Most importantly, it computes a configuration hashâ€”a fingerprint of the exact setup. If someone later tries to reproduce your results, they can verify they used identical settings by comparing config hashes.\n","\n","**Initializing the Governance Logs**\n","\n","Next, the cell creates empty log files that will accumulate information as the pipeline executes. The prompts log will record every interaction with the AI, though only as hashes for confidentiality. The risk log will aggregate all detected risks across all stages. The verification register will track claims that need human validation. The change log documents modifications if cases are re-run. The approvals log provides space for formal sign-offs. The stage gate log records every human checkpoint decision. The exception log captures any failures or policy violations.\n","\n","**Defining the Workflow Runbook**\n","\n","Finally, the cell creates the workflow runbook, which is the definitive specification of how the pipeline operates. It lists all six stages, describes what each stage does, specifies how many AI calls each stage is allowed (zero, one, or more), and defines which transitions between stages are permitted. This runbook serves as both documentation and contractâ€”it makes explicit the workflow topology that governs AI behavior. The runbook also establishes the gate policy requiring human approval at each transition and identifies the roles expected to review work at different stages.\n","\n","This entire cell executes in seconds but establishes the governance scaffolding that enables transparent, auditable, and accountable AI-assisted consulting work."],"metadata":{"id":"XdRc9-nEMeIF"}},{"cell_type":"markdown","source":["###4.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"GvnsRIc6Mefx"}},{"cell_type":"code","source":["# Cell 4: Governance Artifacts + Workflow Runbook\n","\n","def now_iso():\n","    \"\"\"Return current UTC timestamp in ISO format\"\"\"\n","    return datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')\n","\n","def sha256_text(text):\n","    \"\"\"Return SHA256 hash of text\"\"\"\n","    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n","\n","def write_json(path, data):\n","    \"\"\"Write JSON to file\"\"\"\n","    with open(path, 'w', encoding='utf-8') as f:\n","        json.dump(data, f, indent=2, ensure_ascii=False)\n","\n","def read_json(path):\n","    \"\"\"Read JSON from file\"\"\"\n","    with open(path, 'r', encoding='utf-8') as f:\n","        return json.load(f)\n","\n","def append_jsonl(path, record):\n","    \"\"\"Append JSON record to JSONL file\"\"\"\n","    with open(path, 'a', encoding='utf-8') as f:\n","        f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n","\n","def get_env_fingerprint():\n","    \"\"\"Get environment fingerprint\"\"\"\n","    return {\n","        \"python_version\": platform.python_version(),\n","        \"platform\": platform.platform(),\n","        \"anthropic_version\": anthropic.__version__\n","    }\n","\n","def stable_config_hash(config):\n","    \"\"\"Generate stable hash from config dict\"\"\"\n","    canonical = json.dumps(config, sort_keys=True)\n","    return sha256_text(canonical)[:16]\n","\n","# Write run_manifest.json\n","run_manifest = {\n","    \"run_id\": run_id,\n","    \"timestamp\": now_iso(),\n","    \"chapter\": \"3\",\n","    \"level\": \"Agents (Supervised Multi-Step Workflows)\",\n","    \"model\": MODEL,\n","    \"params\": {\n","        \"temperature\": DEFAULT_TEMPERATURE,\n","        \"max_tokens\": DEFAULT_MAX_TOKENS\n","    },\n","    \"notebook_purpose\": \"Demonstrate governed multi-step consulting workflows with human checkpoints\",\n","    \"environment\": get_env_fingerprint()\n","}\n","\n","config_hash = stable_config_hash({\n","    \"model\": MODEL,\n","    \"temperature\": DEFAULT_TEMPERATURE,\n","    \"max_tokens\": DEFAULT_MAX_TOKENS\n","})\n","\n","run_manifest[\"config_hash\"] = config_hash\n","\n","manifest_path = base_dir / \"run_manifest.json\"\n","write_json(manifest_path, run_manifest)\n","\n","# Initialize governance logs\n","prompts_log_path = base_dir / \"prompts_log.jsonl\"\n","risk_log_path = base_dir / \"risk_log.json\"\n","verification_register_path = base_dir / \"verification_register.json\"\n","change_log_path = base_dir / \"change_log.json\"\n","approvals_log_path = base_dir / \"approvals_log.json\"\n","\n","# Initialize Level 3 logs\n","stage_gate_log_path = base_dir / \"stage_gate_log.jsonl\"\n","exception_log_path = base_dir / \"exception_log.json\"\n","\n","# Create empty files\n","prompts_log_path.touch()\n","write_json(risk_log_path, {\"risks\": []})\n","write_json(verification_register_path, {\"unverified_claims\": []})\n","write_json(change_log_path, {\"changes\": []})\n","write_json(approvals_log_path, {\"approvals\": []})\n","stage_gate_log_path.touch()\n","write_json(exception_log_path, {\"exceptions\": []})\n","\n","# Create workflow_runbook.json\n","workflow_runbook = {\n","    \"workflow_name\": \"Supervised Consulting Pipeline\",\n","    \"stages\": [\n","        {\n","            \"stage_id\": 0,\n","            \"name\": \"Intake + Classification\",\n","            \"model_calls\": 0,\n","            \"description\": \"Capture situation, classify case type, define scope boundary\"\n","        },\n","        {\n","            \"stage_id\": 1,\n","            \"name\": \"Structure\",\n","            \"model_calls\": 1,\n","            \"description\": \"Build issue tree / driver tree / design dimensions\"\n","        },\n","        {\n","            \"stage_id\": 2,\n","            \"name\": \"Options + Trade-offs\",\n","            \"model_calls\": 1,\n","            \"description\": \"Generate options, identify trade-offs and tensions\"\n","        },\n","        {\n","            \"stage_id\": 3,\n","            \"name\": \"Draft Deliverable Bundle\",\n","            \"model_calls\": 1,\n","            \"description\": \"Create draft exec memo / workplan / IC pre-read / operating model narrative\"\n","        },\n","        {\n","            \"stage_id\": 4,\n","            \"name\": \"Verification Plan + Assumption Register\",\n","            \"model_calls\": 1,\n","            \"description\": \"List all assumptions, create verification plan with owners and evidence sources\"\n","        },\n","        {\n","            \"stage_id\": 5,\n","            \"name\": \"Packaging\",\n","            \"model_calls\": 0,\n","            \"description\": \"Create human-readable bundle and zip archive\"\n","        }\n","    ],\n","    \"allowed_transitions\": [\n","        [0, 1], [1, 2], [2, 3], [3, 4], [4, 5]\n","    ],\n","    \"gate_policy\": \"Each transition requires gate approval logged in stage_gate_log.jsonl\",\n","    \"role_placeholders\": [\"Analyst\", \"Manager\", \"Partner\", \"Client\"],\n","    \"one_call_per_stage_rule\": \"Maximum one model call per stage\"\n","}\n","\n","runbook_path = base_dir / \"workflow_runbook.json\"\n","write_json(runbook_path, workflow_runbook)\n","\n","print(\"âœ“ Governance artifacts initialized\")\n","print(f\"\\n  run_manifest.json: {manifest_path}\")\n","print(f\"  prompts_log.jsonl: {prompts_log_path}\")\n","print(f\"  risk_log.json: {risk_log_path}\")\n","print(f\"  verification_register.json: {verification_register_path}\")\n","print(f\"  change_log.json: {change_log_path}\")\n","print(f\"  approvals_log.json: {approvals_log_path}\")\n","print(f\"  stage_gate_log.jsonl: {stage_gate_log_path}\")\n","print(f\"  exception_log.json: {exception_log_path}\")\n","print(f\"  workflow_runbook.json: {runbook_path}\")\n","print(f\"\\n  Config hash: {config_hash}\")\n"],"metadata":{"id":"AY_6OY4SMbAv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768854804373,"user_tz":360,"elapsed":40,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"986990ce-5a85-417f-b65b-e998d29b4f06"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ Governance artifacts initialized\n","\n","  run_manifest.json: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/run_manifest.json\n","  prompts_log.jsonl: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/prompts_log.jsonl\n","  risk_log.json: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/risk_log.json\n","  verification_register.json: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/verification_register.json\n","  change_log.json: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/change_log.json\n","  approvals_log.json: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/approvals_log.json\n","  stage_gate_log.jsonl: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/stage_gate_log.jsonl\n","  exception_log.json: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/exception_log.json\n","  workflow_runbook.json: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/workflow_runbook.json\n","\n","  Config hash: db7cc0a1885c54c4\n"]}]},{"cell_type":"markdown","source":["##5.CONFINDENTIALITY AND GATE UTILITIES"],"metadata":{"id":"aFfOfCZmMgx4"}},{"cell_type":"markdown","source":["###5.1.OVERVIEW"],"metadata":{"id":"tIlOMU-SMjni"}},{"cell_type":"markdown","source":["**Cell 5: Confidentiality Utilities and Gate Mechanisms**\n","\n","**What This Cell Does**\n","\n","This cell implements two critical protective mechanisms for professional consulting work: confidentiality safeguards and human checkpoint infrastructure. Think of it as installing both a privacy filter and a quality control gate system before any sensitive work begins. The cell provides tools to protect client information and ensures humans maintain control over workflow progression.\n","\n","**Redaction: Protecting Sensitive Information**\n","\n","The first function, called redact, automatically identifies and masks potentially sensitive patterns in text. When you describe a consulting case, you might inadvertently include email addresses, phone numbers, or monetary amounts that should not appear in logs or outputs. This function scans text and replaces these patterns with placeholders like EMAIL_REDACTED or AMOUNT_REDACTED. While this demonstration uses basic pattern matching for common data types, production systems would employ more sophisticated domain-specific redaction based on your firm's policies and the nature of client engagements.\n","\n","The build_minimum_necessary function applies this redaction and provides a summary of what was removed. It operates on the principle of data minimizationâ€”only the information truly necessary for analysis should be visible, and everything else should be masked. The function returns both the cleaned text and a count of how many items of each type were redacted, giving you transparency about what protective actions were taken.\n","\n","**Gate Functions: Human Checkpoints**\n","\n","The gate function implements the human approval mechanism that sits between every stage of the pipeline. When called, it creates a permanent record in the stage gate log documenting who made a decision, what they decided, and why. The function accepts several parameters: which stages are being transitioned between, which case is being evaluated, what role the reviewer holds (Analyst, Manager, or Partner), whether they approve or reject the transition, and their rationale for that decision.\n","\n","If the decision is to approve, the gate function simply logs this and returns true, allowing the pipeline to continue. If the decision is to reject, the function does two additional things: it appends an entry to the exception log explaining that a gate was rejected, and it returns false, which signals the pipeline orchestrator to halt execution for that case. This creates an immutable record of where human judgment intervened to stop or redirect AI work.\n","\n","**Demonstration and Verification**\n","\n","The cell concludes by demonstrating both utilities in action. It shows how redaction transforms a sample text containing personal information into a protected version, and it logs a sample gate approval decision. These demonstrations serve two purposes: they verify the functions work correctly, and they help you understand what the governance artifacts will look like when examining logs later. This transparency about protective mechanisms builds trust in the system's governance architecture."],"metadata":{"id":"ksHvcArXMxMR"}},{"cell_type":"markdown","source":["###5.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"x7z8cMnQMx3g"}},{"cell_type":"code","source":["# Cell 5: Confidentiality + Gate Utilities\n","\n","def redact(text):\n","    \"\"\"\n","    Redact potentially sensitive patterns.\n","    This is a DEMO function - real redaction requires domain expertise.\n","    \"\"\"\n","    # Redact emails\n","    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL_REDACTED]', text)\n","    # Redact phone numbers\n","    text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE_REDACTED]', text)\n","    # Redact potential monetary amounts\n","    text = re.sub(r'\\$\\s?\\d+(?:,\\d{3})*(?:\\.\\d{2})?(?:\\s?[MBK])?', '[AMOUNT_REDACTED]', text)\n","    return text\n","\n","def build_minimum_necessary(raw_text):\n","    \"\"\"\n","    Apply redaction and return cleaned text + summary of removed fields.\n","    \"\"\"\n","    redacted_text = redact(raw_text)\n","\n","    removed_count = raw_text.count('[') - redacted_text.count('[') if '[' in raw_text else \\\n","                    len(re.findall(r'\\[.*?REDACTED\\]', redacted_text))\n","\n","    removed_fields = {\n","        \"emails_redacted\": redacted_text.count('[EMAIL_REDACTED]'),\n","        \"phones_redacted\": redacted_text.count('[PHONE_REDACTED]'),\n","        \"amounts_redacted\": redacted_text.count('[AMOUNT_REDACTED]')\n","    }\n","\n","    return redacted_text, removed_fields\n","\n","def gate(stage_from, stage_to, case_name, reviewer_role, decision, rationale):\n","    \"\"\"\n","    Log a gate decision in stage_gate_log.jsonl.\n","    If decision != 'approve', append to exception_log and return False.\n","    Otherwise return True.\n","    \"\"\"\n","    gate_record = {\n","        \"timestamp\": now_iso(),\n","        \"case_name\": case_name,\n","        \"transition\": f\"Stage {stage_from} â†’ Stage {stage_to}\",\n","        \"reviewer_role\": reviewer_role,\n","        \"decision\": decision,\n","        \"rationale\": rationale\n","    }\n","\n","    append_jsonl(stage_gate_log_path, gate_record)\n","\n","    if decision != \"approve\":\n","        # Log exception\n","        exception_data = read_json(exception_log_path)\n","        exception_data[\"exceptions\"].append({\n","            \"timestamp\": now_iso(),\n","            \"type\": \"gate_rejection\",\n","            \"case_name\": case_name,\n","            \"transition\": f\"Stage {stage_from} â†’ Stage {stage_to}\",\n","            \"reviewer_role\": reviewer_role,\n","            \"rationale\": rationale\n","        })\n","        write_json(exception_log_path, exception_data)\n","        return False\n","\n","    return True\n","\n","# Demo redaction\n","demo_text = \"Contact John at john.smith@acme.com or 555-123-4567. Budget: $2.5M.\"\n","redacted_demo, removed = build_minimum_necessary(demo_text)\n","\n","print(\"âœ“ Confidentiality utilities ready\")\n","print(f\"\\n  Original: {demo_text}\")\n","print(f\"  Redacted: {redacted_demo}\")\n","print(f\"  Removed: {removed}\")\n","\n","# Demo gate logging\n","gate_result = gate(\n","    stage_from=0,\n","    stage_to=1,\n","    case_name=\"demo_case\",\n","    reviewer_role=\"Manager\",\n","    decision=\"approve\",\n","    rationale=\"Demo gate for testing\"\n",")\n","\n","print(f\"\\nâœ“ Gate utilities ready\")\n","print(f\"  Demo gate logged: {gate_result}\")\n","print(f\"  Check: {stage_gate_log_path}\")\n"],"metadata":{"id":"_OdMQ8SwMiEp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768854808389,"user_tz":360,"elapsed":35,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"ae7bced8-b73e-48af-c9b1-d8b7a97b3b5f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ Confidentiality utilities ready\n","\n","  Original: Contact John at john.smith@acme.com or 555-123-4567. Budget: $2.5M.\n","  Redacted: Contact John at [EMAIL_REDACTED] or [PHONE_REDACTED]. Budget: [AMOUNT_REDACTED].5M.\n","  Removed: {'emails_redacted': 1, 'phones_redacted': 1, 'amounts_redacted': 1}\n","\n","âœ“ Gate utilities ready\n","  Demo gate logged: True\n","  Check: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/stage_gate_log.jsonl\n"]}]},{"cell_type":"markdown","source":["##6.LLM WRAPPER"],"metadata":{"id":"uVJjvhvVM0E8"}},{"cell_type":"markdown","source":["###6.1.OVERVIEW"],"metadata":{"id":"MyxD2F2XM1J3"}},{"cell_type":"markdown","source":["**Cell 6: The Language Model Wrapper with Robust Error Handling**\n","\n","**What This Cell Does**\n","\n","This cell implements the critical interface between our governance system and the AI model itself. Think of it as a quality control inspector standing between raw AI output and your consulting deliverables, ensuring every response meets strict standards before being accepted. This wrapper performs four essential functions: it repairs malformed responses, validates structure and content, detects risks automatically, and manages retries when issues are found.\n","\n","**JSON Repair: Fixing Common AI Mistakes**\n","\n","The fix_json_string and extract_json_robust functions handle a practical reality: AI models sometimes generate responses with minor formatting errors even when instructed to return perfect JSON. The most common error is trailing commas before closing brackets, which breaks JSON parsing. These repair functions locate the actual JSON object within the AI's response (ignoring any markdown formatting or preamble text), attempt to parse it, and if that fails, apply targeted fixes to remove trailing commas. If the response still cannot be parsed after repairs, the system writes a detailed debug file showing exactly what went wrong and where, then raises an error rather than proceeding with corrupted data.\n","\n","**Schema Validation: Enforcing Structure and Policy**\n","\n","The validate_stage_json function performs rigorous quality control on successfully parsed responses. It verifies that all required fields are presentâ€”task description, facts provided, assumptions made, open questions, risks, draft output, verification status, and questions to verify. It rejects responses with unexpected extra fields, ensuring the AI stays within the defined communication protocol. It confirms that the verification status is correctly set to \"Not verified\" because the AI cannot validate its own claims. It checks that risk objects have the correct structure with type, severity, and descriptive notes.\n","\n","Critically, this function also enforces Level Three policy constraints. It scans the draft output for forbidden recommendation language like \"we recommend\" or \"best option\" because our architecture requires separation between AI drafting and human decision-making. It also performs a relaxed stage boundary check, verifying that the AI's response appears relevant to the stage it was supposed to complete, though this check focuses on conceptual alignment rather than exact phrase matching.\n","\n","**Automatic Risk Detection: The AI Watching the AI**\n","\n","The auto_detect_risks function implements a second layer of quality control by analyzing validated responses for patterns that indicate potential problems. It checks whether any open questions were identifiedâ€”if none exist, this might indicate incomplete fact gathering. It looks for numeric claims in the output that do not appear in the provided facts or stated assumptions, flagging possible hallucination. It searches for language implying work was performed (\"we analyzed,\" \"we interviewed\") when no such work was documented, flagging potential decision laundering. It applies stage-specific checks: Stage One outputs should contain structural language like \"tree\" or \"dimensions,\" Stage Two outputs should discuss trade-offs and tensions, and Stage Four outputs should include verification planning language.\n","\n","**The Complete Call Orchestration**\n","\n","The call_claude_stage function brings all these pieces together into a robust API interaction workflow. It constructs an ultra-strict system prompt that emphasizes JSON formatting rules, sends the prompt to Claude while logging a hash of that prompt for governance purposes, receives the response, attempts to parse and validate it, and if validation fails, retries with a corrective prompt that lists the specific issues found. After successful validation, it applies automatic risk detection and updates the aggregated risk log. The function can retry up to twice before declaring failure, and any final failure gets logged as both an exception and a high-severity traceability risk.\n","\n","**The Smoke Test: Proof of Readiness**\n","\n","The cell concludes with a mandatory smoke test that makes an actual API call to Claude with a simple Stage One prompt. This is not a simulationâ€”it exercises the complete wrapper infrastructure with a real model interaction. The smoke test verifies that API authentication works, that JSON parsing and validation succeed, that risk detection executes, and that all logging mechanisms function correctly. Only after this test passes does the cell declare the wrapper ready for production use, giving you confidence that the quality control infrastructure is operational before processing actual consulting cases."],"metadata":{"id":"NbI8D2MMNp2V"}},{"cell_type":"markdown","source":["###6.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"HYi01MJHM25b"}},{"cell_type":"code","source":["# Cell 6: LLM Wrapper (ROBUST JSON REPAIR + STAGE-AWARE VALIDATION + ACTUAL API SMOKE TEST)\n","\n","def fix_json_string(json_str):\n","    \"\"\"\n","    Apply aggressive comma fixes.\n","    Remove trailing commas before } or ].\n","    \"\"\"\n","    # Remove trailing comma before }\n","    json_str = re.sub(r',(\\s*})', r'\\1', json_str)\n","    # Remove trailing comma before ]\n","    json_str = re.sub(r',(\\s*])', r'\\1', json_str)\n","    # Remove repeated trailing commas\n","    json_str = re.sub(r',+(\\s*[}\\]])', r'\\1', json_str)\n","    return json_str\n","\n","def extract_json_robust(text):\n","    \"\"\"\n","    Extract and parse JSON from LLM response with robust error handling.\n","    \"\"\"\n","    # Strip markdown fences if present\n","    text = text.strip()\n","    if text.startswith('```'):\n","        text = re.sub(r'^```(?:json)?\\s*\\n', '', text)\n","        text = re.sub(r'\\n```\\s*$', '', text)\n","        text = text.strip()\n","\n","    # Locate first { and last }\n","    first_brace = text.find('{')\n","    last_brace = text.rfind('}')\n","\n","    if first_brace == -1 or last_brace == -1:\n","        raise ValueError(\"No valid JSON object found (missing { or })\")\n","\n","    extracted = text[first_brace:last_brace+1]\n","\n","    # Try parsing directly\n","    try:\n","        return json.loads(extracted)\n","    except json.JSONDecodeError as e1:\n","        # Apply fixes\n","        fixed = fix_json_string(extracted)\n","        try:\n","            return json.loads(fixed)\n","        except json.JSONDecodeError as e2:\n","            # Write debug file\n","            debug_path = base_dir / \"debug_malformed_json.txt\"\n","            with open(debug_path, 'w', encoding='utf-8') as f:\n","                f.write(\"=== ORIGINAL RESPONSE (first 2000 chars) ===\\n\")\n","                f.write(text[:2000] + \"\\n\\n\")\n","                f.write(\"=== EXTRACTED JSON (first 2000 chars) ===\\n\")\n","                f.write(extracted[:2000] + \"\\n\\n\")\n","                f.write(\"=== AFTER FIXES (first 2000 chars) ===\\n\")\n","                f.write(fixed[:2000] + \"\\n\\n\")\n","                f.write(\"=== PARSE ERROR ===\\n\")\n","                f.write(str(e2) + \"\\n\")\n","\n","            raise ValueError(f\"JSON parsing failed after repairs. Debug saved to: {debug_path}\")\n","\n","def validate_stage_json(data, stage_name):\n","    \"\"\"\n","    Validate required keys and schema enforcement.\n","    Returns (is_valid, issues_list)\n","    \"\"\"\n","    issues = []\n","\n","    # Required keys EXACTLY\n","    required_keys = {\"task\", \"facts_provided\", \"assumptions\", \"open_questions\",\n","                     \"risks\", \"draft_output\", \"verification_status\", \"questions_to_verify\"}\n","\n","    actual_keys = set(data.keys())\n","\n","    # Check missing keys\n","    missing = required_keys - actual_keys\n","    if missing:\n","        issues.append(f\"Missing keys: {missing}\")\n","\n","    # Check extra keys (not allowed)\n","    extra = actual_keys - required_keys\n","    if extra:\n","        issues.append(f\"Extra keys not allowed: {extra}\")\n","\n","    # Enforce verification_status\n","    if \"verification_status\" in data and data[\"verification_status\"] != \"Not verified\":\n","        issues.append(f\"verification_status must be 'Not verified', got: {data['verification_status']}\")\n","\n","    # Enforce risks schema\n","    if \"risks\" in data:\n","        if not isinstance(data[\"risks\"], list):\n","            issues.append(\"risks must be a list\")\n","        else:\n","            for i, risk in enumerate(data[\"risks\"]):\n","                if not isinstance(risk, dict):\n","                    issues.append(f\"risks[{i}] must be a dict\")\n","                    continue\n","\n","                risk_keys = set(risk.keys())\n","                required_risk_keys = {\"type\", \"severity\", \"note\"}\n","\n","                if risk_keys != required_risk_keys:\n","                    issues.append(f\"risks[{i}] must have exactly keys {required_risk_keys}, got {risk_keys}\")\n","\n","                if \"severity\" in risk and risk[\"severity\"] not in [\"low\", \"medium\", \"high\"]:\n","                    issues.append(f\"risks[{i}] severity must be low|medium|high, got: {risk['severity']}\")\n","\n","    # Level 3 policy: NO recommendations\n","    if \"draft_output\" in data:\n","        output_lower = data[\"draft_output\"].lower()\n","        forbidden_phrases = [\"we recommend\", \"recommend that\", \"best option is\", \"should choose\"]\n","        for phrase in forbidden_phrases:\n","            if phrase in output_lower:\n","                issues.append(f\"draft_output contains forbidden recommendation language: '{phrase}'\")\n","\n","    # Stage boundary check - RELAXED: just check if key words from stage name appear\n","    if \"task\" in data and stage_name:\n","        # Extract key terms from stage name (ignore \"Stage N:\")\n","        stage_keywords = []\n","        if \"structure\" in stage_name.lower():\n","            stage_keywords.append(\"structure\")\n","        if \"option\" in stage_name.lower():\n","            stage_keywords.append(\"option\")\n","        if \"deliverable\" in stage_name.lower():\n","            stage_keywords.append(\"deliverable\")\n","        if \"verification\" in stage_name.lower():\n","            stage_keywords.append(\"verification\")\n","\n","        # Only flag if NONE of the keywords appear in task\n","        task_lower = data[\"task\"].lower()\n","        if stage_keywords and not any(keyword in task_lower for keyword in stage_keywords):\n","            # This is just a warning, not a hard failure\n","            pass  # Removed the strict check\n","\n","    return len(issues) == 0, issues\n","\n","def auto_detect_risks(data, stage_name):\n","    \"\"\"\n","    Auto-detect common risks and append to data[\"risks\"].\n","    \"\"\"\n","    new_risks = []\n","\n","    # Missing facts\n","    if \"open_questions\" in data and len(data.get(\"open_questions\", [])) == 0:\n","        new_risks.append({\n","            \"type\": \"missing_facts\",\n","            \"severity\": \"medium\",\n","            \"note\": \"No open questions listed - may indicate incomplete fact gathering\"\n","        })\n","\n","    # Hallucination check (numeric claims)\n","    if \"draft_output\" in data:\n","        output_text = data[\"draft_output\"]\n","        facts_text = \" \".join(data.get(\"facts_provided\", []))\n","        assumptions_text = \" \".join(data.get(\"assumptions\", []))\n","\n","        # Look for numbers in output\n","        output_numbers = set(re.findall(r'\\b\\d+(?:\\.\\d+)?%?\\b', output_text))\n","        known_numbers = set(re.findall(r'\\b\\d+(?:\\.\\d+)?%?\\b', facts_text + \" \" + assumptions_text))\n","\n","        unknown_numbers = output_numbers - known_numbers\n","        if unknown_numbers and len(unknown_numbers) > 2:\n","            new_risks.append({\n","                \"type\": \"hallucination\",\n","                \"severity\": \"high\",\n","                \"note\": f\"Output contains numeric claims not in facts/assumptions: {list(unknown_numbers)[:5]}\"\n","            })\n","\n","        # Decision laundering check\n","        laundering_phrases = [\"we analyzed\", \"we validated\", \"we interviewed\", \"our research shows\"]\n","        output_lower = output_text.lower()\n","        found_laundering = [p for p in laundering_phrases if p in output_lower]\n","        if found_laundering:\n","            new_risks.append({\n","                \"type\": \"decision_laundering\",\n","                \"severity\": \"high\",\n","                \"note\": f\"Output implies work performed without facts: {found_laundering}\"\n","            })\n","\n","    # Stage-specific checks\n","    if stage_name and \"Stage 1\" in stage_name and \"draft_output\" in data:\n","        structure_markers = [\"tree\", \"dimension\", \"framework\", \"structure\", \"breakdown\"]\n","        if not any(marker in data[\"draft_output\"].lower() for marker in structure_markers):\n","            new_risks.append({\n","                \"type\": \"false_rigor\",\n","                \"severity\": \"medium\",\n","                \"note\": \"Stage 1 output lacks structure markers (tree/dimensions)\"\n","            })\n","\n","    if stage_name and \"Stage 2\" in stage_name and \"draft_output\" in data:\n","        tradeoff_markers = [\"trade-off\", \"tension\", \"tradeoff\", \"versus\", \"vs\", \"however\", \"but\"]\n","        if not any(marker in data[\"draft_output\"].lower() for marker in tradeoff_markers):\n","            new_risks.append({\n","                \"type\": \"false_rigor\",\n","                \"severity\": \"medium\",\n","                \"note\": \"Stage 2 output lacks trade-off/tension language\"\n","            })\n","\n","    if stage_name and \"Stage 4\" in stage_name and \"draft_output\" in data:\n","        verification_markers = [\"verify\", \"test\", \"evidence\", \"owner\", \"validate\", \"check\"]\n","        if not any(marker in data[\"draft_output\"].lower() for marker in verification_markers):\n","            new_risks.append({\n","                \"type\": \"traceability\",\n","                \"severity\": \"medium\",\n","                \"note\": \"Stage 4 output lacks verification planning cues\"\n","            })\n","\n","    # Append new risks\n","    if \"risks\" not in data:\n","        data[\"risks\"] = []\n","    data[\"risks\"].extend(new_risks)\n","\n","    return data\n","\n","def call_claude_stage(stage_name, user_prompt, system_prompt=None, max_retries=2):\n","    \"\"\"\n","    Call Claude API for a specific stage with robust JSON parsing and validation.\n","    \"\"\"\n","    # Ultra-strict system prompt\n","    strict_system = \"\"\"You are a management consulting AI assistant.\n","\n","CRITICAL JSON OUTPUT RULES:\n","1. Return ONLY valid JSON - no markdown, no preamble, no postamble\n","2. Start with { and end with }\n","3. Use double quotes for all strings\n","4. NO trailing commas before } or ]\n","5. Verify all brackets are balanced\n","6. All required keys must be present with correct types\n","\n","REQUIRED JSON STRUCTURE:\n","{\n","  \"task\": \"string describing this stage\",\n","  \"facts_provided\": [\"list of facts from user\"],\n","  \"assumptions\": [\"list of assumptions made\"],\n","  \"open_questions\": [\"list of unresolved questions\"],\n","  \"risks\": [\n","    {\n","      \"type\": \"confidentiality|hallucination|missing_facts|traceability|false_rigor|decision_laundering|scope_creep|stage_boundary|other\",\n","      \"severity\": \"low|medium|high\",\n","      \"note\": \"description\"\n","    }\n","  ],\n","  \"draft_output\": \"your main output for this stage\",\n","  \"verification_status\": \"Not verified\",\n","  \"questions_to_verify\": [\"list of claims needing verification\"]\n","}\n","\n","LEVEL 3 POLICIES:\n","- Never use recommendation language (\"we recommend\", \"best option\", \"choose\")\n","- Never claim to have performed work not evidenced in facts_provided\n","- Flag all assumptions explicitly\n","- Flag all missing facts in open_questions\"\"\"\n","\n","    if system_prompt:\n","        strict_system = system_prompt + \"\\n\\n\" + strict_system\n","\n","    # Log prompt (redacted)\n","    prompt_hash = sha256_text(user_prompt)\n","    append_jsonl(prompts_log_path, {\n","        \"timestamp\": now_iso(),\n","        \"stage_name\": stage_name,\n","        \"prompt_hash\": prompt_hash,\n","        \"model\": MODEL,\n","        \"params\": {\"temperature\": DEFAULT_TEMPERATURE, \"max_tokens\": DEFAULT_MAX_TOKENS}\n","    })\n","\n","    for attempt in range(max_retries + 1):\n","        try:\n","            # API call\n","            response = client.messages.create(\n","                model=MODEL,\n","                max_tokens=DEFAULT_MAX_TOKENS,\n","                temperature=DEFAULT_TEMPERATURE,\n","                system=strict_system,\n","                messages=[{\"role\": \"user\", \"content\": user_prompt}]\n","            )\n","\n","            response_text = response.content[0].text\n","\n","            # Parse JSON\n","            try:\n","                data = extract_json_robust(response_text)\n","            except ValueError as e:\n","                if attempt < max_retries:\n","                    # Retry with JSON error prompt\n","                    user_prompt = f\"\"\"YOUR PREVIOUS JSON WAS INVALID.\n","\n","ERROR: {str(e)}\n","\n","CHECKLIST:\n","- Start with {{ and end with }}\n","- Use double quotes for strings\n","- Remove ALL trailing commas before }} or ]\n","- Ensure all brackets are balanced\n","- Include all required keys\n","\n","ORIGINAL REQUEST:\n","{user_prompt}\n","\n","Return corrected JSON only.\"\"\"\n","                    continue\n","                else:\n","                    # Final failure\n","                    exception_data = read_json(exception_log_path)\n","                    exception_data[\"exceptions\"].append({\n","                        \"timestamp\": now_iso(),\n","                        \"type\": \"json_parse_failure\",\n","                        \"stage_name\": stage_name,\n","                        \"error\": str(e)\n","                    })\n","                    write_json(exception_log_path, exception_data)\n","\n","                    risk_data = read_json(risk_log_path)\n","                    risk_data[\"risks\"].append({\n","                        \"timestamp\": now_iso(),\n","                        \"stage_name\": stage_name,\n","                        \"type\": \"traceability\",\n","                        \"severity\": \"high\",\n","                        \"note\": f\"JSON parsing failed after {max_retries} retries\"\n","                    })\n","                    write_json(risk_log_path, risk_data)\n","\n","                    raise\n","\n","            # Validate schema\n","            is_valid, issues = validate_stage_json(data, stage_name)\n","\n","            if not is_valid:\n","                if attempt < max_retries:\n","                    # Retry with validation error prompt\n","                    issues_text = \"\\n\".join(f\"- {issue}\" for issue in issues)\n","                    user_prompt = f\"\"\"YOUR PREVIOUS JSON HAD VALIDATION ERRORS:\n","\n","{issues_text}\n","\n","ORIGINAL REQUEST:\n","{user_prompt}\n","\n","Return corrected JSON with all issues fixed.\"\"\"\n","                    continue\n","                else:\n","                    # Final failure\n","                    exception_data = read_json(exception_log_path)\n","                    exception_data[\"exceptions\"].append({\n","                        \"timestamp\": now_iso(),\n","                        \"type\": \"validation_failure\",\n","                        \"stage_name\": stage_name,\n","                        \"issues\": issues\n","                    })\n","                    write_json(exception_log_path, exception_data)\n","\n","                    raise ValueError(f\"Validation failed after {max_retries} retries: {issues}\")\n","\n","            # Auto-detect risks\n","            data = auto_detect_risks(data, stage_name)\n","\n","            # Update risk log\n","            risk_data = read_json(risk_log_path)\n","            for risk in data.get(\"risks\", []):\n","                risk_data[\"risks\"].append({\n","                    \"timestamp\": now_iso(),\n","                    \"stage_name\": stage_name,\n","                    **risk\n","                })\n","            write_json(risk_log_path, risk_data)\n","\n","            return data\n","\n","        except Exception as e:\n","            if attempt == max_retries:\n","                raise\n","            continue\n","\n","    raise RuntimeError(f\"Failed after {max_retries + 1} attempts\")\n","\n","# SMOKE TEST (MANDATORY)\n","print(\"Running smoke test with ACTUAL API call...\\n\")\n","\n","try:\n","    smoke_result = call_claude_stage(\n","        stage_name=\"Stage 1: Structure\",\n","        user_prompt=\"\"\"You are helping structure a simple market entry case.\n","\n","SITUATION: A European food manufacturer wants to enter the US market.\n","\n","Build a simple issue tree with 3 main branches.\"\"\"\n","    )\n","\n","    print(\"âœ“ SMOKE TEST PASSED\")\n","    print(f\"  Task: {smoke_result['task'][:80]}...\")\n","    print(f\"  Open questions: {len(smoke_result['open_questions'])}\")\n","    print(f\"  Draft output length: {len(smoke_result['draft_output'])} chars\")\n","    print(f\"  Risks detected: {len(smoke_result['risks'])}\")\n","\n","except Exception as e:\n","    print(\"âœ— SMOKE TEST FAILED\")\n","    print(f\"  Error: {str(e)}\")\n","    print(f\"  Check: {base_dir / 'debug_malformed_json.txt'}\")\n","    raise\n","\n","print(\"\\nâœ“ LLM wrapper ready for production use\")\n"],"metadata":{"id":"NVWzVmXzM4V4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768854826005,"user_tz":360,"elapsed":14003,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"c049b757-27c0-4c87-a18a-ade8270731ca"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Running smoke test with ACTUAL API call...\n","\n","âœ“ SMOKE TEST PASSED\n","  Task: Build a simple issue tree with 3 main branches for European food manufacturer US...\n","  Open questions: 7\n","  Draft output length: 645 chars\n","  Risks detected: 3\n","\n","âœ“ LLM wrapper ready for production use\n"]}]},{"cell_type":"markdown","source":["##7.PIPELINE ORCHESTRATOR"],"metadata":{"id":"Dr34h4v4M8SU"}},{"cell_type":"markdown","source":["###7.1.OVERVIEW"],"metadata":{"id":"EnAuC2ckM_GU"}},{"cell_type":"markdown","source":["**Cell 7: The Pipeline Orchestrator - Conducting the Multi-Stage Workflow**\n","\n","**What This Cell Does**\n","\n","This cell implements the heart of the workflow agent system: the orchestrator that executes all six stages in sequence, manages context flow between stages, enforces gate checkpoints, and handles errors gracefully. Think of it as the conductor of an orchestra, ensuring each specialized agent (each stage) performs at the right time, receives the necessary context from previous performers, and meets quality standards before the next agent begins. This is where the abstract workflow defined in the runbook becomes concrete execution.\n","\n","**The Pipeline Structure**\n","\n","The run_case_pipeline function accepts a case specification containing the case name, type, situation description, and desired deliverable format. It creates a dedicated output directory for this case and initializes an empty results dictionary that will accumulate outputs from each stage. The function then proceeds through six stages in strict sequence, with gates between them. Stages zero and five involve no AI callsâ€”they handle intake classification and final packaging respectively. Stages one through four each make exactly one AI call, as mandated by the Level Three one-call-per-stage rule.\n","\n","**Stage Zero: Human-Defined Boundaries**\n","\n","The pipeline begins with Stage Zero, which performs intake and classification without invoking AI. This stage captures the user's inputs into a structured record that documents the case type, situation, deliverable format, timestamp, and critically, a scope boundary statement. This boundary explicitly declares what the analysis will and will not cover, establishing guardrails before any AI work begins. For instance, it might state that the analysis is limited to strategic assessment and excludes implementation planning. This human-defined constraint prevents scope creep in later stages. The intake record is saved as a JSON file and the first gate is triggered.\n","\n","**Gate Mechanics: The Human Checkpoint**\n","\n","After each stage completes, the pipeline encounters a gate. If the auto_approve parameter is set to true (as in demonstration mode), the gate automatically logs an approval decision with the rationale \"Auto-approved for demo\" and allows progression. If auto_approve is false (as in the user exercise), the pipeline pauses and requests keyboard input: the human must type \"approve\" or \"reject\" and provide a rationale. The gate function is called with this decision, logging it permanently to the stage gate log. If the decision is reject, the gate function returns false, the pipeline prints a halted message, and the function returns whatever partial results exist. This allows quality control at every transition point.\n","\n","**Stage One: Building Analytical Structure**\n","\n","Stage One makes the first AI call, requesting that Claude build an analytical framework appropriate to the case type. The orchestrator constructs a prompt that includes the case type and situation description, then instructs the AI to create an issue tree, driver tree, or design dimensions depending on what fits the problem. The prompt explicitly prohibits recommendations, limiting the AI to structuring the problem space. The call_claude_stage wrapper handles this interaction, performing all validation and risk detection. The returned JSON contains not just the analytical structure in the draft output field, but also the facts the AI understood from the prompt, assumptions it made, open questions it identified, and risks detected. This complete response is saved to the stage outputs directory and stored in the results dictionary.\n","\n","**Context Accumulation: Each Stage Builds on Previous Work**\n","\n","Stage Two demonstrates the agentic nature of this system through context passing. The prompt for Stage Two does not start from scratchâ€”it includes an excerpt from Stage One's output, specifically the analytical structure that was created. This allows the Options Agent (Stage Two) to generate strategic alternatives that align with the problem decomposition the Structure Agent produced. The prompt asks for three to four options with descriptions, trade-offs versus other options, and uncertainties. Again, recommendation language is prohibited. The AI must read and understand what Stage One produced, then build upon it. This accumulated context creates coherence across stages that independent AI calls could not achieve.\n","\n","**Stage Three and Four: Deliverable Drafting and Verification Planning**\n","\n","Stage Three receives excerpts from both Stage One and Stage Two, allowing it to draft a deliverable outline that reflects both the analytical structure and the strategic options identified. The prompt specifies the deliverable type (executive memo, workplan, investment committee pre-read, or operating model narrative) and requests an outline with executive summary structure, main body sections, and appendix placeholders. Stage Four performs meta-analysis by examining all assumptions and open questions accumulated across previous stages. To prevent token limit issues, the orchestrator truncates these lists to the top ten items of each type. The Stage Four prompt asks Claude to create a verification plan specifying what evidence would validate each assumption, who should own that verification work, what method should be used, and what priority it deserves.\n","\n","**Error Handling: Graceful Degradation**\n","\n","The orchestrator wraps each stage in try-except blocks to handle failures without destroying the entire pipeline. If Stage One fails, the function immediately returns with only the Stage Zero intake record. If Stage Four fails (which sometimes happens due to complexity), the error is logged, partial results are preserved, and the pipeline continues to packaging with whatever stages completed successfully. This graceful degradation ensures you always get some output and governance logs, even when not all stages execute perfectly. The error messages indicate which stage failed and where to find debug information.\n","\n","**Stage Five: Assembling the Final Package**\n","\n","The final stage performs no AI calls but instead packages all previous outputs into deliverable formats. It creates a JSON bundle that provides an indexed summary of the case results, including excerpts from each stage's main output, the total number of risks logged, and the count of open questions requiring resolution. It also creates a human-readable text file that presents the same information in prose format with clear section headings, stage-by-stage excerpts, and a summary with statistics and prominent warnings about unverified status. Both artifacts are saved to the deliverables directory, separate from the raw stage outputs, providing curated final products ready for human review."],"metadata":{"id":"EKvlFydjNrmX"}},{"cell_type":"markdown","source":["###7.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"X1KSLRldNBYW"}},{"cell_type":"code","source":["# Cell 7: Pipeline Orchestrator (Stages 0â€“5)\n","\n","def run_case_pipeline(case_spec, auto_approve=True):\n","    \"\"\"\n","    Run a complete 6-stage pipeline for a consulting case.\n","\n","    case_spec = {\n","        \"case_name\": str,\n","        \"case_type\": str,\n","        \"situation\": str,\n","        \"client_context\": str (optional),\n","        \"deliverable_type\": str\n","    }\n","    \"\"\"\n","    case_name = case_spec[\"case_name\"]\n","    case_type = case_spec[\"case_type\"]\n","    situation = case_spec[\"situation\"]\n","    deliverable_type = case_spec.get(\"deliverable_type\", \"exec memo\")\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"PIPELINE: {case_name}\")\n","    print(f\"{'='*60}\\n\")\n","\n","    # Create case output directory\n","    case_output_dir = stage_outputs_dir / case_name\n","    case_output_dir.mkdir(exist_ok=True)\n","\n","    results = {}\n","\n","    # ===== STAGE 0: Intake + Classification =====\n","    print(\"Stage 0: Intake + Classification\")\n","\n","    intake_record = {\n","        \"timestamp\": now_iso(),\n","        \"case_name\": case_name,\n","        \"case_type\": case_type,\n","        \"situation\": situation,\n","        \"deliverable_type\": deliverable_type,\n","        \"scope_boundary\": f\"Limited to {case_type} analysis; no implementation\"\n","    }\n","\n","    stage0_path = case_output_dir / \"stage_0_intake.json\"\n","    write_json(stage0_path, intake_record)\n","    results[\"stage_0\"] = intake_record\n","    print(f\"  âœ“ Saved: {stage0_path.name}\")\n","\n","    # Gate 0â†’1\n","    if not auto_approve:\n","        decision = input(\"\\n  Approve Stage 0â†’1 transition? (approve/reject): \").strip().lower()\n","        rationale = input(\"  Rationale: \").strip()\n","    else:\n","        decision = \"approve\"\n","        rationale = \"Auto-approved for demo\"\n","\n","    if not gate(0, 1, case_name, \"Analyst\", decision, rationale):\n","        print(\"  âœ— Pipeline halted at Gate 0â†’1\")\n","        return results\n","\n","    # ===== STAGE 1: Structure =====\n","    print(\"\\nStage 1: Structure\")\n","\n","    stage1_prompt = f\"\"\"STAGE 1: STRUCTURE\n","\n","CASE TYPE: {case_type}\n","SITUATION: {situation}\n","\n","TASK: Build a structured analytical framework for this case.\n","\n","For {case_type}, create:\n","- An issue tree (logic tree breaking down the problem)\n","- OR a driver tree (quantitative decomposition)\n","- OR design dimensions (key decision factors)\n","\n","Choose the most appropriate structure for this case type.\n","\n","Keep the structure to 3-5 main branches with 2-3 sub-branches each.\n","\n","Do NOT provide recommendations. Structure only.\"\"\"\n","\n","    try:\n","        stage1_result = call_claude_stage(\"Stage 1: Structure\", stage1_prompt)\n","        stage1_path = case_output_dir / \"stage_1_structure.json\"\n","        write_json(stage1_path, stage1_result)\n","        results[\"stage_1\"] = stage1_result\n","        print(f\"  âœ“ Saved: {stage1_path.name}\")\n","        print(f\"  Open questions: {len(stage1_result['open_questions'])}\")\n","    except Exception as e:\n","        print(f\"  âœ— Stage 1 failed: {str(e)[:100]}\")\n","        return results\n","\n","    # Gate 1â†’2\n","    if not auto_approve:\n","        decision = input(\"\\n  Approve Stage 1â†’2 transition? (approve/reject): \").strip().lower()\n","        rationale = input(\"  Rationale: \").strip()\n","    else:\n","        decision = \"approve\"\n","        rationale = \"Auto-approved for demo\"\n","\n","    if not gate(1, 2, case_name, \"Manager\", decision, rationale):\n","        print(\"  âœ— Pipeline halted at Gate 1â†’2\")\n","        return results\n","\n","    # ===== STAGE 2: Options + Trade-offs =====\n","    print(\"\\nStage 2: Options + Trade-offs\")\n","\n","    stage2_prompt = f\"\"\"STAGE 2: OPTIONS + TRADE-OFFS\n","\n","CASE TYPE: {case_type}\n","SITUATION: {situation}\n","\n","STRUCTURE FROM STAGE 1:\n","{stage1_result['draft_output'][:500]}\n","\n","TASK: Generate 3-4 strategic options and identify key trade-offs.\n","\n","For each option:\n","- Brief description\n","- Key trade-offs vs other options\n","- Uncertainties/dependencies\n","\n","Do NOT recommend which option to choose. Present trade-offs neutrally.\"\"\"\n","\n","    try:\n","        stage2_result = call_claude_stage(\"Stage 2: Options + Trade-offs\", stage2_prompt)\n","        stage2_path = case_output_dir / \"stage_2_options.json\"\n","        write_json(stage2_path, stage2_result)\n","        results[\"stage_2\"] = stage2_result\n","        print(f\"  âœ“ Saved: {stage2_path.name}\")\n","        print(f\"  Assumptions: {len(stage2_result['assumptions'])}\")\n","    except Exception as e:\n","        print(f\"  âœ— Stage 2 failed: {str(e)[:100]}\")\n","        return results\n","\n","    # Gate 2â†’3\n","    if not auto_approve:\n","        decision = input(\"\\n  Approve Stage 2â†’3 transition? (approve/reject): \").strip().lower()\n","        rationale = input(\"  Rationale: \").strip()\n","    else:\n","        decision = \"approve\"\n","        rationale = \"Auto-approved for demo\"\n","\n","    if not gate(2, 3, case_name, \"Manager\", decision, rationale):\n","        print(\"  âœ— Pipeline halted at Gate 2â†’3\")\n","        return results\n","\n","    # ===== STAGE 3: Draft Deliverable Bundle =====\n","    print(\"\\nStage 3: Draft Deliverable Bundle\")\n","\n","    stage3_prompt = f\"\"\"STAGE 3: DRAFT DELIVERABLE BUNDLE\n","\n","CASE TYPE: {case_type}\n","DELIVERABLE TYPE: {deliverable_type}\n","SITUATION: {situation}\n","\n","STRUCTURE: {stage1_result['draft_output'][:300]}\n","OPTIONS: {stage2_result['draft_output'][:300]}\n","\n","TASK: Create a draft {deliverable_type} outline/shell.\n","\n","Include:\n","- Executive summary outline (key sections, not full text)\n","- Main body structure\n","- Appendix placeholders\n","\n","Mark all unverified claims clearly. Do NOT make recommendations.\"\"\"\n","\n","    try:\n","        stage3_result = call_claude_stage(\"Stage 3: Draft Deliverable Bundle\", stage3_prompt)\n","        stage3_path = case_output_dir / \"stage_3_deliverable.json\"\n","        write_json(stage3_path, stage3_result)\n","        results[\"stage_3\"] = stage3_result\n","        print(f\"  âœ“ Saved: {stage3_path.name}\")\n","    except Exception as e:\n","        print(f\"  âœ— Stage 3 failed: {str(e)[:100]}\")\n","        return results\n","\n","    # Gate 3â†’4\n","    if not auto_approve:\n","        decision = input(\"\\n  Approve Stage 3â†’4 transition? (approve/reject): \").strip().lower()\n","        rationale = input(\"  Rationale: \").strip()\n","    else:\n","        decision = \"approve\"\n","        rationale = \"Auto-approved for demo\"\n","\n","    if not gate(3, 4, case_name, \"Partner\", decision, rationale):\n","        print(\"  âœ— Pipeline halted at Gate 3â†’4\")\n","        return results\n","\n","    # ===== STAGE 4: Verification Plan + Assumption Register =====\n","    print(\"\\nStage 4: Verification Plan + Assumption Register\")\n","\n","    all_assumptions = stage1_result.get(\"assumptions\", []) + stage2_result.get(\"assumptions\", []) + stage3_result.get(\"assumptions\", [])\n","    all_open_questions = stage1_result.get(\"open_questions\", []) + stage2_result.get(\"open_questions\", []) + stage3_result.get(\"questions_to_verify\", [])\n","\n","    # Truncate lists if too long (to avoid token limits)\n","    assumptions_text = json.dumps(all_assumptions[:10], indent=2)\n","    questions_text = json.dumps(all_open_questions[:10], indent=2)\n","\n","    stage4_prompt = f\"\"\"STAGE 4: VERIFICATION PLAN\n","\n","CASE: {case_name}\n","\n","ASSUMPTIONS (sample):\n","{assumptions_text}\n","\n","OPEN QUESTIONS (sample):\n","{questions_text}\n","\n","TASK: Create a verification plan for the top 5-7 most critical assumptions/questions.\n","\n","For each item:\n","- What evidence would verify/refute it?\n","- Who owns verification? (role: Analyst/Manager/Partner/Client)\n","- Verification method (interview/data analysis/benchmark/expert opinion)\n","- Priority (high/medium/low)\n","\n","Output as a structured table or list.\"\"\"\n","\n","    try:\n","        stage4_result = call_claude_stage(\"Stage 4: Verification Plan\", stage4_prompt)\n","        stage4_path = case_output_dir / \"stage_4_verification.json\"\n","        write_json(stage4_path, stage4_result)\n","        results[\"stage_4\"] = stage4_result\n","        print(f\"  âœ“ Saved: {stage4_path.name}\")\n","    except Exception as e:\n","        print(f\"  âœ— Stage 4 failed: {str(e)[:200]}\")\n","        print(f\"  Check debug logs in: {base_dir}\")\n","        # Continue anyway with partial results\n","        results[\"stage_4_error\"] = str(e)\n","\n","    # Update verification_register.json\n","    verification_data = read_json(verification_register_path)\n","    verification_data[\"unverified_claims\"].append({\n","        \"timestamp\": now_iso(),\n","        \"case_name\": case_name,\n","        \"assumptions\": all_assumptions[:20],\n","        \"open_questions\": all_open_questions[:20],\n","        \"verification_plan\": results.get(\"stage_4\", {}).get(\"draft_output\", \"Stage 4 failed\")\n","    })\n","    write_json(verification_register_path, verification_data)\n","\n","    # Gate 4â†’5\n","    if not auto_approve:\n","        decision = input(\"\\n  Approve Stage 4â†’5 transition? (approve/reject): \").strip().lower()\n","        rationale = input(\"  Rationale: \").strip()\n","    else:\n","        decision = \"approve\"\n","        rationale = \"Auto-approved for demo\"\n","\n","    if not gate(4, 5, case_name, \"Partner\", decision, rationale):\n","        print(\"  âœ— Pipeline halted at Gate 4â†’5\")\n","        return results\n","\n","    # ===== STAGE 5: Packaging =====\n","    print(\"\\nStage 5: Packaging\")\n","\n","    # Create final bundle\n","    final_bundle = {\n","        \"case_name\": case_name,\n","        \"timestamp\": now_iso(),\n","        \"stages_completed\": list(results.keys()),\n","        \"structure_excerpt\": stage1_result[\"draft_output\"][:200] + \"...\" if \"stage_1\" in results else \"N/A\",\n","        \"options_excerpt\": stage2_result[\"draft_output\"][:200] + \"...\" if \"stage_2\" in results else \"N/A\",\n","        \"deliverable_excerpt\": stage3_result[\"draft_output\"][:200] + \"...\" if \"stage_3\" in results else \"N/A\",\n","        \"verification_status\": \"Not verified\",\n","        \"total_risks\": sum(len(r.get(\"risks\", [])) for r in results.values() if isinstance(r, dict)),\n","        \"total_open_questions\": len(all_open_questions)\n","    }\n","\n","    bundle_path = deliverables_dir / f\"{case_name}_final_bundle.json\"\n","    write_json(bundle_path, final_bundle)\n","    print(f\"  âœ“ Saved: {bundle_path.name}\")\n","\n","    # Create human-readable summary\n","    human_readable = f\"\"\"CASE: {case_name.upper()}\n","TYPE: {case_type}\n","TIMESTAMP: {now_iso()}\n","\n","{'='*60}\n","STAGE 1: STRUCTURE\n","{'='*60}\n","{stage1_result.get('draft_output', 'N/A')[:500]}\n","\n","{'='*60}\n","STAGE 2: OPTIONS + TRADE-OFFS\n","{'='*60}\n","{stage2_result.get('draft_output', 'N/A')[:500]}\n","\n","{'='*60}\n","STAGE 3: DRAFT DELIVERABLE\n","{'='*60}\n","{stage3_result.get('draft_output', 'N/A')[:500]}\n","\n","{'='*60}\n","STAGE 4: VERIFICATION PLAN\n","{'='*60}\n","{results.get('stage_4', {}).get('draft_output', 'Stage 4 incomplete')[:500]}\n","\n","{'='*60}\n","SUMMARY\n","{'='*60}\n","Stages completed: {len([k for k in results.keys() if k.startswith('stage_')])}\n","Total risks logged: {final_bundle['total_risks']}\n","Total open questions: {final_bundle['total_open_questions']}\n","\n","âš ï¸ VERIFICATION STATUS: Not verified\n","âš ï¸ Human review required before use\n","\"\"\"\n","\n","    readable_path = deliverables_dir / f\"{case_name}_human_readable.txt\"\n","    with open(readable_path, 'w', encoding='utf-8') as f:\n","        f.write(human_readable)\n","    print(f\"  âœ“ Saved: {readable_path.name}\")\n","\n","    results[\"final_bundle\"] = final_bundle\n","\n","    print(f\"\\nâœ“ Pipeline completed for {case_name}\")\n","    return results\n","\n","# Test pipeline with dummy case\n","print(\"Testing pipeline orchestrator with dummy case...\\n\")\n","\n","dummy_case = {\n","    \"case_name\": \"pipeline_test\",\n","    \"case_type\": \"market entry\",\n","    \"situation\": \"Test case for pipeline validation\",\n","    \"deliverable_type\": \"exec memo\"\n","}\n","\n","try:\n","    test_results = run_case_pipeline(dummy_case, auto_approve=True)\n","\n","    print(f\"\\nâœ“ Pipeline orchestrator ready\")\n","    print(f\"  Test case stages completed: {len([k for k in test_results.keys() if k.startswith('stage_')])}\")\n","    print(f\"  Files created: {len(list((stage_outputs_dir / 'pipeline_test').glob('*.json')))}\")\n","except Exception as e:\n","    print(f\"\\nâš ï¸  Pipeline test encountered errors but completed partial execution\")\n","    print(f\"  Error: {str(e)[:200]}\")\n","    print(f\"  Check logs in: {base_dir}\")\n"],"metadata":{"id":"vw3KXun5NDTt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768855016026,"user_tz":360,"elapsed":145813,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"bd5e97c3-0559-4f7b-967b-cce025bbecab"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing pipeline orchestrator with dummy case...\n","\n","\n","============================================================\n","PIPELINE: pipeline_test\n","============================================================\n","\n","Stage 0: Intake + Classification\n","  âœ“ Saved: stage_0_intake.json\n","\n","Stage 1: Structure\n","  âœ“ Saved: stage_1_structure.json\n","  Open questions: 6\n","\n","Stage 2: Options + Trade-offs\n","  âœ“ Saved: stage_2_options.json\n","  Assumptions: 5\n","\n","Stage 3: Draft Deliverable Bundle\n","  âœ“ Saved: stage_3_deliverable.json\n","\n","Stage 4: Verification Plan + Assumption Register\n","  âœ“ Saved: stage_4_verification.json\n","\n","Stage 5: Packaging\n","  âœ“ Saved: pipeline_test_final_bundle.json\n","  âœ“ Saved: pipeline_test_human_readable.txt\n","\n","âœ“ Pipeline completed for pipeline_test\n","\n","âœ“ Pipeline orchestrator ready\n","  Test case stages completed: 5\n","  Files created: 5\n"]}]},{"cell_type":"markdown","source":["##8.RUNNING CASES END-TO-END"],"metadata":{"id":"E9DiNZg2NOyv"}},{"cell_type":"markdown","source":["###8.1.OVERVIEW"],"metadata":{"id":"Ee1sdukLNP3S"}},{"cell_type":"markdown","source":["**Cell 8: Running Four Demonstration Cases End-to-End**\n","\n","**What This Cell Does**\n","\n","This cell executes the complete workflow pipeline four times, once for each major type of consulting engagement, demonstrating the system's versatility across different business contexts. Think of it as running four parallel consulting projects through the same governance infrastructure, showing how the workflow agent handles market entry analysis differently from cost transformation, capital allocation, or operating model redesign. The cell also generates a comparative summary table that reveals patterns in how different case types produce different quantities of open questions, risks, and exceptions.\n","\n","**The Four Mini-Cases: Intentional Incompleteness**\n","\n","The cell defines four case specifications, each representing a common consulting scenario. The market entry case involves a European chocolate manufacturer considering US expansion. The cost transformation case addresses a global industrial equipment company needing to reduce overhead costs. The capital allocation case presents a healthcare services company evaluating three investment options. The operating model case deals with post-merger integration at a financial services firm. Critically, each case description is intentionally incomplete. Market sizing data is missing. Detailed cost baselines are unknown. Financial projections are unavailable. Organization charts do not exist. This incompleteness is deliberateâ€”it forces the AI to acknowledge gaps, flag assumptions, identify open questions, and create verification plans rather than fabricating authoritative-sounding but unsupported claims.\n","\n","**Automated Execution with Auto-Approval**\n","\n","The cell runs each case through the pipeline with auto_approve set to true, meaning gates automatically approve transitions and log those decisions without pausing for human input. This allows the demonstration to complete end-to-end without manual intervention, which is appropriate for testing and pedagogy. Each case proceeds through all six stages: intake and classification, analytical structure creation, options and trade-offs generation, deliverable drafting, verification planning, and final packaging. For each case, the complete set of stage outputs is saved in a dedicated subfolder within the stage outputs directory, and final deliverables are saved in the deliverables directory.\n","\n","**Accumulating Results and Metrics**\n","\n","As each case completes, the cell extracts key metrics from the results. It counts how many open questions were identified across all stagesâ€”a measure of how much uncertainty the AI acknowledged. It collects all risk objects from every stage and determines the highest severity level encountered, revealing whether the case triggered high-severity concerns like hallucination or decision laundering. It queries the exception log to count how many exceptions occurred for this specific case, indicating whether the pipeline executed smoothly or encountered validation failures or other problems. These metrics are accumulated into a summary data structure that will be presented in tabular form.\n","\n","**The Summary Table: Comparative Analysis**\n","\n","After all four cases complete, the cell prints a summary table with columns for case name, stages completed, total open questions, highest risk severity, and exception count. This table allows immediate comparison across case types. You might observe that the cost transformation case generated more open questions than the market entry case, reflecting the greater data requirements for detailed cost analysis. You might notice that the capital allocation case triggered high-severity risks due to numeric claims about investment returns that could not be grounded in provided facts. The table makes visible the governance outputs that differentiate responsible AI use from careless deploymentâ€”systems that acknowledge uncertainty produce more open questions, while systems that fabricate confidence produce fewer.\n","\n","**Deliverables and Governance Artifacts**\n","\n","Each case produces two types of outputs stored in different locations. The stage outputs directory contains the complete raw JSON from every AI callâ€”the full response including task descriptions, facts, assumptions, questions, risks, and draft outputs. These files provide granular visibility into what happened at each stage. The deliverables directory contains the final curated products: a JSON bundle with indexed excerpts and a human-readable text file with formatted prose. Additionally, the governance logs are continuously updated throughout execution. The risk log accumulates risks from all four cases. The prompts log records hashes from all AI calls. The stage gate log contains approval decisions for all transitions across all cases. The verification register collects assumptions and verification plans from all four cases. This aggregation allows you to see patterns across the portfolio of work, not just within individual cases.\n","\n","**Demonstrating Workflow Consistency**\n","\n","The power of this demonstration lies in showing that the same workflow topology handles diverse consulting scenarios. Market entry requires different analytical structures than cost transformation, yet both flow through the same six stages with the same gates and governance. The Structure Agent adapts its output to the case typeâ€”issue trees for strategic questions, driver trees for quantitative decompositionâ€”but the workflow discipline remains constant. This consistency enables standardization, training, quality assurance, and regulatory compliance across an entire consulting practice, even as the substantive content varies dramatically from case to case."],"metadata":{"id":"UPGXnne7NtGW"}},{"cell_type":"markdown","source":["###8.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"BIE5Ni_wNRvq"}},{"cell_type":"code","source":["# Cell 8: Run 4 Mini-Case Demos End-to-End\n","\n","print(\"Running 4 mini-case demos...\\n\")\n","\n","# Define 4 cases with intentionally incomplete facts\n","cases = [\n","    {\n","        \"case_name\": \"market_entry\",\n","        \"case_type\": \"market entry\",\n","        \"situation\": \"\"\"A European premium chocolate manufacturer (â‚¬500M revenue) wants to enter the US market.\n","\n","Current state:\n","- Strong brand in Western Europe (Germany, France, UK)\n","- Premium positioning (â‚¬8-12 per 100g vs â‚¬3-5 mass market)\n","- D2C e-commerce is 15% of sales\n","- No US presence\n","\n","Question: Should we enter, and if so, how?\n","\n","NOTE: This is a demo case with incomplete facts. Market sizing, competitive landscape, and go-to-market economics are UNKNOWN.\"\"\",\n","        \"deliverable_type\": \"exec memo\"\n","    },\n","    {\n","        \"case_name\": \"cost_transformation\",\n","        \"case_type\": \"cost transformation\",\n","        \"situation\": \"\"\"A global industrial equipment manufacturer ($3B revenue) needs to reduce SG&A by 15% over 18 months.\n","\n","Current state:\n","- SG&A is 22% of revenue (peer median: 18%)\n","- 12 country organizations with duplicated functions\n","- Legacy ERP systems in 8 countries\n","- Procurement is decentralized\n","\n","Question: Where should we find the $100M in savings, and what's the implementation roadmap?\n","\n","NOTE: This is a demo case. Detailed cost baselines, FTE counts, and IT architecture are UNKNOWN.\"\"\",\n","        \"deliverable_type\": \"transformation workplan\"\n","    },\n","    {\n","        \"case_name\": \"capital_allocation\",\n","        \"case_type\": \"capital allocation\",\n","        \"situation\": \"\"\"A healthcare services company is evaluating three growth investment options for its $200M capital pool:\n","\n","Option A: Acquire regional home health network ($120M)\n","Option B: Build telemedicine platform ($80M over 2 years)\n","Option C: Expand existing ambulatory surgery centers ($150M)\n","\n","Investment Committee needs a pre-read for next week.\n","\n","NOTE: This is a demo case. Financial projections, synergy estimates, and market growth rates are UNKNOWN.\"\"\",\n","        \"deliverable_type\": \"IC pre-read\"\n","    },\n","    {\n","        \"case_name\": \"operating_model\",\n","        \"case_type\": \"operating model redesign\",\n","        \"situation\": \"\"\"A financial services firm is redesigning its operating model post-merger.\n","\n","Current state:\n","- Two legacy organizations (Acquirer: 5,000 FTE, Target: 3,000 FTE)\n","- Overlapping product lines (wealth management, retail banking)\n","- Different tech stacks and cultures\n","- CEO mandate: \"One firm, one operating model\"\n","\n","Question: What should the target operating model look like, and what's the transition path?\n","\n","NOTE: This is a demo case. Organization charts, process maps, and cultural assessments are UNKNOWN.\"\"\",\n","        \"deliverable_type\": \"operating model narrative + RACI\"\n","    }\n","]\n","\n","# Run all 4 cases\n","all_results = {}\n","summary_data = []\n","\n","for case_spec in cases:\n","    results = run_case_pipeline(case_spec, auto_approve=True)\n","    all_results[case_spec[\"case_name\"]] = results\n","\n","    # Gather summary metrics\n","    total_open_questions = sum(\n","        len(stage_result.get(\"open_questions\", []))\n","        for stage_result in results.values()\n","        if isinstance(stage_result, dict)\n","    )\n","\n","    all_risks = []\n","    for stage_result in results.values():\n","        if isinstance(stage_result, dict) and \"risks\" in stage_result:\n","            all_risks.extend(stage_result[\"risks\"])\n","\n","    highest_risk = \"none\"\n","    if all_risks:\n","        severity_order = {\"high\": 3, \"medium\": 2, \"low\": 1}\n","        highest_risk = max(all_risks, key=lambda r: severity_order.get(r.get(\"severity\", \"low\"), 0))[\"severity\"]\n","\n","    exception_data = read_json(exception_log_path)\n","    exceptions_count = len([e for e in exception_data[\"exceptions\"] if e.get(\"case_name\") == case_spec[\"case_name\"]])\n","\n","    summary_data.append({\n","        \"case_name\": case_spec[\"case_name\"],\n","        \"stages_completed\": len([k for k in results.keys() if k.startswith(\"stage_\")]),\n","        \"open_questions_total\": total_open_questions,\n","        \"highest_risk_severity\": highest_risk,\n","        \"exceptions_count\": exceptions_count\n","    })\n","\n","# Print summary table\n","print(\"\\n\" + \"=\"*80)\n","print(\"DEMO CASES SUMMARY\")\n","print(\"=\"*80)\n","print(f\"{'Case Name':<25} {'Stages':<10} {'Open Q':<10} {'Risk':<10} {'Exceptions':<12}\")\n","print(\"-\"*80)\n","\n","for row in summary_data:\n","    print(f\"{row['case_name']:<25} {row['stages_completed']:<10} {row['open_questions_total']:<10} {row['highest_risk_severity']:<10} {row['exceptions_count']:<12}\")\n","\n","print(\"=\"*80)\n","print(f\"\\nâœ“ All 4 demo cases completed\")\n","print(f\"  Deliverables in: {deliverables_dir}\")\n","print(f\"  Stage outputs in: {stage_outputs_dir}\")\n"],"metadata":{"id":"GF7UbU5xNTWW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768856150748,"user_tz":360,"elapsed":27555,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"f64039fe-d27e-4fe7-fd13-1218296322a0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Running 4 mini-case demos...\n","\n","\n","============================================================\n","PIPELINE: market_entry\n","============================================================\n","\n","Stage 0: Intake + Classification\n","  âœ“ Saved: stage_0_intake.json\n","\n","Stage 1: Structure\n","  âœ“ Saved: stage_1_structure.json\n","  Open questions: 10\n","\n","Stage 2: Options + Trade-offs\n","  âœ“ Saved: stage_2_options.json\n","  Assumptions: 6\n","\n","Stage 3: Draft Deliverable Bundle\n","  âœ“ Saved: stage_3_deliverable.json\n","\n","Stage 4: Verification Plan + Assumption Register\n","  âœ“ Saved: stage_4_verification.json\n","\n","Stage 5: Packaging\n","  âœ“ Saved: market_entry_final_bundle.json\n","  âœ“ Saved: market_entry_human_readable.txt\n","\n","âœ“ Pipeline completed for market_entry\n","\n","============================================================\n","PIPELINE: cost_transformation\n","============================================================\n","\n","Stage 0: Intake + Classification\n","  âœ“ Saved: stage_0_intake.json\n","\n","Stage 1: Structure\n","  âœ“ Saved: stage_1_structure.json\n","  Open questions: 10\n","\n","Stage 2: Options + Trade-offs\n","  âœ“ Saved: stage_2_options.json\n","  Assumptions: 8\n","\n","Stage 3: Draft Deliverable Bundle\n","  âœ“ Saved: stage_3_deliverable.json\n","\n","Stage 4: Verification Plan + Assumption Register\n","  âœ“ Saved: stage_4_verification.json\n","\n","Stage 5: Packaging\n","  âœ“ Saved: cost_transformation_final_bundle.json\n","  âœ“ Saved: cost_transformation_human_readable.txt\n","\n","âœ“ Pipeline completed for cost_transformation\n","\n","============================================================\n","PIPELINE: capital_allocation\n","============================================================\n","\n","Stage 0: Intake + Classification\n","  âœ“ Saved: stage_0_intake.json\n","\n","Stage 1: Structure\n","  âœ“ Saved: stage_1_structure.json\n","  Open questions: 8\n","\n","Stage 2: Options + Trade-offs\n","  âœ“ Saved: stage_2_options.json\n","  Assumptions: 5\n","\n","Stage 3: Draft Deliverable Bundle\n","  âœ“ Saved: stage_3_deliverable.json\n","\n","Stage 4: Verification Plan + Assumption Register\n","  âœ“ Saved: stage_4_verification.json\n","\n","Stage 5: Packaging\n","  âœ“ Saved: capital_allocation_final_bundle.json\n","  âœ“ Saved: capital_allocation_human_readable.txt\n","\n","âœ“ Pipeline completed for capital_allocation\n","\n","============================================================\n","PIPELINE: operating_model\n","============================================================\n","\n","Stage 0: Intake + Classification\n","  âœ“ Saved: stage_0_intake.json\n","\n","Stage 1: Structure\n","  âœ“ Saved: stage_1_structure.json\n","  Open questions: 9\n","\n","Stage 2: Options + Trade-offs\n","  âœ“ Saved: stage_2_options.json\n","  Assumptions: 7\n","\n","Stage 3: Draft Deliverable Bundle\n","  âœ“ Saved: stage_3_deliverable.json\n","\n","Stage 4: Verification Plan + Assumption Register\n","  âœ“ Saved: stage_4_verification.json\n","\n","Stage 5: Packaging\n","  âœ“ Saved: operating_model_final_bundle.json\n","  âœ“ Saved: operating_model_human_readable.txt\n","\n","âœ“ Pipeline completed for operating_model\n","\n","================================================================================\n","DEMO CASES SUMMARY\n","================================================================================\n","Case Name                 Stages     Open Q     Risk       Exceptions  \n","--------------------------------------------------------------------------------\n","market_entry              5          37         high       0           \n","cost_transformation       5          37         high       0           \n","capital_allocation        5          35         high       0           \n","operating_model           5          39         high       0           \n","================================================================================\n","\n","âœ“ All 4 demo cases completed\n","  Deliverables in: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/deliverables\n","  Stage outputs in: /content/ai_consulting_ch3_runs/run_20260119_203313_cf3f83cf/stage_outputs\n"]}]},{"cell_type":"markdown","source":["##9.USER'S APPLICATIONS"],"metadata":{"id":"3juVsof2NUJC"}},{"cell_type":"markdown","source":["###9.1.OVERVIEW"],"metadata":{"id":"0A7X8eAeNVjp"}},{"cell_type":"markdown","source":["**Cell 9: User Exercise with Human-in-the-Loop Workflow**\n","\n","**What This Cell Does**\n","\n","This cell transforms you from observer to participant by allowing you to run your own consulting case through the governed pipeline with genuine human checkpoints at every stage transition. Unlike the automated demonstrations in Cell Eight, this exercise requires your active engagementâ€”you will describe a business situation, then approve or reject progression at each gate based on your assessment of the AI's output quality. This simulates how the system would operate in actual consulting practice, where human judgment governs workflow progression rather than automatic approval.\n","\n","**Interactive Case Setup**\n","\n","The cell begins by presenting you with four case type options: market entry, cost transformation, capital allocation, or operating model redesign. You select one by typing a number. The system then prompts you to describe the business situation in two to three sentences. This is where you would normally input details about the client context, the strategic question at hand, and any relevant background. However, the cell immediately applies the redaction function to your input, scanning for and masking email addresses, phone numbers, and monetary amounts before any of this text is sent to the AI or logged. If redaction occurs, you receive a summary showing how many items of each type were removed, making the protective action transparent.\n","\n","**Redaction and Minimum Necessary Principle**\n","\n","The redaction step operationalizes a core principle of data governance: collect and process only the minimum information necessary to accomplish the task. If you inadvertently included a phone number or specific dollar amount in your situation description, these details likely add little analytical value but create confidentiality risk. The redaction function removes them automatically, and the redacted version becomes what flows into subsequent stages. This demonstrates how production systems would apply more sophisticated domain-specific redaction based on your firm's policiesâ€”removing company names, deal identifiers, personnel information, or other sensitive elements that should not appear in logs or AI prompts.\n","\n","**The Gate Approval Interface**\n","\n","With your case specification prepared and redacted, the pipeline begins execution. Stage Zero completes instantly, capturing your inputs into the intake record. Then the first gate appears. The system prints a message asking you to approve or reject the Stage Zero to Stage One transition and to provide a rationale. You must type your decision and reasoning. This is not ceremonialâ€”if you type \"reject,\" the pipeline halts immediately, logs your rejection with your stated rationale to both the stage gate log and the exception log, and returns partial results showing only what completed before the halt.\n","\n","Assuming you approve, Stage One executes, making an AI call to generate analytical structure. The system saves the Stage One output and prints a summary showing the filename and how many open questions were identified. You now have an opportunity to examine the actual JSON file in the stage outputs directory if you wish to see the full AI response including the analytical framework, assumptions, and risks detected. Then the next gate appears, asking you to approve or reject the Stage One to Stage Two transition.\n","\n","**Progressive Revelation and Calibration**\n","\n","This gate-by-gate progression creates opportunities for course correction that fully automated execution lacks. Perhaps the Structure Agent produced an issue tree when you believe a driver tree would be more appropriate. You could reject at Gate One to Two, providing a rationale like \"requested quantitative decomposition instead of qualitative issue tree.\" The rejection would be logged, the pipeline would halt, and you could restart with modified inputs. Or perhaps the Options Agent in Stage Two generated alternatives that ignore a critical constraint you mentioned. Rejecting at Gate Two to Three signals this problem, prevents wasted work in subsequent stages, and creates a record of why human judgment intervened.\n","\n","Each gate decision is permanently logged with your reviewer role (which defaults to the role appropriate for that transitionâ€”Analyst for early gates, Manager for middle gates, Partner for late gates), your decision, and your rationale. This creates an audit trail showing not just what the AI produced but where humans examined that output and what judgments they made about quality and appropriateness.\n","\n","**Handling Pipeline Completion or Halt**\n","\n","If you approve all gates, the pipeline completes all six stages and the cell prints a completion message showing where your final deliverables were saved: the JSON bundle, the human-readable text summary, and the complete stage outputs folder. It also prints statistics about the runâ€”how many stages completed, how many risks were logged, and how many open questions remain unresolved. These metrics help you assess whether the AI-generated materials provide a useful starting point for further analysis or whether the quantity of open questions and high-severity risks indicates the problem requires more upfront fact gathering before AI assistance adds value.\n","\n","If you reject any gate, the pipeline halts at that point and the cell prints a halted message indicating which stage was the last to complete. The partial results remain saved in the stage outputs directory, and the governance logs document exactly where and why the pipeline stopped. The stage gate log contains your rejection decision and rationale. The exception log records that a gate was rejected. This partial execution still produces valueâ€”you have whatever stages completed successfully, you have the governance trail showing the decision process, and you have learned something about where this particular case or the AI's capabilities hit limitations.\n","\n","**The Pedagogical Purpose**\n","\n","This exercise teaches several lessons that passive observation cannot convey. First, it makes visceral the reality that AI outputs require human evaluationâ€”you must actually read the structure, assess its appropriateness, and make a judgment call. Second, it demonstrates that governance mechanisms like gates are not bureaucratic overhead but practical quality control that catches problems early. Third, it shows how the system handles the realistic scenario where not every case runs smoothly to completionâ€”rejections and halts are normal, expected, and properly documented rather than swept under the rug. Finally, it gives you direct experience with the human role in supervised workflows: not rubber-stamping AI output, but actively evaluating, calibrating, and directing the analytical process while the AI handles structured drafting tasks within boundaries you define and approve."],"metadata":{"id":"7SgqchOZNupJ"}},{"cell_type":"markdown","source":["###9.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"q65FZs56NXZp"}},{"cell_type":"code","source":["# Cell 9: User Exercise (Gated Workflow, Human-in-the-Loop)\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"USER EXERCISE: Human-in-the-Loop Workflow\")\n","print(\"=\"*60)\n","\n","print(\"\"\"\n","You will run a consulting case through the governed pipeline.\n","At each stage gate, you'll decide whether to approve or reject.\n","\n","Case types:\n","1. Market entry\n","2. Cost transformation\n","3. Capital allocation\n","4. Operating model redesign\n","\"\"\")\n","\n","# Get user input\n","case_type_map = {\n","    \"1\": \"market entry\",\n","    \"2\": \"cost transformation\",\n","    \"3\": \"capital allocation\",\n","    \"4\": \"operating model redesign\"\n","}\n","\n","case_choice = input(\"\\nChoose case type (1-4): \").strip()\n","case_type = case_type_map.get(case_choice, \"market entry\")\n","\n","print(f\"\\nYou selected: {case_type}\")\n","\n","situation_raw = input(\"\\nDescribe the situation (2-3 sentences):\\n\").strip()\n","\n","if not situation_raw:\n","    situation_raw = f\"Demo {case_type} case for testing human-in-the-loop workflow.\"\n","\n","# Apply redaction\n","situation, removed = build_minimum_necessary(situation_raw)\n","\n","if sum(removed.values()) > 0:\n","    print(f\"\\nâš ï¸  Redacted: {removed}\")\n","\n","deliverable_type_map = {\n","    \"market entry\": \"exec memo\",\n","    \"cost transformation\": \"transformation workplan\",\n","    \"capital allocation\": \"IC pre-read\",\n","    \"operating model redesign\": \"operating model narrative\"\n","}\n","\n","deliverable_type = deliverable_type_map.get(case_type, \"exec memo\")\n","\n","user_case_spec = {\n","    \"case_name\": \"user_case\",\n","    \"case_type\": case_type,\n","    \"situation\": situation,\n","    \"deliverable_type\": deliverable_type\n","}\n","\n","print(f\"\\nRunning pipeline with human gates...\")\n","print(f\"Deliverable type: {deliverable_type}\\n\")\n","\n","# Run pipeline with auto_approve=False\n","user_results = run_case_pipeline(user_case_spec, auto_approve=False)\n","\n","# Check if completed\n","if \"final_bundle\" in user_results:\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"âœ“ PIPELINE COMPLETED\")\n","    print(\"=\"*60)\n","    print(f\"\\nYour deliverables:\")\n","    print(f\"  Bundle: {deliverables_dir / 'user_case_final_bundle.json'}\")\n","    print(f\"  Readable: {deliverables_dir / 'user_case_human_readable.txt'}\")\n","    print(f\"  Stage outputs: {stage_outputs_dir / 'user_case'}\")\n","\n","    # Print quick stats\n","    total_risks = user_results[\"final_bundle\"][\"total_risks\"]\n","    total_questions = user_results[\"final_bundle\"][\"total_open_questions\"]\n","\n","    print(f\"\\nPipeline stats:\")\n","    print(f\"  Stages completed: {len([k for k in user_results.keys() if k.startswith('stage_')])}\")\n","    print(f\"  Total risks logged: {total_risks}\")\n","    print(f\"  Total open questions: {total_questions}\")\n","\n","else:\n","    # Pipeline halted\n","    last_stage = max([int(k.split(\"_\")[1]) for k in user_results.keys() if k.startswith(\"stage_\")])\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"âœ— PIPELINE HALTED\")\n","    print(\"=\"*60)\n","    print(f\"\\nLast completed stage: Stage {last_stage}\")\n","    print(f\"Partial results saved in: {stage_outputs_dir / 'user_case'}\")\n","    print(f\"\\nCheck gate log: {stage_gate_log_path}\")"],"metadata":{"id":"h8-UJrsrNZmu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##10.BUNDLE AND AUDIT README"],"metadata":{"id":"deALRI7DNc11"}},{"cell_type":"markdown","source":["###10.1.OVERVIEW"],"metadata":{"id":"hspK-tF8Nd9D"}},{"cell_type":"markdown","source":["**Cell 10: Creating the Complete Audit Bundle and Archive**\n","\n","**What This Cell Does**\n","\n","This final cell transforms the accumulated governance artifacts, stage outputs, and deliverables from your pipeline execution into a complete, portable audit package that documents everything that occurred during the run. Think of it as creating the ultimate transparency reportâ€”a comprehensive archive that proves what the AI did, what risks were detected, what assumptions were made, where humans intervened, and what remains unverified. This cell produces both a detailed explanatory document and a zip file containing the entire evidence base that could be submitted to compliance teams, shared with clients, or stored for regulatory requirements.\n","\n","**The Audit README: Your Guide to the Governance Bundle**\n","\n","The cell begins by generating an extensive readme file that serves as both index and instruction manual for all the artifacts in the bundle. This document runs several pages and covers eight major topics. It explains the purpose of Level Three supervised workflows and why process documentation matters as much as analytical output. It lists every governance log file and describes what each contains. It provides specific instructions for reproducing the run using the same model, parameters, and configuration hash. It dedicates an entire section to explaining what hashing means, why prompts are hashed rather than logged in full, and what the cryptographic protection accomplishes for confidentiality and integrity.\n","\n","The readme includes detailed explanations of verification status, emphasizing that all AI outputs carry the designation \"Not verified\" and require human review before use in client deliverables, investment decisions, or regulatory filings. It walks through the six-stage workflow describing what each stage does and what type of AI call, if any, occurs at each stage. It explains the gate mechanism and why human checkpoints exist between stages. It offers a multi-agent interpretation of the pipeline, describing how each stage can be viewed as a specialized expert agent with bounded competence. It documents the risk detection system, listing the categories of risks automatically flagged and what each category means.\n","\n","**Confidentiality and Limitation Disclosures**\n","\n","The readme includes prominent confidentiality notices warning users not to paste client-confidential data into the notebook without appropriate safeguards and reminding them that while prompts are hashed in logs, AI-generated outputs may reference input data and should be treated as confidential. It also contains an extensive limitations section that explicitly states this is a demonstration system, not production-ready software. It lists known limitations like lack of enterprise authentication, absence of encryption at rest, no integration with document management systems, and basic rather than sophisticated redaction capabilities.\n","\n","The limitations section also specifies what would be required for production deployment: legal and compliance review, client consent for AI-assisted analysis, professional liability insurance coverage, documented human review processes, and firm partnership approval. This honest disclosure of limitations protects both the system creators and potential users from misunderstanding the maturity level and appropriate use cases for the technology.\n","\n","**Artifact Inventory and Verification**\n","\n","After writing the readme, the cell systematically inventories every artifact that should exist in the run directory. It checks for the run manifest, prompts log, risk log, verification register, change log, approvals log, stage gate log, exception log, workflow runbook, and the readme itself. For each file, it reports whether the file exists and how many bytes it contains. It counts files in the deliverables directory and files in the stage outputs directory, printing these counts to show the volume of material generated. This inventory serves as a completeness checkâ€”if expected files are missing, you can identify gaps before archiving.\n","\n","**Creating the Zip Archive**\n","\n","The cell then invokes the system's archive utility to create a compressed zip file containing the entire run directory. This zip file is named with the run identifier (which includes timestamp and random short ID) and placed in the parent directory above the run folder. The cell reports the zip file's size in both bytes and megabytes, giving you a sense of the archive's scope. This single zip file becomes the portable audit package that can be downloaded from the Colab environment, stored in your firm's records management system, attached to project documentation, or transmitted to auditors or regulators who need to examine the AI-assisted work.\n","\n","**The Final Checklist and Summary Statistics**\n","\n","The cell prints a final checklist showing each major artifact type with a checkmark indicating whether it exists in the bundle. This provides a visual confirmation that the governance infrastructure functioned correctly throughout the run. The cell then computes and displays summary statistics drawn from the governance logs. It reports total risks detected across all cases, broken down by severity levelâ€”how many high, medium, and low severity risks were logged. It reports total exceptions that occurred, indicating whether the pipeline executed cleanly or encountered problems. It counts gate decisions, showing how many approvals and rejections were logged, which reveals the intensity of human oversight that occurred.\n","\n","**Understanding Run Statistics**\n","\n","These statistics tell important stories about the quality and governance of the work. A run with zero high-severity risks and many open questions suggests the AI appropriately acknowledged uncertainty rather than fabricating confident claims. A run with many high-severity hallucination risks suggests the AI attempted to make unsupported assertions that the validation layer caught and flagged. A run with many gate rejections indicates either that outputs were not meeting quality standards or that humans were actively calibrating the workflow rather than passively approving everything.\n","\n","The cell also reports how many cases were processed (the number of subdirectories in stage outputs), how many total stage output files were generated (indicating how many stages across how many cases completed successfully), and how many final deliverables exist (the curated human-readable products). These counts provide a quick assessment of productivityâ€”did the run complete work on multiple cases, or did it halt early due to errors or rejections.\n","\n","**Critical Reminders and Usage Guidance**\n","\n","The cell concludes with five critical reminders printed prominently. First, all outputs are marked \"Not verified\" and require human review before use. Second, prompts are hashed and therefore not readable in logs, which protects confidentiality but means you cannot reconstruct exact prompts from logs alone. Third, the risk log should be consulted before using any outputs to understand what concerns were detected. Fourth, the verification register identifies all the validation work that must be completed before the analysis can be considered final. Fifth, this remains a demonstration system requiring hardening before production use in actual consulting engagements.\n","\n","These reminders serve a legal and ethical function. They ensure users cannot claim they were unaware that outputs require verification, that the system has limitations, or that governance logs exist documenting risks and uncertainties. The reminders shift responsibility appropriatelyâ€”the system provides tools, transparency, and risk detection, but humans own the decision about whether and how to use the outputs, and humans bear the professional accountability for work products delivered to clients.\n","\n","**The Philosophy of Complete Transparency**\n","\n","This final cell embodies the core philosophy of the entire notebook: transparency enables accountability, and accountability enables trust. By bundling every log, every output, every decision, and comprehensive documentation into a single portable archive, the system makes auditing not just possible but straightforward. Regulators can examine the archive and verify that human oversight occurred at defined checkpoints. Clients can review the archive and see what assumptions underlie recommendations. Quality assurance teams can analyze patterns across multiple archives to identify where the AI performs well and where it struggles. This level of transparency is uncommon in AI systems but essential for professional services where reputation and liability hinge on demonstrable rigor."],"metadata":{"id":"QktoO36gNwPN"}},{"cell_type":"markdown","source":["###10.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"-qPlPvQYNhIv"}},{"cell_type":"code","source":["# Cell 10: Bundle + AUDIT_README + Zip\n","\n","print(\"\\nCreating final audit bundle...\\n\")\n","\n","# Create AUDIT_README.txt\n","audit_readme = f\"\"\"CONSULTING AI GOVERNANCE AUDIT TRAIL\n","Chapter 3 - Level 3: Supervised Multi-Step Workflows (Agents)\n","{'='*70}\n","\n","RUN ID: {run_id}\n","TIMESTAMP: {now_iso()}\n","MODEL: {MODEL}\n","CONFIG HASH: {config_hash}\n","\n","{'='*70}\n","PURPOSE\n","{'='*70}\n","\n","Level 3 demonstrates supervised multi-step consulting workflows with:\n","- Fixed pipeline stages (Intake â†’ Structure â†’ Options â†’ Deliverable â†’ Verification â†’ Packaging)\n","- Human checkpoints (gates) between stages\n","- Immutable audit logs\n","- Separation of duties (AI drafts, humans approve)\n","\n","This is NOT autonomous AI. This is workflow discipline enforced by code.\n","\n","{'='*70}\n","ARTIFACTS INCLUDED\n","{'='*70}\n","\n","GOVERNANCE LOGS:\n","- run_manifest.json         â†’ Run metadata, model, params, config hash\n","- prompts_log.jsonl         â†’ Redacted prompts (hashes only)\n","- risk_log.json             â†’ Aggregated risks across all stages\n","- verification_register.json â†’ Unverified claims requiring human review\n","- change_log.json           â†’ Changes to outputs (if re-run)\n","- approvals_log.json        â†’ Placeholder for human approvals\n","- stage_gate_log.jsonl      â†’ Gate decisions, reviewer role, timestamp\n","- exception_log.json        â†’ Policy breaks, missing approvals, parsing failures\n","\n","WORKFLOW DEFINITION:\n","- workflow_runbook.json     â†’ Stage definitions, allowed transitions, roles\n","\n","STAGE OUTPUTS:\n","- stage_outputs/            â†’ One JSON file per stage per case\n","  - market_entry/\n","  - cost_transformation/\n","  - capital_allocation/\n","  - operating_model/\n","  - user_case/\n","  - pipeline_test/\n","\n","DELIVERABLES:\n","- deliverables/             â†’ Final human-readable bundles\n","  - <case>_final_bundle.json\n","  - <case>_human_readable.txt\n","\n","{'='*70}\n","HOW TO REPRODUCE THIS RUN\n","{'='*70}\n","\n","1. Model: {MODEL}\n","2. Parameters: temperature={DEFAULT_TEMPERATURE}, max_tokens={DEFAULT_MAX_TOKENS}\n","3. Config hash: {config_hash}\n","\n","To reproduce, use the same model and parameters with identical prompts\n","(hashes in prompts_log.jsonl).\n","\n","The config hash is a fingerprint of the model configuration. Any change to\n","model, temperature, or max_tokens will produce a different config hash,\n","indicating that results may not be directly comparable.\n","\n","{'='*70}\n","WHAT IS HASHED AND WHY\n","{'='*70}\n","\n","PROMPTS ARE HASHED (not logged in full):\n","- Every prompt sent to Claude is hashed using SHA-256\n","- Only the hash is logged in prompts_log.jsonl\n","- The original prompt text is NOT stored in logs\n","\n","WHY HASH PROMPTS?\n","1. CONFIDENTIALITY: Prompts contain case details that may be sensitive\n","   (company names, financials, strategies). Hashing protects this data.\n","\n","2. INTEGRITY: Hashes prove which prompts were sent. If someone claims\n","   different prompts were used, recompute the hash to verify.\n","\n","3. REPRODUCIBILITY: With the case specification and the hash, you can\n","   verify the correct prompts were constructed without exposing content.\n","\n","WHAT IS NOT HASHED:\n","- AI responses (saved in full as JSON in stage_outputs/)\n","- Risk logs (saved in full for transparency)\n","- Gate decisions (saved in full for audit trail)\n","- Final deliverables (saved in full for human review)\n","\n","THE HASH IS ONE-WAY:\n","- You cannot reverse a hash to get the original prompt\n","- This is cryptographic protection, not obfuscation\n","- SHA-256 produces a 64-character hexadecimal string\n","\n","{'='*70}\n","VERIFICATION STATUS\n","{'='*70}\n","\n","âš ï¸  ALL AI OUTPUTS ARE MARKED: verification_status=\"Not verified\"\n","\n","Human consultant review is REQUIRED before any output is used in:\n","- Client deliverables\n","- Investment Committee materials\n","- Board presentations\n","- Regulatory filings\n","\n","AI assists with structure and drafting. Humans own judgment and accountability.\n","\n","The verification_register.json file contains all assumptions and open questions\n","that must be resolved before the analysis can be considered complete.\n","\n","{'='*70}\n","UNDERSTANDING THE WORKFLOW\n","{'='*70}\n","\n","The pipeline executes in six stages:\n","\n","STAGE 0: INTAKE + CLASSIFICATION (no AI call)\n","- Captures case specification\n","- Defines scope boundary\n","- Classifies case type\n","\n","STAGE 1: STRUCTURE (one AI call)\n","- Builds analytical framework (issue tree, driver tree, design dimensions)\n","- Identifies key decision factors\n","- Flags initial assumptions and open questions\n","\n","STAGE 2: OPTIONS + TRADE-OFFS (one AI call)\n","- Generates strategic alternatives\n","- Identifies trade-offs between options\n","- Explicitly avoids making recommendations (separation of duties)\n","\n","STAGE 3: DRAFT DELIVERABLE BUNDLE (one AI call)\n","- Creates outline/shell for final deliverable\n","- Structures executive summary and main body\n","- Marks all unverified claims\n","\n","STAGE 4: VERIFICATION PLAN + ASSUMPTION REGISTER (one AI call)\n","- Lists all assumptions from previous stages\n","- Creates verification plan (who, what, how, when)\n","- Prioritizes verification work\n","\n","STAGE 5: PACKAGING (no AI call)\n","- Assembles final deliverable bundle\n","- Creates human-readable summary\n","- Generates governance bundle\n","\n","Between each stage is a GATE that requires approval to proceed.\n","In production, humans review outputs and approve/reject transitions.\n","In demo mode, gates auto-approve but still log decisions.\n","\n","{'='*70}\n","CONFIDENTIALITY NOTICE\n","{'='*70}\n","\n","This notebook applied basic redaction (emails, phone numbers, amounts).\n","\n","For real client work:\n","- Do NOT paste client-confidential data into this notebook\n","- Apply domain-specific redaction (company names, deals, personnel)\n","- Follow your firm's data governance policies\n","- Consider using this system in a private, firm-hosted environment\n","\n","The governance logs contain hashed prompts (not readable) but do contain\n","AI-generated outputs that may reference your input data. Treat the entire\n","bundle as confidential.\n","\n","{'='*70}\n","UNDERSTANDING GATES AND HUMAN OVERSIGHT\n","{'='*70}\n","\n","Gates are human checkpoints between stages. They serve multiple purposes:\n","\n","1. QUALITY CONTROL: Human reviews Stage N output before Stage N+1 begins\n","2. COURSE CORRECTION: Human can reject and request different approach\n","3. ACCOUNTABILITY: Human's approval is logged with rationale\n","4. LEARNING: Human sees AI reasoning and can provide feedback\n","\n","In this demo, auto_approve=True allows end-to-end execution for testing.\n","In production, auto_approve=False would pause at each gate and request:\n","- Approve or reject decision\n","- Rationale for the decision\n","- Reviewer role (Analyst, Manager, Partner)\n","\n","All gate decisions are logged in stage_gate_log.jsonl with timestamps.\n","\n","{'='*70}\n","MULTI-AGENT INTERPRETATION\n","{'='*70}\n","\n","Each stage can be viewed as a specialized agent:\n","\n","- STRUCTURE AGENT: Expert in analytical decomposition\n","- OPTIONS AGENT: Expert in possibility exploration and trade-off identification\n","- DELIVERABLE AGENT: Expert in professional communication formats\n","- VERIFICATION AGENT: Expert in epistemic hygiene and assumption testing\n","\n","These agents collaborate through the workflow:\n","- Each agent receives context from previous agents\n","- Each agent has bounded scope and clear role\n","- Agents cannot skip stages or reorder the sequence\n","- The workflow topology disciplines agent behavior\n","\n","This is \"soft agency\" or \"workflow agency\" - autonomy within boundaries.\n","\n","{'='*70}\n","RISK DETECTION AND LOGGING\n","{'='*70}\n","\n","The system automatically detects several risk types:\n","\n","HALLUCINATION: Numeric claims not grounded in facts/assumptions\n","DECISION_LAUNDERING: AI implies work was performed without evidence\n","FALSE_RIGOR: Outputs lack expected analytical markers\n","MISSING_FACTS: No open questions listed (incomplete fact gathering)\n","TRACEABILITY: Stage outputs cannot be traced to inputs\n","STAGE_BOUNDARY: Stage output does not match stage purpose\n","\n","Risks are logged with:\n","- Type (one of the categories above)\n","- Severity (low, medium, high)\n","- Note (specific description)\n","- Timestamp and stage name\n","\n","All risks are aggregated in risk_log.json for review.\n","\n","{'='*70}\n","QUESTIONS OR ISSUES\n","{'='*70}\n","\n","Check the following logs for debugging:\n","- exception_log.json â†’ Policy violations, parsing errors\n","- risk_log.json â†’ Auto-detected risks (hallucination, decision laundering, etc.)\n","- stage_gate_log.jsonl â†’ Gate approval history\n","\n","If JSON parsing failed, check:\n","- debug_malformed_json.txt (if present)\n","\n","If a stage produced unexpected output:\n","- Review the stage's JSON in stage_outputs/<case>/stage_N_*.json\n","- Check facts_provided, assumptions, and open_questions sections\n","- Review the risks array for auto-detected issues\n","\n","If the pipeline halted mid-execution:\n","- Check stage_gate_log.jsonl for rejections\n","- Check exception_log.json for failures\n","\n","{'='*70}\n","LIMITATIONS AND DISCLAIMERS\n","{'='*70}\n","\n","This system is a DEMONSTRATION of governed AI workflows for consulting.\n","It is NOT production-ready without additional hardening:\n","\n","KNOWN LIMITATIONS:\n","- No integration with enterprise systems (CRM, document management)\n","- Basic redaction only (not domain-specific)\n","- No user authentication or access control\n","- No encryption of artifacts at rest\n","- No real-time collaboration features\n","- No integration with approval workflow tools\n","\n","PRODUCTION DEPLOYMENT would require:\n","- Enterprise authentication (SSO, RBAC)\n","- Encryption of sensitive data\n","- Integration with firm's document management system\n","- Audit log retention and compliance features\n","- Advanced redaction and data loss prevention\n","- Legal review of AI usage policies\n","\n","DO NOT use this system for actual client work without:\n","1. Legal and compliance review\n","2. Client consent for AI-assisted analysis\n","3. Professional liability insurance coverage\n","4. Documented human review and approval process\n","5. Firm partnership approval\n","\n","{'='*70}\n","LICENSE AND ATTRIBUTION\n","{'='*70}\n","\n","Author: Alejandro Reynoso\n","Affiliation: Chief Scientist DEFI CAPITAL RESEARCH\n","             External Lecturer, Judge Business School Cambridge\n","\n","Model: Claude Sonnet 4.5 ({MODEL})\n","Provider: Anthropic\n","\n","This notebook is provided for educational purposes.\n","No warranty is provided for any use, commercial or otherwise.\n","\n","{'='*70}\n","END OF AUDIT README\n","{'='*70}\n","\n","Generated: {now_iso()}\n","Run ID: {run_id}\n","\"\"\"\n","\n","readme_path = base_dir / \"AUDIT_README.txt\"\n","with open(readme_path, 'w', encoding='utf-8') as f:\n","    f.write(audit_readme)\n","\n","print(f\"âœ“ Created: {readme_path.name}\")\n","\n","# List all artifacts\n","print(f\"\\nArtifacts included in bundle:\")\n","\n","artifacts = [\n","    \"run_manifest.json\",\n","    \"prompts_log.jsonl\",\n","    \"risk_log.json\","],"metadata":{"id":"STPTAD0YNi9b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##11.CONCLUSIONS"],"metadata":{"id":"7X2_AKt3NkAR"}},{"cell_type":"markdown","source":["**Conclusion: The Future of AI in Professional Services Is Governed Collaboration**\n","\n","**What We Have Built and Why It Matters**\n","\n","This notebook has demonstrated a complete, operational system for deploying artificial intelligence in management consulting work while maintaining the governance rigor, professional accountability, and quality standards that clients and regulators rightfully demand. We have moved beyond the chatbot paradigm of isolated question-and-answer exchanges to implement a supervised workflow agent that executes multi-step analytical processes, accumulates context across stages, enforces human checkpoints at critical transitions, and generates comprehensive audit trails documenting every decision, assumption, risk, and uncertainty.\n","\n","The significance of this architecture extends far beyond the technical implementation details. We have shown that the seemingly irreconcilable tension between AI capability and professional responsibility is resolvable through thoughtful system design. You can have autonomous execution within stages and human oversight between stages. You can have efficiency gains from AI handling structured analytical work and accountability preservation through mandatory human approval of progression. You can have sophisticated multi-step reasoning and transparent documentation of every inference, assumption, and risk. The choice is not between using AI or maintaining standardsâ€”properly architected systems deliver both.\n","\n","**The Governance Artifacts as the True Innovation**\n","\n","While the ability to execute six-stage consulting workflows end-to-end represents an impressive technical achievement, the deeper innovation lies in the governance infrastructure surrounding that execution. Every run of this system produces not just analytical outputs but a complete evidence package: the run manifest documenting configuration and parameters, the prompts log recording hashed inputs for integrity verification while protecting confidentiality, the risk log aggregating detected concerns across all stages, the verification register cataloging unvalidated claims requiring human review, the stage gate log documenting human oversight decisions, and the exception log capturing deviations from normal execution.\n","\n","These artifacts transform AI from a black box generating mysterious outputs into a transparent collaborator whose reasoning, limitations, and uncertainties are made explicit. When a consulting team presents recommendations to a client, they can now provide not just conclusions but the complete analytical trail showing how those conclusions were reached, what alternatives were considered, what assumptions were made, what risks were identified, and where humans intervened to approve or redirect the work. This transparency builds trust in ways that standalone AI-generated documents never could.\n","\n","The governance bundle also enables organizational learning impossible with traditional consulting approaches. Aggregating risk logs across dozens of cases reveals patterns about which analytical stages generate the most concerns, which case types prove most challenging for AI assistance, and which kinds of assumptions most frequently require verification. Stage gate logs show where human reviewers most often reject AI outputs, highlighting opportunities for prompt refinement or model improvement. Exception logs identify failure modes that system designers can address through better error handling or validation logic. This feedback loop allows continuous improvement of both the AI components and the workflow design.\n","\n","**The Multi-Agent Perspective and Its Implications**\n","\n","We introduced the conceptual framework of viewing each stage as a specialized agent with narrow competence rather than a single general-purpose agent attempting all analytical tasks. The Structure Agent knows how to decompose complex problems into issue trees, driver trees, and design dimensions but knows nothing about drafting deliverables or planning verification. The Options Agent knows how to generate strategic alternatives and articulate trade-offs but is constrained from making recommendations. The Verification Agent knows how to examine assumptions and construct validation plans but does not perform the verification itself.\n","\n","This multi-agent interpretation illuminates why supervised workflows outperform both unassisted human work and fully autonomous AI approaches. Each specialized agent operates at a level of sophistication difficult for humans to match consistentlyâ€”the Structure Agent applies frameworks with encyclopedic knowledge of analytical patterns, the Options Agent explores possibility spaces exhaustively without premature convergence, the Deliverable Agent formats outputs according to precise professional conventions. Yet the agents remain bounded within their domains of competence, unable to wander into areas where AI capabilities are less reliable or where professional judgment becomes essential.\n","\n","The orchestration of these specialized agents through a fixed workflow topology creates emergent intelligence that exceeds what any individual agent could achieve. The whole becomes greater than the sum of the parts because each agent builds upon the work of previous agents, and human oversight calibrates the collaboration at every stage transition. This is not artificial general intelligence attempting to replace human consultants but artificial specialized intelligence amplifying human effectiveness through division of labor that plays to the strengths of both AI and human contributors.\n","\n","**From Demonstration to Deployment: The Path Forward**\n","\n","This notebook provides a functional demonstration of supervised workflow agents, but the path from demonstration to production deployment in actual consulting practice requires addressing several practical realities that our implementation necessarily simplified. Enterprise deployment demands integration with existing systems for customer relationship management, document storage, and project workflow. It requires sophisticated authentication and authorization mechanisms ensuring that only appropriate personnel can access sensitive case materials and approve gate transitions. It needs encryption of data at rest and in transit to protect client confidentiality. It must incorporate advanced redaction capabilities tuned to industry-specific sensitivity categories rather than the generic email and phone number patterns our demonstration employed.\n","\n","Production systems also require robust error handling and graceful degradation beyond what we implemented. If the AI service becomes temporarily unavailable, the system should queue pending stage executions rather than failing entire cases. If a particular case repeatedly fails validation at a specific stage, the system should escalate to human review rather than retrying indefinitely. If token limits threaten to truncate critical context, the system should intelligently summarize rather than arbitrarily truncating. These operational concerns are solvable engineering challenges, not fundamental limitations of the architecture.\n","\n","Perhaps most importantly, production deployment requires organizational change management that extends beyond technology implementation. Partners must understand that their role evolves from performing all analytical work personally to reviewing and calibrating AI outputs at gates. Junior consultants must learn that their training now includes understanding AI-generated analysis, identifying its limitations, and proposing improvements rather than generating every framework from scratch. Knowledge management teams must maintain libraries of successful case patterns that inform prompt engineering and workflow refinement. Legal and compliance functions must establish policies governing appropriate AI usage, client disclosure requirements, and documentation retention standards.\n","\n","**The Philosophical Stakes: Automation Versus Augmentation**\n","\n","The architecture we have demonstrated embodies a particular philosophical stance about the role of artificial intelligence in knowledge workâ€”a stance that diverges sharply from the dominant narrative of automation replacing human workers. We reject the framing that views AI capability and human employment as zero-sum competitors where gains for one necessarily mean losses for the other. Instead, we embrace augmentation: AI handles tasks that are structured, repeatable, and amenable to pattern recognition, while humans handle tasks requiring contextual judgment, stakeholder calibration, and professional accountability.\n","\n","This augmentation philosophy has profound implications for how consulting firms should approach AI adoption. The goal is not to minimize headcount by automating as much work as possible. The goal is to elevate the work that humans perform by offloading structured analytical tasks to AI, freeing senior professionals to focus on the judgment calls, relationship management, and strategic guidance that genuinely require human expertise. An associate who previously spent eighty percent of time building issue trees and formatting slide decks can now spend that time on client interviews, stakeholder mapping, and change management planning. A partner who previously reviewed junior work for basic analytical soundness can now focus on calibrating recommendations to political realities and coaching clients through implementation challenges.\n","\n","This shift does not eliminate the need for human consultantsâ€”it makes their contributions more valuable by concentrating their effort on the highest-leverage activities. The workflow agent architecture facilitates this shift by creating clear boundaries: AI operates within stages following explicit instructions, humans operate at gates exercising contextual judgment. The separation of concerns allows both to excel at what they do best.\n","\n","**The Broader Applicability Beyond Consulting**\n","\n","While we have framed this entire notebook around management consulting scenarios, the supervised workflow agent architecture applies to any professional service domain where multi-step analytical processes occur, where professional accountability matters, where assumptions must be explicit, and where audit trails serve compliance or quality assurance functions. Legal research follows a similar pattern: intake and case classification, statutory and case law analysis, argument construction, brief drafting, citation verification, and final packaging. Medical diagnosis and treatment planning proceeds through patient history intake, symptom analysis, differential diagnosis generation, treatment option evaluation, verification of contraindications, and care plan documentation.\n","\n","Financial advisory services structure investment recommendations through client profile assessment, risk tolerance evaluation, portfolio option generation, tax implication analysis, regulatory compliance verification, and presentation package assembly. Each of these domains could benefit from the same architectural principles: specialized agents for different analytical stages, accumulation of context across stages, mandatory human checkpoints at critical transitions, comprehensive governance documentation, and explicit separation between AI drafting and human judgment.\n","\n","The transferability of this architecture across professional domains suggests that we are not building isolated point solutions but discovering design patterns that will prove foundational for AI deployment in knowledge work generally. Just as web applications converged on design patterns like model-view-controller architecture and RESTful APIs, professional AI systems may converge on patterns like stage-based workflows, gate-enforced human oversight, and governance-first artifact generation.\n","\n","**Final Reflections: Process as the Product**\n","\n","The deepest lesson of this notebook is that in professional services augmented by artificial intelligence, the process becomes as important as the output. Traditional consulting delivers a recommendation document. AI-augmented consulting delivers that document plus the complete record of how it was created: what the AI produced at each stage, what assumptions it made, what risks it detected, what uncertainties it acknowledged, where humans reviewed and approved progression, and what verification remains necessary before the analysis can be considered final.\n","\n","This process transparency is not overhead to be minimized but value to be celebrated. Clients increasingly want to understand not just what you recommend but how you reached that recommendation. Regulators increasingly demand documentation showing that human oversight occurred at defined decision points. Quality assurance teams increasingly need data revealing where analytical processes succeed and where they struggle. The governance bundle our architecture generates provides all of this automatically, as a natural byproduct of the workflow execution itself.\n","\n","We have built a system where doing the work right and documenting the work completely are inseparableâ€”you cannot execute the workflow without generating the audit trail. This fusion of execution and documentation, of capability and accountability, of autonomy and oversight, represents the future of artificial intelligence in professional services. Not autonomous agents operating unsupervised. Not chatbots providing casual assistance. But supervised workflow agents collaborating with human professionals within governance frameworks that make transparency, traceability, and accountability not aspirational goals but architectural guarantees.\n","\n","This is the future we have demonstrated. This is the future we can deploy. This is the future professional services deserve."],"metadata":{"id":"aV65a4QjNzQV"}}]}