{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNkt6sDgeU+M2PonO9fmGmQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**AI CONSULTING CHAPTER 4: INNOVATORS**\n","\n","---"],"metadata":{"id":"cnQK8yTVMAhC"}},{"cell_type":"markdown","source":["##0.REFERENCE"],"metadata":{"id":"tw2-7uYoMGHg"}},{"cell_type":"markdown","source":["https://claude.ai/share/966ce083-0740-4c4f-80bd-ab640aa81204"],"metadata":{"id":"axSMA0YpNnYX"}},{"cell_type":"markdown","source":["##1.CONTEXT"],"metadata":{"id":"4LFo5LtiMIff"}},{"cell_type":"markdown","source":["**Introduction: Why Level 4 Matters for Strategic Professionals**\n","\n","Welcome to Chapter Four of our journey through AI-assisted consulting. If you've worked through the previous chapters, you've learned how to use AI safely for one-off tasks, how to build reliable workflows with structured outputs, and how to create sophisticated multi-step analyses. Those capabilities are valuable, but they share a common limitation - they're disposable. You create something, use it once, and move on.\n","\n","Level 4 represents a fundamental shift in thinking. We're no longer creating individual outputs. We're building reusable assets that can be deployed repeatedly across your organization. Think of the difference between cooking a single meal and developing a restaurant recipe that dozens of chefs will execute hundreds of times. The standards change completely.\n","\n","**Understanding the Innovator Mindset**\n","\n","We call Level 4 practitioners Innovators because they're not just using AI - they're creating infrastructure for others to use AI safely and effectively. You're building templates, playbooks, and evaluation systems that enable broader adoption while maintaining governance standards. This is the level where AI moves from being a personal productivity tool to becoming organizational capability.\n","\n","Consider a typical scenario. Your firm has successfully used AI to draft market entry memos for three different client engagements. Each time, a senior consultant carefully crafted prompts, reviewed outputs, and integrated the results into final deliverables. The work was good, but it wasn't scalable. Each engagement required the same expert oversight.\n","\n","At Level 4, you take that successful pattern and systematize it. You create a Market Entry Memo Shell - a complete asset bundle with standardized prompts, quality checks, and usage guidelines. Now junior team members can generate initial drafts without senior supervision. The asset enforces quality automatically through built-in validation. Experts focus their time on review and refinement rather than basic drafting.\n","\n","**Why Reuse Increases Risk**\n","\n","Here's the paradox that makes Level 4 challenging. Reusable assets are more valuable precisely because they get used more often. But broader usage also means bigger consequences when things go wrong. If one person uses a flawed prompt and gets a bad output, that's an isolated incident. If fifty people use the same flawed template over six months, you've systematically produced fifty bad outputs.\n","\n","This is why Level 4 demands governance infrastructure that earlier levels could skip. You need evaluation harnesses that test assets against synthetic cases before anyone uses them. You need regression baselines that detect when quality degrades over time. You need change management processes that track modifications and require re-approval. You need comprehensive logging so you can audit what happened when questions arise.\n","\n","Traditional consulting doesn't have good models for this. When senior partners develop methodologies, they spread through apprenticeship and judgment-based adaptation. There's no formal testing, no regression detection, no automated quality control. That approach worked when deployment was slow and expert oversight was constant. It fails when assets get deployed broadly and used independently.\n","\n","**The Governance-First Philosophy**\n","\n","Level 4 is not about making AI smarter or more capable. It's about making AI deployable at scale with appropriate controls. Every decision in this notebook reflects that priority. We don't ask Claude to generate more sophisticated analysis. We ask it to produce outputs that can be validated programmatically, that separate facts from assumptions clearly, that flag risks automatically, and that never generate recommendations disguised as neutral analysis.\n","\n","This governance-first approach frustrates some users initially. Why can't the asset just tell me the best option? Why all these disclaimers and verification requirements? Why must everything be logged and tracked? The answer is accountability. When fifty people use an asset, you need mechanisms to ensure consistent quality and clear audit trails. Informal judgment doesn't scale.\n","\n","Think about how pharmaceutical companies develop drugs. They don't just find something that works in one patient. They conduct systematic trials, document every step, track adverse events, and maintain rigorous quality control. Not because they don't trust their scientists, but because broader deployment demands higher standards. Level 4 applies similar thinking to AI-assisted work products.\n","\n","**What This Notebook Teaches**\n","\n","Over the next ten cells, you'll build a complete Level 4 system from scratch. You'll create four different Asset Bundles spanning common consulting scenarios - market entry analysis, cost transformation planning, investment committee preparation, and organizational design. More importantly, you'll implement the governance infrastructure that makes these assets trustworthy.\n","\n","You'll learn how to automatically redact sensitive information so assets can be tested safely. You'll build robust JSON parsing that handles the messiness of AI outputs. You'll create evaluation harnesses that test assets against synthetic cases and detect regressions. You'll generate comprehensive audit trails capturing every decision and every risk. You'll package everything into professional deliverables ready for stakeholder review.\n","\n","The notebook is deliberately designed for management consultants and strategy professionals without deep technical backgrounds. You don't need to understand machine learning, write complex code, or master statistical methods. The focus is on business logic and governance processes. The technical implementation is handled through clear, commented code that you can use as-is.\n","\n","**Setting Realistic Expectations**\n","\n","Let me be direct about what this notebook does not do. It does not create production-ready assets that you can immediately deploy across your organization. Everything generated here carries a draft label and requires human review. The evaluation uses only synthetic test cases, not real client scenarios. The quality thresholds are demonstrations, not validated standards.\n","\n","What the notebook does provide is the complete framework and methodology for creating production assets. You'll understand what components an asset bundle needs, what testing looks like, what approval workflows should include, and what documentation stakeholders require. You can take these patterns and apply them to your specific context with appropriate rigor.\n","\n","Think of this as learning restaurant operations by running a practice service. You'll go through all the motions - prep, cooking, plating, service - with practice ingredients and volunteer diners. It's not a real restaurant opening, but it teaches you the systems and processes you'll need when you do open for real.\n","\n","**The Path Forward**\n","\n","Level 4 is not the final destination. It's an intermediate stage between individual productivity and full organizational deployment. Some of you will stop here, using these techniques to create small libraries of internal assets for your immediate teams. Others will continue to Level 5, implementing sophisticated deployment infrastructure with monitoring, feedback loops, and continuous improvement.\n","\n","But regardless of where you ultimately land, understanding Level 4 thinking is essential. It teaches you to see AI not as a magic answer machine but as a capability requiring systematic development, rigorous testing, and ongoing governance. That mindset separates sustainable AI adoption from the hype cycles that promise transformation but deliver disappointment.\n","\n","Let's begin building your first Asset Bundle."],"metadata":{"id":"6n-x0MLbMKUs"}},{"cell_type":"markdown","source":["##2.LIBRARIES AND ENVIRONMENT"],"metadata":{"id":"HzSSYRQ0MKs8"}},{"cell_type":"code","source":["# Cell 2: Install + Imports + Run Directory\n","\n","# Install Anthropic SDK\n","!pip install -q anthropic\n","\n","# Standard library imports\n","import json\n","import os\n","import re\n","import hashlib\n","import uuid\n","from datetime import datetime, timezone\n","from pathlib import Path\n","import textwrap\n","import random\n","\n","# Create base run directory with timestamp + short ID\n","timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')\n","short_id = str(uuid.uuid4())[:8]\n","run_name = f\"run_{timestamp}_{short_id}\"\n","base_dir = Path(f\"/content/ai_consulting_ch4_runs/{run_name}\")\n","\n","# Create required folders\n","folders = [\n","    base_dir,\n","    base_dir / \"deliverables\",\n","    base_dir / \"asset_bundles\",\n","    base_dir / \"stage_outputs\",\n","    base_dir / \"logs\"\n","]\n","\n","for folder in folders:\n","    folder.mkdir(parents=True, exist_ok=True)\n","\n","print(\"âœ“ Run directory created:\")\n","print(f\"  Base: {base_dir}\")\n","print(f\"  Deliverables: {base_dir / 'deliverables'}\")\n","print(f\"  Asset Bundles: {base_dir / 'asset_bundles'}\")\n","print(f\"  Logs: {base_dir / 'logs'}\")\n","print(f\"\\nâœ“ Run ID: {run_name}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvaYPv9Qckf_","executionInfo":{"status":"ok","timestamp":1768915200981,"user_tz":360,"elapsed":5011,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"2e7bd1fa-1cfb-4da5-c87b-a3ebd9f9cf50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ Run directory created:\n","  Base: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b\n","  Deliverables: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/deliverables\n","  Asset Bundles: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/asset_bundles\n","  Logs: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/logs\n","\n","âœ“ Run ID: run_20260120_132001_54ba806b\n"]}]},{"cell_type":"markdown","source":["##3.API AND CLIENT INITIALIZATION"],"metadata":{"id":"STtjBQgwMNHi"}},{"cell_type":"markdown","source":["###3.1.OVERVIEW"],"metadata":{"id":"ShWjt_vyMO17"}},{"cell_type":"markdown","source":["**Cell 3: Connecting to Claude's API**\n","\n","Welcome everyone. In this cell, we're establishing the connection between your Google Colab notebook and Anthropic's Claude API. Think of this as setting up your phone to make calls - before you can use the service, you need to authenticate yourself.\n","\n","**What's happening here**\n","\n","First, we retrieve your API key from Colab's secure storage system. An API key is like a password that proves you have permission to use Claude's services. We store it in Colab Secrets rather than directly in the code for security reasons - this way, if you share your notebook with colleagues, your credentials remain private.\n","\n","Once we have the key, we create what's called a client object. This is your dedicated communication channel to Claude. Every time you want Claude to analyze something or generate content, you'll send your request through this client.\n","\n","**The model configuration**\n","\n","We also lock in three critical parameters that control how Claude behaves. The model name specifies exactly which version of Claude you're using - in this case, Claude Sonnet 4.5. The temperature setting of 0.2 keeps Claude's responses focused and consistent, which is essential for professional work. A higher temperature would make responses more creative but less predictable, which isn't what we want when creating reusable business assets.\n","\n","The max tokens parameter sets a limit on response length. Tokens are roughly equivalent to words, so 4,128 tokens means Claude can generate responses of about three to four thousand words. This is sufficient for most consulting deliverables while keeping costs manageable.\n","\n","**Why this matters**\n","\n","By fixing these parameters at the start, we ensure every asset created in this session behaves consistently. If you run this notebook next month with the same settings, you should get comparable results. This reproducibility is crucial for enterprise use - you can't have your templates generating wildly different outputs depending on when someone runs them.\n","\n","If the API key fails to load, the cell will stop execution immediately and display clear instructions for adding your key to Colab Secrets. This fail-fast approach prevents you from wasting time running subsequent cells that would inevitably fail."],"metadata":{"id":"n_uMwdv8MRcU"}},{"cell_type":"markdown","source":["###3.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"Xml9f9GoMR7J"}},{"cell_type":"code","source":["# Cell 3: API Key + Client Initialization\n","\n","import anthropic\n","from google.colab import userdata\n","\n","# Retrieve API key from Colab Secrets\n","try:\n","    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n","    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n","    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n","    api_key_loaded = True\n","    print(\"âœ“ API key loaded: YES\")\n","except Exception as e:\n","    api_key_loaded = False\n","    print(f\"âœ— API key loaded: NO\")\n","    print(f\"  Error: {e}\")\n","    print(\"  â†’ Set ANTHROPIC_API_KEY in Colab Secrets (left sidebar: ðŸ”‘)\")\n","    raise\n","\n","# Model configuration (non-negotiable)\n","MODEL = \"claude-haiku-4-5-20251001\"\n","TEMPERATURE = 0.2\n","MAX_TOKENS = 4128\n","\n","print(f\"\\nâœ“ Model: {MODEL}\")\n","print(f\"âœ“ Temperature: {TEMPERATURE}\")\n","print(f\"âœ“ Max Tokens: {MAX_TOKENS}\")"],"metadata":{"id":"UExynb0iMJwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768915201628,"user_tz":360,"elapsed":645,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"09df4865-9186-4583-a39e-da424fccca3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ API key loaded: YES\n","\n","âœ“ Model: claude-haiku-4-5-20251001\n","âœ“ Temperature: 0.2\n","âœ“ Max Tokens: 4128\n"]}]},{"cell_type":"markdown","source":["##4.GOVERNANCE ARTIFACTS AND REGISTRY INITIALIZATION"],"metadata":{"id":"zSjFjeDuMaZn"}},{"cell_type":"markdown","source":["###4.1.OVERVIEW"],"metadata":{"id":"6OWXlpZoMcHa"}},{"cell_type":"markdown","source":["**Cell 4: Setting Up Your Governance Foundation**\n","\n","Welcome to what I consider the heart of Level 4 thinking. This cell doesn't generate any business content - instead, it builds the scaffolding that makes reusable assets trustworthy and auditable. Think of this as constructing the foundation before building a house.\n","\n","**Understanding helper functions**\n","\n","We start by defining several utility functions that will be used throughout the notebook. These are simple tools that perform routine tasks like generating timestamps, creating secure hashes of text, and reading or writing files. The reason we define them upfront is efficiency - rather than writing the same code repeatedly, we create these reusable building blocks once.\n","\n","The timestamp function ensures every action is recorded with precise timing in UTC format. The hashing function creates unique fingerprints of text without storing the actual content, which protects confidentiality while maintaining traceability. These might seem like technical details, but they're essential for governance.\n","\n","**Creating the audit trail**\n","\n","The cell then creates what we call the run manifest. This is a comprehensive record of your session - which model you used, what settings you applied, when you ran it, and a unique identifier for this specific run. Six months from now, if someone questions an asset created today, you can trace it back to this exact configuration.\n","\n","**Initializing governance logs**\n","\n","Next, we create several empty log files that will track different aspects of the session. The risk log captures every potential issue Claude identifies. The verification register tracks claims that need human fact-checking. The change log records every modification to assets. The approvals log manages who needs to sign off before assets go live.\n","\n","Think of these as your compliance documentation. In regulated industries or large organizations, you need proof that proper procedures were followed. These logs provide that evidence automatically.\n","\n","**Level 4 specific tracking**\n","\n","We also create the asset registry, which is your master inventory of everything created in this session, and the regression baseline, which stores quality metrics so future runs can be compared against current performance. If quality degrades over time, you'll know immediately.\n","\n","The configuration hash is particularly clever. It creates a unique identifier based on your model settings. If someone later claims they used the same setup but got different results, you can verify whether their configuration actually matches yours.\n","\n","This infrastructure might feel like overhead, but it's what separates professional asset creation from casual experimentation."],"metadata":{"id":"XdRc9-nEMeIF"}},{"cell_type":"markdown","source":["###4.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"GvnsRIc6Mefx"}},{"cell_type":"code","source":["# Cell 4: Governance Artifacts + Asset Registry Initialization\n","\n","# Helper functions\n","def now_iso():\n","    \"\"\"Return current UTC timestamp in ISO format.\"\"\"\n","    return datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')\n","\n","def sha256_text(text):\n","    \"\"\"Return SHA-256 hash of text.\"\"\"\n","    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n","\n","def write_json(filepath, data):\n","    \"\"\"Write JSON to file with pretty formatting.\"\"\"\n","    with open(filepath, 'w', encoding='utf-8') as f:\n","        json.dump(data, f, indent=2, ensure_ascii=False)\n","\n","def read_json(filepath):\n","    \"\"\"Read JSON from file.\"\"\"\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        return json.load(f)\n","\n","def append_jsonl(filepath, record):\n","    \"\"\"Append a JSON record to a JSONL file.\"\"\"\n","    with open(filepath, 'a', encoding='utf-8') as f:\n","        f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n","\n","def get_env_fingerprint():\n","    \"\"\"Get environment fingerprint for reproducibility.\"\"\"\n","    import platform\n","    return {\n","        \"python_version\": platform.python_version(),\n","        \"platform\": platform.platform(),\n","        \"timestamp\": now_iso()\n","    }\n","\n","def stable_config_hash():\n","    \"\"\"Generate stable hash of model configuration.\"\"\"\n","    config_str = f\"{MODEL}|{TEMPERATURE}|{MAX_TOKENS}\"\n","    return sha256_text(config_str)[:16]\n","\n","# Initialize run_manifest.json\n","manifest = {\n","    \"chapter\": \"4\",\n","    \"level\": \"Innovators\",\n","    \"purpose\": \"Reusable internal assets with evaluation and controlled release\",\n","    \"model\": MODEL,\n","    \"temperature\": TEMPERATURE,\n","    \"max_tokens\": MAX_TOKENS,\n","    \"run_id\": run_name,\n","    \"created_at\": now_iso(),\n","    \"config_hash\": stable_config_hash(),\n","    \"environment\": get_env_fingerprint()\n","}\n","write_json(base_dir / \"run_manifest.json\", manifest)\n","\n","# Initialize base governance artifacts\n","write_json(base_dir / \"logs\" / \"risk_log.json\", {\"risks\": []})\n","write_json(base_dir / \"logs\" / \"verification_register.json\", {\"entries\": []})\n","write_json(base_dir / \"logs\" / \"change_log.json\", {\"changes\": []})\n","write_json(base_dir / \"logs\" / \"approvals_log.json\", {\"approvals\": []})\n","write_json(base_dir / \"logs\" / \"exception_log.json\", {\"exceptions\": []})\n","\n","# Initialize Level 4 specific artifacts\n","write_json(base_dir / \"asset_registry.json\", {\"assets\": []})\n","write_json(base_dir / \"regression_baseline.json\", {})\n","\n","# Create empty JSONL files\n","Path(base_dir / \"logs\" / \"prompts_log.jsonl\").touch()\n","Path(base_dir / \"logs\" / \"evaluation_harness_log.jsonl\").touch()\n","\n","print(\"âœ“ Governance artifacts initialized:\")\n","print(f\"  run_manifest.json\")\n","print(f\"  logs/prompts_log.jsonl\")\n","print(f\"  logs/risk_log.json\")\n","print(f\"  logs/verification_register.json\")\n","print(f\"  logs/change_log.json\")\n","print(f\"  logs/approvals_log.json\")\n","print(f\"  logs/exception_log.json\")\n","print(f\"\\nâœ“ Level 4 artifacts initialized:\")\n","print(f\"  asset_registry.json\")\n","print(f\"  regression_baseline.json\")\n","print(f\"  logs/evaluation_harness_log.jsonl\")\n","print(f\"\\nâœ“ Config hash: {stable_config_hash()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThXIE63DdLh4","executionInfo":{"status":"ok","timestamp":1768915204048,"user_tz":360,"elapsed":36,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"1c608b62-8471-4c32-d82e-d9147463d5fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ Governance artifacts initialized:\n","  run_manifest.json\n","  logs/prompts_log.jsonl\n","  logs/risk_log.json\n","  logs/verification_register.json\n","  logs/change_log.json\n","  logs/approvals_log.json\n","  logs/exception_log.json\n","\n","âœ“ Level 4 artifacts initialized:\n","  asset_registry.json\n","  regression_baseline.json\n","  logs/evaluation_harness_log.jsonl\n","\n","âœ“ Config hash: d2ef82f08f0c34b9\n"]}]},{"cell_type":"markdown","source":["##5.CONFINDENTIALITY AND MINIMUN NECESSARY UTILITIES"],"metadata":{"id":"aFfOfCZmMgx4"}},{"cell_type":"markdown","source":["###5.1.OVERVIEW"],"metadata":{"id":"tIlOMU-SMjni"}},{"cell_type":"markdown","source":["**Cell 5: Protecting Confidential Information**\n","\n","This cell addresses one of the most critical risks when using AI in professional services - accidentally exposing sensitive client or proprietary information. Every consulting firm has horror stories about confidential data ending up where it shouldn't. This cell builds your first line of defense.\n","\n","**The redaction mechanism**\n","\n","We create a function that automatically scans text for patterns that typically indicate sensitive information - email addresses, phone numbers, social security numbers, and similar identifiers. When it finds these patterns, it replaces them with a placeholder tag. This happens automatically before any text gets sent to Claude's API.\n","\n","Now, this is a simplified demonstration. In a production environment, you would deploy more sophisticated tools - named entity recognition systems that identify people's names, companies, financial figures, or proprietary terminology specific to your organization. But the principle remains the same: sanitize inputs before they leave your control.\n","\n","**Minimum necessary principle**\n","\n","The second function implements what privacy professionals call the minimum necessary standard. Just because you have a fifty-page document doesn't mean you should send all fifty pages to the AI. This function helps you extract only the essential information needed to complete the task, removing extraneous details that might contain sensitive information.\n","\n","It also tracks what was removed. This creates an audit trail showing you actively protected confidential information rather than carelessly exposing it. If you ever face questions about data handling, you have documentation proving you followed proper protocols.\n","\n","**Asset guardrails**\n","\n","Finally, we define standard warnings that will be embedded in every asset created today. These guardrails remind users that outputs are drafts requiring verification, that recommendations are prohibited, and that fabricating facts is unacceptable. Think of these as the safety labels that appear on every piece of equipment.\n","\n","**Why this matters for reusable assets**\n","\n","When you create a one-time analysis, you control the entire process and can manually redact sensitive information. But when you create a reusable template that fifty people will use over the next year, you cannot manually review every instance. You need automated protections built into the asset itself.\n","\n","The demonstration at the end shows you the redaction in action - original text containing contact information gets transformed into a sanitized version suitable for processing. This visible proof helps users trust that the system is actually protecting their data."],"metadata":{"id":"ksHvcArXMxMR"}},{"cell_type":"markdown","source":["###5.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"x7z8cMnQMx3g"}},{"cell_type":"code","source":["# Cell 5: Confidentiality + Minimum-Necessary Utilities\n","\n","def redact(text, placeholder=\"[REDACTED]\"):\n","    \"\"\"\n","    Redact sensitive patterns from text.\n","\n","    This is a simple demonstration. In production:\n","    - Use named entity recognition for PII\n","    - Apply domain-specific redaction rules\n","    - Log what was removed for audit\n","    \"\"\"\n","    # Redact email-like patterns\n","    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n","                  placeholder, text)\n","    # Redact phone-like patterns\n","    text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', placeholder, text)\n","    # Redact SSN-like patterns\n","    text = re.sub(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', placeholder, text)\n","    return text\n","\n","def build_minimum_necessary(raw_text):\n","    \"\"\"\n","    Extract minimum necessary information and log removed fields.\n","\n","    Returns:\n","        dict with 'sanitized_text' and 'removed_fields'\n","    \"\"\"\n","    sanitized = redact(raw_text)\n","\n","    # Track what was removed (simplified example)\n","    removed_fields = []\n","    if raw_text != sanitized:\n","        removed_fields.append(\"PII patterns detected and redacted\")\n","\n","    # Remove excessive detail (example: truncate very long inputs)\n","    if len(sanitized) > 2000:\n","        sanitized = sanitized[:2000] + \"... [TRUNCATED]\"\n","        removed_fields.append(\"Input truncated to 2000 chars\")\n","\n","    return {\n","        \"sanitized_text\": sanitized,\n","        \"removed_fields\": removed_fields\n","    }\n","\n","def asset_guardrails():\n","    \"\"\"\n","    Return standard guardrails text for all asset prompt packs.\n","    \"\"\"\n","    return \"\"\"\n","CRITICAL GUARDRAILS:\n","1. All outputs must include verification_status: \"Not verified\"\n","2. Never provide recommendations or rankings\n","3. No fabricated facts, benchmarks, or citations\n","4. Separate: facts_provided / assumptions / open_questions / draft_output\n","5. Flag confidentiality risks and missing information\n","6. This is a DRAFT asset requiring human review before release\n","\"\"\"\n","\n","# Demo redaction\n","demo_text = \"\"\"\n","Our client contact is john.doe@example.com and can be reached at 555-123-4567.\n","The project involves analyzing revenue data across 15 markets.\n","SSN on file: 123-45-6789.\n","\"\"\"\n","\n","result = build_minimum_necessary(demo_text)\n","\n","print(\"âœ“ Confidentiality utilities loaded\")\n","print(\"\\n--- DEMO: Redaction ---\")\n","print(\"\\nBefore redaction:\")\n","print(demo_text)\n","print(\"\\nAfter redaction:\")\n","print(result['sanitized_text'])\n","print(\"\\nRemoved fields:\")\n","for field in result['removed_fields']:\n","    print(f\"  - {field}\")\n","print(\"\\nâœ“ Asset guardrails ready for prompt packs\")"],"metadata":{"id":"_OdMQ8SwMiEp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768915206231,"user_tz":360,"elapsed":11,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"c8c86515-7365-4744-c047-d5dbfd0384de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ Confidentiality utilities loaded\n","\n","--- DEMO: Redaction ---\n","\n","Before redaction:\n","\n","Our client contact is john.doe@example.com and can be reached at 555-123-4567.\n","The project involves analyzing revenue data across 15 markets.\n","SSN on file: 123-45-6789.\n","\n","\n","After redaction:\n","\n","Our client contact is [REDACTED] and can be reached at [REDACTED].\n","The project involves analyzing revenue data across 15 markets.\n","SSN on file: [REDACTED].\n","\n","\n","Removed fields:\n","  - PII patterns detected and redacted\n","\n","âœ“ Asset guardrails ready for prompt packs\n"]}]},{"cell_type":"markdown","source":["##6.LLM WRAPPER"],"metadata":{"id":"uVJjvhvVM0E8"}},{"cell_type":"markdown","source":["###6.1.OVERVIEW"],"metadata":{"id":"MyxD2F2XM1J3"}},{"cell_type":"markdown","source":["**Cell 6: Building a Bulletproof Communication System**\n","\n","This is where we build what I call the reliability engine. When you use AI in professional settings, you cannot afford responses that fail to parse, violate your policies, or produce unusable output. This cell creates a wrapper that catches and fixes common problems before they derail your work.\n","\n","**The JSON challenge**\n","\n","Claude communicates in natural language, but we need structured data we can validate and process programmatically. We ask Claude to return responses in JSON format - think of it as a standardized form with specific fields. However, AI models sometimes add extra formatting like markdown code fences, or make small syntax errors like trailing commas that break parsers.\n","\n","The first set of functions handles these quirks. They strip away formatting artifacts, extract the actual JSON content from surrounding text, and repair common syntax issues. This is less exciting than the AI itself, but absolutely essential. A response that cannot be parsed is useless, regardless of how insightful the content might be.\n","\n","**Enforcing exact specifications**\n","\n","The validation function is where we get strict. It checks that the response contains exactly the keys we specified - no more, no less. Extra keys suggest Claude misunderstood instructions. Missing keys mean we lack critical information. We validate data types to ensure lists are actually lists and strings are actually strings.\n","\n","Most importantly, we enforce policy requirements. Every response must have verification status set to not verified. We scan the draft output for prohibited language like recommend, best option, or ranked number one. If we find these phrases, we reject the response entirely. This might seem harsh, but it prevents the single biggest risk in AI-assisted work - decision laundering, where people treat AI suggestions as authoritative recommendations.\n","\n","**The retry mechanism**\n","\n","When validation fails, we don't just give up. The wrapper automatically retries the API call, this time including specific feedback about what went wrong. Claude sees the validation errors and can correct them. We allow up to three attempts before finally failing.\n","\n","**Comprehensive logging**\n","\n","Every API call gets logged with a timestamp, a hash of the prompt for traceability, and the parsing outcome. Risks get automatically detected and recorded - if there are too many open questions, that suggests insufficient input data. If there are no assumptions listed, that suggests poor traceability.\n","\n","**The smoke test**\n","\n","Finally, we run an actual API call to prove the system works end to end. This isn't just checking syntax - we verify that we can call Claude, receive a response, parse it successfully, and validate it against our schema. This fail-fast approach means if something is misconfigured, you discover it immediately rather than after running expensive evaluations."],"metadata":{"id":"NbI8D2MMNp2V"}},{"cell_type":"markdown","source":["###6.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"HYi01MJHM25b"}},{"cell_type":"code","source":["# Cell 6: LLM Wrapper (ROBUST JSON REPAIR + VALIDATION + ACTUAL API SMOKE TEST)\n","\n","def fix_json_string(s):\n","    \"\"\"Remove trailing commas that break JSON parsing.\"\"\"\n","    s = re.sub(r',\\s*}', '}', s)\n","    s = re.sub(r',\\s*]', ']', s)\n","    return s\n","\n","def extract_json_robust(text):\n","    \"\"\"\n","    Extract JSON from text, handling common formatting issues.\n","    \"\"\"\n","    # Remove markdown code fences\n","    text = re.sub(r'```json\\s*', '', text)\n","    text = re.sub(r'```\\s*', '', text)\n","\n","    # Try to find JSON object\n","    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n","    if match:\n","        json_str = match.group(0)\n","        json_str = fix_json_string(json_str)\n","        return json_str\n","\n","    return text\n","\n","def validate_output_json(data, asset_id=None):\n","    \"\"\"\n","    Validate output JSON against exact schema requirements.\n","\n","    Returns: (is_valid, issues_list)\n","    \"\"\"\n","    issues = []\n","\n","    # Check required keys (exact set)\n","    required_keys = {\n","        \"task\", \"facts_provided\", \"assumptions\", \"open_questions\",\n","        \"risks\", \"draft_output\", \"verification_status\", \"questions_to_verify\"\n","    }\n","\n","    actual_keys = set(data.keys())\n","\n","    if actual_keys != required_keys:\n","        missing = required_keys - actual_keys\n","        extra = actual_keys - required_keys\n","        if missing:\n","            issues.append(f\"Missing keys: {missing}\")\n","        if extra:\n","            issues.append(f\"Extra keys not allowed: {extra}\")\n","\n","    # Check types\n","    if not isinstance(data.get(\"facts_provided\", None), list):\n","        issues.append(\"facts_provided must be a list\")\n","    if not isinstance(data.get(\"assumptions\", None), list):\n","        issues.append(\"assumptions must be a list\")\n","    if not isinstance(data.get(\"open_questions\", None), list):\n","        issues.append(\"open_questions must be a list\")\n","    if not isinstance(data.get(\"risks\", None), list):\n","        issues.append(\"risks must be a list\")\n","    if not isinstance(data.get(\"questions_to_verify\", None), list):\n","        issues.append(\"questions_to_verify must be a list\")\n","\n","    # Check verification_status\n","    if data.get(\"verification_status\") != \"Not verified\":\n","        issues.append('verification_status must be exactly \"Not verified\"')\n","\n","    # Check for prohibited content (recommendations/rankings)\n","    draft_output = data.get(\"draft_output\", \"\")\n","    prohibited_phrases = [\n","        \"recommend\", \"best option\", \"top choice\", \"should select\",\n","        \"ranked #1\", \"optimal choice\", \"advise choosing\"\n","    ]\n","    for phrase in prohibited_phrases:\n","        if phrase.lower() in draft_output.lower():\n","            issues.append(f'Prohibited language detected in draft_output: \"{phrase}\"')\n","\n","    # Validate risk structure\n","    for i, risk in enumerate(data.get(\"risks\", [])):\n","        if not isinstance(risk, dict):\n","            issues.append(f\"Risk {i} must be a dict\")\n","            continue\n","\n","        valid_types = {\n","            \"confidentiality\", \"hallucination\", \"missing_facts\", \"traceability\",\n","            \"false_rigor\", \"decision_laundering\", \"scope_creep\", \"change_mgmt\",\n","            \"evaluation\", \"other\"\n","        }\n","        if risk.get(\"type\") not in valid_types:\n","            issues.append(f\"Risk {i} has invalid type: {risk.get('type')}\")\n","\n","        valid_severities = {\"low\", \"medium\", \"high\"}\n","        if risk.get(\"severity\") not in valid_severities:\n","            issues.append(f\"Risk {i} has invalid severity: {risk.get('severity')}\")\n","\n","    return (len(issues) == 0, issues)\n","\n","def auto_detect_risks(data):\n","    \"\"\"Auto-detect risks from output structure.\"\"\"\n","    risks = []\n","\n","    if len(data.get(\"open_questions\", [])) > 5:\n","        risks.append({\n","            \"type\": \"missing_facts\",\n","            \"severity\": \"medium\",\n","            \"note\": \"High number of open questions suggests insufficient input\"\n","        })\n","\n","    if len(data.get(\"assumptions\", [])) == 0:\n","        risks.append({\n","            \"type\": \"traceability\",\n","            \"severity\": \"low\",\n","            \"note\": \"No explicit assumptions logged\"\n","        })\n","\n","    return risks\n","\n","def call_claude(task_description, context=\"\", asset_id=None, max_retries=3):\n","    \"\"\"\n","    Call Claude API with robust JSON parsing and validation.\n","\n","    Returns: parsed dict or raises exception\n","    \"\"\"\n","    system_prompt = f\"\"\"You are an AI assistant helping management consultants create draft internal assets.\n","\n","{asset_guardrails()}\n","\n","CRITICAL: Return ONLY valid JSON with EXACTLY these keys:\n","- task (string)\n","- facts_provided (array)\n","- assumptions (array)\n","- open_questions (array)\n","- risks (array of {{type, severity, note}})\n","- draft_output (string)\n","- verification_status (must be \"Not verified\")\n","- questions_to_verify (array)\n","\n","Valid risk types: confidentiality, hallucination, missing_facts, traceability, false_rigor, decision_laundering, scope_creep, change_mgmt, evaluation, other\n","Valid severities: low, medium, high\n","\n","NO extra keys. NO prose outside JSON. NO recommendations or rankings in draft_output.\"\"\"\n","\n","    user_prompt = f\"\"\"Task: {task_description}\n","\n","{context}\n","\n","Return structured JSON output following the exact schema.\"\"\"\n","\n","    prompt_hash = sha256_text(system_prompt + user_prompt)\n","\n","    for attempt in range(max_retries):\n","        try:\n","            # Call API\n","            message = client.messages.create(\n","                model=MODEL,\n","                max_tokens=MAX_TOKENS,\n","                temperature=TEMPERATURE,\n","                system=system_prompt,\n","                messages=[{\"role\": \"user\", \"content\": user_prompt}]\n","            )\n","\n","            raw_output = message.content[0].text\n","\n","            # Extract and parse JSON\n","            json_str = extract_json_robust(raw_output)\n","            data = json.loads(json_str)\n","\n","            # Validate schema\n","            is_valid, issues = validate_output_json(data, asset_id)\n","\n","            if not is_valid:\n","                if attempt < max_retries - 1:\n","                    # Retry with validation feedback\n","                    context += f\"\\n\\nPREVIOUS ATTEMPT FAILED VALIDATION:\\n\" + \"\\n\".join(issues)\n","                    continue\n","                else:\n","                    # Final attempt failed - log and raise\n","                    debug_path = base_dir / \"logs\" / \"debug_malformed_json.txt\"\n","                    with open(debug_path, 'w') as f:\n","                        f.write(f\"Attempt {attempt + 1} - Validation issues:\\n\")\n","                        f.write(\"\\n\".join(issues))\n","                        f.write(f\"\\n\\nRaw output:\\n{raw_output}\")\n","\n","                    append_jsonl(base_dir / \"logs\" / \"exception_log.json\", {\n","                        \"timestamp\": now_iso(),\n","                        \"asset_id\": asset_id,\n","                        \"exception_type\": \"validation_failure\",\n","                        \"issues\": issues,\n","                        \"prompt_hash\": prompt_hash\n","                    })\n","\n","                    raise ValueError(f\"Validation failed after {max_retries} attempts: {issues}\")\n","\n","            # Add auto-detected risks\n","            auto_risks = auto_detect_risks(data)\n","            data[\"risks\"].extend(auto_risks)\n","\n","            # Log to prompts_log.jsonl\n","            append_jsonl(base_dir / \"logs\" / \"prompts_log.jsonl\", {\n","                \"timestamp\": now_iso(),\n","                \"asset_id\": asset_id,\n","                \"prompt_hash\": prompt_hash,\n","                \"model\": MODEL,\n","                \"temperature\": TEMPERATURE,\n","                \"max_tokens\": MAX_TOKENS,\n","                \"parsing_status\": \"success\"\n","            })\n","\n","            # Log risks\n","            risk_log = read_json(base_dir / \"logs\" / \"risk_log.json\")\n","            for risk in data[\"risks\"]:\n","                risk_log[\"risks\"].append({\n","                    \"timestamp\": now_iso(),\n","                    \"asset_id\": asset_id,\n","                    **risk\n","                })\n","            write_json(base_dir / \"logs\" / \"risk_log.json\", risk_log)\n","\n","            return data\n","\n","        except json.JSONDecodeError as e:\n","            if attempt < max_retries - 1:\n","                continue\n","            else:\n","                debug_path = base_dir / \"logs\" / \"debug_malformed_json.txt\"\n","                with open(debug_path, 'w') as f:\n","                    f.write(f\"JSON decode error: {e}\\n\\nRaw output:\\n{raw_output}\")\n","                raise\n","\n","        except Exception as e:\n","            append_jsonl(base_dir / \"logs\" / \"exception_log.json\", {\n","                \"timestamp\": now_iso(),\n","                \"asset_id\": asset_id,\n","                \"exception_type\": type(e).__name__,\n","                \"message\": str(e),\n","                \"prompt_hash\": prompt_hash\n","            })\n","            raise\n","\n","# SMOKE TEST (ACTUAL API CALL)\n","print(\"Running smoke test with ACTUAL API call...\")\n","smoke_test_result = call_claude(\n","    task_description=\"Generate a simple test output to verify the wrapper works correctly\",\n","    context=\"This is a smoke test. Include 2 open questions and 1 assumption.\",\n","    asset_id=\"smoke_test\"\n",")\n","\n","print(\"\\nâœ“ SMOKE TEST PASSED\")\n","print(f\"  Task: {smoke_test_result['task']}\")\n","print(f\"  Open questions: {len(smoke_test_result['open_questions'])}\")\n","print(f\"  Draft output length: {len(smoke_test_result['draft_output'])} chars\")\n","print(f\"  Verification status: {smoke_test_result['verification_status']}\")\n","print(\"\\nâœ“ LLM wrapper ready with robust JSON parsing and validation\")"],"metadata":{"id":"NVWzVmXzM4V4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768915219946,"user_tz":360,"elapsed":11059,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"dda77045-da33-4495-dbe9-0025269e48b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running smoke test with ACTUAL API call...\n","\n","âœ“ SMOKE TEST PASSED\n","  Task: Generate a simple test output to verify the wrapper works correctly\n","  Open questions: 2\n","  Draft output length: 415 chars\n","  Verification status: Not verified\n","\n","âœ“ LLM wrapper ready with robust JSON parsing and validation\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"W9uaT3o9M45h"}},{"cell_type":"markdown","source":["##7.ASSET BUNDLE BUILDER (THE ARTIFACT)"],"metadata":{"id":"Dr34h4v4M8SU"}},{"cell_type":"markdown","source":["###7.1.OVERVIEW"],"metadata":{"id":"EnAuC2ckM_GU"}},{"cell_type":"markdown","source":["**Cell 7: Manufacturing Reusable Assets**\n","\n","Now we transition from infrastructure to actual asset creation. This cell defines the factory process for building what we call Asset Bundles - complete, self-contained packages that your team can deploy repeatedly with confidence. Think of this as creating a product, not just a document.\n","\n","**What makes an asset bundle complete**\n","\n","A traditional consulting approach might create a template document and call it done. Level 4 thinking demands much more. Each asset bundle contains seven distinct components, and this cell automates their creation. You provide four key inputs - the asset name, its purpose, what it's allowed to do, and what it's explicitly forbidden from doing - and the function generates everything else.\n","\n","**The specification document**\n","\n","First, we create a formal specification that defines the asset's scope and boundaries. This includes the intended users, the minimum information required as input, the exact structure of outputs, and prominent risk warnings. The specification also locks in the model version and parameters, so six months from now someone can verify whether they're using the asset as originally designed.\n","\n","Critically, we assign a version number starting at zero point one dash draft. This signals clearly that the asset has not been approved for production use. Nothing leaves this notebook with a released version number - that requires human approval after thorough review.\n","\n","**The prompt pack**\n","\n","Next, we generate the prompt pack - the actual instructions that will be sent to Claude each time someone uses this asset. This isn't just a casual request. It includes the system instructions, a template for user input, explicit guardrails about prohibited behaviors, and prominent warnings about verification requirements.\n","\n","Notice we create this file statically from known text rather than asking Claude to write it. We want complete control over these instructions. Allowing an AI to write its own operating procedures would be recursive and potentially unstable.\n","\n","**The deliverable template**\n","\n","We also create a structured template showing exactly what the output should look like. It has labeled sections for facts provided, assumptions made, open questions, draft output, risks identified, verification status, and questions requiring fact-checking. This structure enforces transparency - users can immediately see what came from source data versus what the AI inferred.\n","\n","**Registry and change management**\n","\n","Every asset gets registered in a central inventory with its creation timestamp and current status. We also log the creation event in the change log, establishing a clear audit trail. If this asset ever gets modified, updated, or retired, those events will be logged in the same system.\n","\n","**Why this matters**\n","\n","Creating all these components manually would be tedious and error-prone. By automating the process, we ensure consistency and completeness. Every asset bundle follows the same structure, includes the same safety features, and maintains the same documentation standards."],"metadata":{"id":"EKvlFydjNrmX"}},{"cell_type":"markdown","source":["###7.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"X1KSLRldNBYW"}},{"cell_type":"code","source":["# Cell 7: Asset Bundle Builder (Spec + Prompt Pack + Template)\n","\n","def build_asset_bundle(asset_name, purpose, scope_boundary, prohibited_uses):\n","    \"\"\"\n","    Create a complete Asset Bundle with all required components.\n","\n","    Returns: asset_id\n","    \"\"\"\n","    asset_id = f\"asset_{asset_name.lower().replace(' ', '_')}_{uuid.uuid4().hex[:8]}\"\n","    asset_dir = base_dir / \"asset_bundles\" / asset_id\n","    asset_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # 1. Create asset_spec.json\n","    asset_spec = {\n","        \"asset_id\": asset_id,\n","        \"name\": asset_name,\n","        \"purpose\": purpose,\n","        \"intended_users\": \"Management consultants and in-house strategy professionals\",\n","        \"scope_boundary\": scope_boundary,\n","        \"prohibited_uses\": prohibited_uses,\n","        \"input_schema\": {\n","            \"minimum_necessary\": True,\n","            \"required_fields\": [\"task_description\", \"context\"],\n","            \"redaction_required\": True\n","        },\n","        \"output_schema\": {\n","            \"format\": \"strict_json\",\n","            \"required_keys\": [\n","                \"task\", \"facts_provided\", \"assumptions\", \"open_questions\",\n","                \"risks\", \"draft_output\", \"verification_status\", \"questions_to_verify\"\n","            ]\n","        },\n","        \"risk_notes\": \"All outputs are drafts requiring human review. No recommendations or rankings permitted.\",\n","        \"model\": MODEL,\n","        \"temperature\": TEMPERATURE,\n","        \"max_tokens\": MAX_TOKENS,\n","        \"version\": \"0.1-draft\",\n","        \"owner\": \"unassigned\",\n","        \"created_at\": now_iso()\n","    }\n","    write_json(asset_dir / \"asset_spec.json\", asset_spec)\n","\n","    # 2. Create prompt_pack.txt\n","    prompt_pack = f\"\"\"ASSET: {asset_name}\n","PURPOSE: {purpose}\n","\n","SCOPE BOUNDARY:\n","{scope_boundary}\n","\n","PROHIBITED USES:\n","{prohibited_uses}\n","\n","{asset_guardrails()}\n","\n","SYSTEM INSTRUCTIONS:\n","You are creating a draft {asset_name.lower()} for management consultants.\n","- Use ONLY information explicitly provided\n","- Clearly separate facts, assumptions, and open questions\n","- Flag all risks (confidentiality, missing information, etc.)\n","- NO recommendations, rankings, or \"best option\" language\n","- Return strict JSON with verification_status: \"Not verified\"\n","\n","USER PROMPT TEMPLATE:\n","---\n","Task: {{task_description}}\n","\n","Context:\n","{{context}}\n","\n","Return structured JSON following the exact schema.\n","---\n","\n","REDACTION WARNING:\n","Never include client-confidential information. All inputs should be pre-redacted.\n","\n","NOT VERIFIED REMINDER:\n","All outputs from this asset are DRAFTS requiring human review before use.\n","\"\"\"\n","    with open(asset_dir / \"prompt_pack.txt\", 'w') as f:\n","        f.write(prompt_pack)\n","\n","    # 3. Create template.txt (deliverable shell)\n","    template = f\"\"\"# {asset_name} (DRAFT - NOT VERIFIED)\n","\n","## Facts Provided\n","[List explicit facts from input]\n","\n","## Assumptions\n","[List assumptions made]\n","\n","## Open Questions\n","[List information gaps and questions to resolve]\n","\n","## Draft Output\n","[Generated content goes here]\n","\n","## Risks Identified\n","[Auto-detected and flagged risks]\n","\n","## Verification Status\n","Not verified - requires human review\n","\n","## Questions to Verify\n","[Specific items requiring fact-checking]\n","\n","---\n","Generated by: AI-Assisted Consulting Level 4 (Innovators)\n","Asset ID: {asset_id}\n","Version: 0.1-draft\n","Timestamp: {now_iso()}\n","\"\"\"\n","    with open(asset_dir / \"template.txt\", 'w') as f:\n","        f.write(template)\n","\n","    # 4. Update asset_registry.json\n","    registry = read_json(base_dir / \"asset_registry.json\")\n","    registry[\"assets\"].append({\n","        \"asset_id\": asset_id,\n","        \"name\": asset_name,\n","        \"version\": \"0.1-draft\",\n","        \"created_at\": now_iso(),\n","        \"status\": \"draft\"\n","    })\n","    write_json(base_dir / \"asset_registry.json\", registry)\n","\n","    # 5. Add to change_log.json\n","    change_log = read_json(base_dir / \"logs\" / \"change_log.json\")\n","    change_log[\"changes\"].append({\n","        \"timestamp\": now_iso(),\n","        \"change_type\": \"asset_created\",\n","        \"asset_id\": asset_id,\n","        \"asset_name\": asset_name,\n","        \"version\": \"0.1-draft\",\n","        \"description\": f\"Created new asset bundle for {asset_name}\"\n","    })\n","    write_json(base_dir / \"logs\" / \"change_log.json\", change_log)\n","\n","    print(f\"âœ“ Asset Bundle created: {asset_id}\")\n","    print(f\"  Location: {asset_dir}\")\n","    print(f\"  Files created:\")\n","    for file in asset_dir.iterdir():\n","        print(f\"    - {file.name}\")\n","\n","    return asset_id\n","\n","# Demo: Create one sample asset\n","sample_asset_id = build_asset_bundle(\n","    asset_name=\"Market Entry Memo Shell\",\n","    purpose=\"Reusable template for market entry analysis memos\",\n","    scope_boundary=\"Structure and scaffolding only; no market-specific recommendations\",\n","    prohibited_uses=\"Direct decision-making; client deliverables without review; external sharing\"\n",")\n","\n","print(f\"\\nâœ“ Asset bundle builder ready\")\n","print(f\"âœ“ Sample asset: {sample_asset_id}\")"],"metadata":{"id":"vw3KXun5NDTt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768915226612,"user_tz":360,"elapsed":14,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"932ded4f-36be-4e4f-a635-6b5d07f67f55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ Asset Bundle created: asset_market_entry_memo_shell_fe37dda7\n","  Location: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/asset_bundles/asset_market_entry_memo_shell_fe37dda7\n","  Files created:\n","    - prompt_pack.txt\n","    - template.txt\n","    - asset_spec.json\n","\n","âœ“ Asset bundle builder ready\n","âœ“ Sample asset: asset_market_entry_memo_shell_fe37dda7\n"]}]},{"cell_type":"markdown","source":["##8.EVALUATION HARNESS: SYNTHETIC CASES PLUS REGRESSON AND LOGGING"],"metadata":{"id":"E9DiNZg2NOyv"}},{"cell_type":"markdown","source":["###8.1.OVERVIEW"],"metadata":{"id":"Ee1sdukLNP3S"}},{"cell_type":"markdown","source":["**Cell 8: Building Quality Control Systems**\n","\n","This cell creates what professional software developers call a test harness - an automated system for evaluating whether your assets actually work as intended. In traditional consulting, quality control happens through senior review after the fact. At Level 4, we build quality checks directly into the asset creation process.\n","\n","**Understanding synthetic test cases**\n","\n","The first function generates five test cases for each type of asset. These are deliberately synthetic - fictional companies, made-up scenarios, non-confidential data. We use synthetic cases because they can be safely stored and reused without privacy concerns. Anyone can examine them, share them, or run them again later.\n","\n","Each test case includes a task description and relevant context, mimicking what a real user might provide. For a market entry memo, we might specify a mid-size retailer entering Southeast Asian consumer electronics. For a cost transformation workplan, perhaps a manufacturing company targeting fifteen percent SG&A reduction. The cases are realistic enough to exercise the asset's capabilities without revealing actual client information.\n","\n","**The evaluation process**\n","\n","The second function runs these test cases through the asset and measures what happens. For each case, we call Claude with the test input and capture the output. Then we validate it against multiple criteria - does it follow the required schema, does it include open questions showing intellectual honesty, does it document assumptions for traceability, does it avoid prohibited recommendation language.\n","\n","Each test gets scored pass or fail on these criteria. We then calculate an overall pass rate - what percentage of test cases the asset handled successfully. This single number becomes your quality metric. If only sixty percent of cases pass, you know the asset needs refinement before deployment.\n","\n","**Progress indicators matter**\n","\n","Notice we include visual feedback showing dots for each completed case. When you're running evaluations that take several minutes, you want confirmation the system is actually working rather than frozen. These simple indicators prevent unnecessary interruptions or restarts.\n","\n","**Regression detection**\n","\n","The third function implements regression testing - comparing current performance against a baseline. The first time you run an asset, we store its pass rate as the baseline. On subsequent runs, we check whether quality has degraded. If the current pass rate falls more than ten percentage points below baseline, we flag this as a regression and log it as a high-severity risk.\n","\n","This catches a common problem with AI systems - gradual quality drift over time as models get updated or prompts get modified. Without regression testing, you might not notice that an asset that used to work ninety percent of the time now only works seventy percent of the time.\n","\n","**No demonstration in this cell**\n","\n","Unlike previous cells, we deliberately skip the demonstration here. Running actual evaluations means making multiple API calls, which takes time and incurs costs. Instead, we simply load the functions and confirm they're ready. The real evaluation happens in Cell 9 where we process all four assets end to end."],"metadata":{"id":"UPGXnne7NtGW"}},{"cell_type":"markdown","source":["###8.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"BIE5Ni_wNRvq"}},{"cell_type":"code","source":["# Cell 8: Evaluation Harness (Synthetic Cases + Regression + Logging)\n","\n","def generate_eval_cases(asset_name):\n","    \"\"\"\n","    Generate 5 synthetic (non-confidential) evaluation cases.\n","\n","    Returns: list of eval cases\n","    \"\"\"\n","    # Synthetic cases tailored to asset type\n","    cases_by_type = {\n","        \"Market Entry Memo Shell\": [\n","            {\"task\": \"Structure memo for entering consumer electronics market\",\n","             \"context\": \"Company: mid-size retailer; Market: Southeast Asia; Timeline: 18 months\"},\n","            {\"task\": \"Create memo outline for healthcare services expansion\",\n","             \"context\": \"Company: regional provider; Market: rural areas; Constraint: limited capital\"},\n","            {\"task\": \"Draft memo framework for B2B software entry\",\n","             \"context\": \"Company: enterprise SaaS; Market: financial services; Focus: compliance\"},\n","            {\"task\": \"Build memo structure for food & beverage launch\",\n","             \"context\": \"Company: CPG brand; Market: urban millennials; Channel: e-commerce\"},\n","            {\"task\": \"Outline memo for industrial equipment market entry\",\n","             \"context\": \"Company: manufacturer; Market: emerging economies; Risk: currency volatility\"}\n","        ],\n","        \"Cost Transformation Workplan\": [\n","            {\"task\": \"Create workplan for SG&A cost reduction\",\n","             \"context\": \"Industry: manufacturing; Target: 15% reduction; Timeline: 12 months\"},\n","            {\"task\": \"Outline transformation plan for IT cost optimization\",\n","             \"context\": \"Industry: financial services; Focus: cloud migration; Constraint: compliance\"},\n","            {\"task\": \"Draft workplan for supply chain cost savings\",\n","             \"context\": \"Industry: retail; Lever: supplier consolidation; Timeline: 9 months\"},\n","            {\"task\": \"Structure plan for overhead reduction\",\n","             \"context\": \"Industry: professional services; Target: real estate and travel; Budget: $2M\"},\n","            {\"task\": \"Build workplan for procurement transformation\",\n","             \"context\": \"Industry: healthcare; Scope: indirect spend; Timeline: 24 months\"}\n","        ],\n","        \"IC Pre-read Shell\": [\n","            {\"task\": \"Create pre-read for acquisition decision\",\n","             \"context\": \"Target: tech startup; Value: $50M; Industry: fintech\"},\n","            {\"task\": \"Draft pre-read for capital expansion\",\n","             \"context\": \"Project: new facility; Investment: $100M; Location: Mexico\"},\n","            {\"task\": \"Outline pre-read for product launch\",\n","             \"context\": \"Product: consumer app; Budget: $20M; Market: Gen Z\"},\n","            {\"task\": \"Structure pre-read for divestiture\",\n","             \"context\": \"Asset: legacy division; Value: $75M; Rationale: strategic focus\"},\n","            {\"task\": \"Build pre-read for partnership decision\",\n","             \"context\": \"Partner: regional distributor; Scope: 5-year contract; Risk: exclusivity\"}\n","        ],\n","        \"RACI + Cadence Shell\": [\n","            {\"task\": \"Create RACI for digital transformation\",\n","             \"context\": \"Scope: enterprise-wide; Duration: 18 months; Stakeholders: 5 departments\"},\n","            {\"task\": \"Draft RACI for merger integration\",\n","             \"context\": \"Companies: similar size; Timeline: 12 months; Focus: synergy capture\"},\n","            {\"task\": \"Outline RACI for product development\",\n","             \"context\": \"Team: cross-functional; Methodology: agile; Complexity: high\"},\n","            {\"task\": \"Structure RACI for cost program\",\n","             \"context\": \"Scope: global; Governance: steering committee; Workstreams: 8\"},\n","            {\"task\": \"Build RACI for compliance initiative\",\n","             \"context\": \"Regulation: new; Timeline: 6 months; Impact: all business units\"}\n","        ]\n","    }\n","\n","    return cases_by_type.get(asset_name, [\n","        {\"task\": f\"Generic task for {asset_name}\", \"context\": \"Placeholder context\"}\n","        for _ in range(5)\n","    ])\n","\n","def run_eval(asset_id, eval_cases):\n","    \"\"\"\n","    Run evaluation harness for an asset.\n","\n","    Returns: summary dict\n","    \"\"\"\n","    asset_dir = base_dir / \"asset_bundles\" / asset_id\n","\n","    results = []\n","    total_cases = len(eval_cases)\n","\n","    print(f\"  Evaluating {total_cases} cases\", end=\"\", flush=True)\n","\n","    for i, case in enumerate(eval_cases):\n","        try:\n","            # Progress indicator\n","            print(f\".\", end=\"\", flush=True)\n","\n","            # Call model with eval case\n","            output = call_claude(\n","                task_description=case[\"task\"],\n","                context=case[\"context\"],\n","                asset_id=asset_id\n","            )\n","\n","            # Validate schema and policy\n","            is_valid, issues = validate_output_json(output, asset_id)\n","\n","            # Compute metrics\n","            metrics = {\n","                \"case_id\": i,\n","                \"pass_schema\": 1 if is_valid else 0,\n","                \"has_open_questions\": 1 if len(output.get(\"open_questions\", [])) > 0 else 0,\n","                \"has_assumptions\": 1 if len(output.get(\"assumptions\", [])) > 0 else 0,\n","                \"no_reco_language\": 1 if is_valid else 0,\n","                \"validation_issues\": issues\n","            }\n","\n","            results.append({\n","                \"case\": case,\n","                \"output\": output,\n","                \"metrics\": metrics\n","            })\n","\n","        except Exception as e:\n","            print(f\"E\", end=\"\", flush=True)\n","            results.append({\n","                \"case\": case,\n","                \"error\": str(e),\n","                \"metrics\": {\n","                    \"case_id\": i,\n","                    \"pass_schema\": 0,\n","                    \"has_open_questions\": 0,\n","                    \"has_assumptions\": 0,\n","                    \"no_reco_language\": 0,\n","                    \"validation_issues\": [str(e)]\n","                }\n","            })\n","\n","    print(\" âœ“\")\n","\n","    # Compute summary\n","    passed = sum(1 for r in results if r.get(\"metrics\", {}).get(\"pass_schema\", 0) == 1)\n","    pass_rate = passed / total_cases if total_cases > 0 else 0\n","\n","    summary = {\n","        \"asset_id\": asset_id,\n","        \"total_cases\": total_cases,\n","        \"passed\": passed,\n","        \"pass_rate\": pass_rate,\n","        \"timestamp\": now_iso()\n","    }\n","\n","    # Save evaluation artifacts\n","    write_json(asset_dir / \"eval_cases.json\", eval_cases)\n","    write_json(asset_dir / \"eval_results.json\", {\n","        \"summary\": summary,\n","        \"results\": results\n","    })\n","\n","    evaluation_plan = {\n","        \"tests\": [\n","            {\"name\": \"schema_compliance\", \"threshold\": 0.8},\n","            {\"name\": \"has_open_questions\", \"threshold\": 0.6},\n","            {\"name\": \"has_assumptions\", \"threshold\": 0.4},\n","            {\"name\": \"no_reco_language\", \"threshold\": 1.0}\n","        ],\n","        \"pass_criteria\": \"All thresholds must be met\",\n","        \"regression_tolerance\": 0.1\n","    }\n","    write_json(asset_dir / \"evaluation_plan.json\", evaluation_plan)\n","\n","    # Log to evaluation_harness_log.jsonl\n","    append_jsonl(base_dir / \"logs\" / \"evaluation_harness_log.jsonl\", {\n","        \"timestamp\": now_iso(),\n","        \"asset_id\": asset_id,\n","        \"cases_hash\": sha256_text(json.dumps(eval_cases)),\n","        \"pass_rate\": pass_rate,\n","        \"passed\": passed,\n","        \"total\": total_cases\n","    })\n","\n","    return summary\n","\n","def regression_check(asset_id, current_pass_rate):\n","    \"\"\"\n","    Check if current evaluation represents a regression from baseline.\n","\n","    Returns: (passed, message)\n","    \"\"\"\n","    baseline_path = base_dir / \"regression_baseline.json\"\n","    baseline = read_json(baseline_path)\n","\n","    # Initialize baseline if empty\n","    if not baseline:\n","        baseline[asset_id] = current_pass_rate\n","        write_json(baseline_path, baseline)\n","        return (True, \"Baseline established\")\n","\n","    # Check for regression\n","    if asset_id not in baseline:\n","        baseline[asset_id] = current_pass_rate\n","        write_json(baseline_path, baseline)\n","        return (True, \"Baseline established for new asset\")\n","\n","    baseline_rate = baseline[asset_id]\n","    tolerance = 0.1  # 10% regression tolerance\n","\n","    if current_pass_rate < baseline_rate - tolerance:\n","        # Log regression risk\n","        risk_log = read_json(base_dir / \"logs\" / \"risk_log.json\")\n","        risk_log[\"risks\"].append({\n","            \"timestamp\": now_iso(),\n","            \"asset_id\": asset_id,\n","            \"type\": \"change_mgmt\",\n","            \"severity\": \"high\",\n","            \"note\": f\"Regression detected: pass rate {current_pass_rate:.2f} vs baseline {baseline_rate:.2f}\"\n","        })\n","        write_json(base_dir / \"logs\" / \"risk_log.json\", risk_log)\n","\n","        return (False, f\"REGRESSION: {current_pass_rate:.2f} vs baseline {baseline_rate:.2f}\")\n","\n","    return (True, f\"No regression: {current_pass_rate:.2f} vs baseline {baseline_rate:.2f}\")\n","\n","# NO DEMO - Functions defined and ready\n","print(\"âœ“ Evaluation harness functions loaded:\")\n","print(\"  - generate_eval_cases()\")\n","print(\"  - run_eval()\")\n","print(\"  - regression_check()\")\n","print(\"\\nâœ“ Ready to evaluate assets in Cell 9\")\n","print(\"  (Note: Cell 9 will make 20 API calls = 4 assets Ã— 5 cases)\")\n","print(\"  (Estimated time: 3-5 minutes)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Da3xmkhojMsY","executionInfo":{"status":"ok","timestamp":1768915230161,"user_tz":360,"elapsed":59,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"a486588d-8bba-45e7-b498-6266152b9934"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ“ Evaluation harness functions loaded:\n","  - generate_eval_cases()\n","  - run_eval()\n","  - regression_check()\n","\n","âœ“ Ready to evaluate assets in Cell 9\n","  (Note: Cell 9 will make 20 API calls = 4 assets Ã— 5 cases)\n","  (Estimated time: 3-5 minutes)\n"]}]},{"cell_type":"markdown","source":["##9.RUNNING END-TO-END MINICASES"],"metadata":{"id":"3juVsof2NUJC"}},{"cell_type":"markdown","source":["###9.1.OVERVIEW"],"metadata":{"id":"0A7X8eAeNVjp"}},{"cell_type":"markdown","source":["**Cell 9: Full Production Run**\n","\n","This is where everything comes together. We're now going to create four complete Asset Bundles from start to finish - building the specifications, generating evaluation cases, running quality tests, checking for regressions, and staging everything for approval. This demonstrates the full Level 4 workflow in action.\n","\n","**The four demonstration assets**\n","\n","We've chosen four realistic consulting scenarios that illustrate different use cases. The market entry memo shell provides structure for analyzing new market opportunities. The cost transformation workplan templates the planning process for efficiency initiatives. The investment committee pre-read creates neutral fact summaries for capital decisions. The RACI and cadence shell documents roles and meeting rhythms for complex programs.\n","\n","Notice each asset has clearly defined boundaries. The market entry memo provides structure only, not market-specific recommendations. The IC pre-read summarizes facts neutrally without making investment recommendations. These boundaries are not arbitrary - they reflect where AI can add value safely versus where human judgment remains essential.\n","\n","**The assembly line process**\n","\n","For each asset, we execute six distinct steps. First, we build the complete asset bundle with all its component files. Second, we generate the synthetic evaluation cases tailored to that asset type. Third, we run the evaluation harness, making actual API calls to test whether the asset performs as expected. Fourth, we check for regression against the quality baseline. Fifth, we create a release candidate document capturing approval requirements and known limitations. Sixth, we save a human-readable summary for stakeholders.\n","\n","The progress tracking shows you exactly where you are in this process. When you see \"Step three of six - running evaluation\", you know API calls are happening and this will take time. The timing information helps you plan - if asset one took ninety seconds, you can estimate the full run will take roughly six minutes.\n","\n","**Configurable evaluation depth**\n","\n","Notice the NUM_EVAL_CASES parameter set to three. This controls how many test cases we run per asset. Three cases gives you quick feedback during development. Five cases provides more comprehensive coverage for final validation. In a production environment, you might run ten or twenty cases to thoroughly exercise edge cases and failure modes.\n","\n","We make this configurable because evaluation has real costs - both API expenses and time. During iterative development, you want fast feedback cycles. Before final release, you accept longer runtimes for greater confidence. The same notebook supports both modes.\n","\n","**The approval staging**\n","\n","Each asset gets a release candidate document that functions like a pre-flight checklist. It lists who must review and approve the asset - subject matter experts, quality assurance leads, governance officers. It documents known limitations based on the evaluation results. It shows the test summary including pass rates and regression status.\n","\n","Critically, every asset remains in pending approval status. The notebook cannot promote assets to production on its own. This human-in-the-loop requirement prevents runaway automation. Someone with appropriate authority must explicitly decide this asset is ready for broad deployment.\n","\n","**The final summary**\n","\n","At completion, you see a table showing all four assets with their pass rates and approval status. This gives leadership a quick overview - are these assets performing well enough to justify the approval process, or do they need refinement first."],"metadata":{"id":"7SgqchOZNupJ"}},{"cell_type":"markdown","source":["###9.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"q65FZs56NXZp"}},{"cell_type":"code","source":["# Cell 9: Run 4 Mini-Case Assets End-to-End (Create + Evaluate + Release Candidate)\n","\n","# Define 4 mini-cases\n","mini_cases = [\n","    {\n","        \"name\": \"Market Entry Memo Shell\",\n","        \"purpose\": \"Reusable template for market entry analysis memos\",\n","        \"scope_boundary\": \"Structure and scaffolding only; no market-specific recommendations\",\n","        \"prohibited_uses\": \"Direct decision-making; client deliverables without review; external sharing\"\n","    },\n","    {\n","        \"name\": \"Cost Transformation Workplan\",\n","        \"purpose\": \"Reusable workplan template for cost reduction initiatives\",\n","        \"scope_boundary\": \"High-level structure and assumptions framework; no cost targets or specific initiatives\",\n","        \"prohibited_uses\": \"Final workplans without validation; unsupported cost estimates; external sharing\"\n","    },\n","    {\n","        \"name\": \"IC Pre-read Shell\",\n","        \"purpose\": \"Reusable pre-read template for investment committee decisions\",\n","        \"scope_boundary\": \"Neutral fact summary only; no investment recommendations or risk assessments\",\n","        \"prohibited_uses\": \"Investment decisions; client-facing materials; regulatory filings\"\n","    },\n","    {\n","        \"name\": \"RACI + Cadence Shell\",\n","        \"purpose\": \"Reusable RACI matrix and meeting cadence template\",\n","        \"scope_boundary\": \"Role definitions and cadence structure; no specific assignments or ownership\",\n","        \"prohibited_uses\": \"Final org charts; performance accountability; external communications\"\n","    }\n","]\n","\n","# Configuration: Set to 3 for faster testing, 5 for full evaluation\n","NUM_EVAL_CASES = 3  # Change to 5 for comprehensive evaluation\n","\n","results_summary = []\n","\n","print(\"=\"*80)\n","print(\"ASSET BUNDLE CREATION AND EVALUATION\")\n","print(\"=\"*80)\n","print(f\"Creating 4 assets with {NUM_EVAL_CASES} evaluation cases each\")\n","print(f\"Estimated time: {NUM_EVAL_CASES * 4 * 0.5:.1f}-{NUM_EVAL_CASES * 4 * 1:.1f} minutes\")\n","print(\"=\"*80 + \"\\n\")\n","\n","import time\n","start_time = time.time()\n","\n","for i, case in enumerate(mini_cases, 1):\n","    case_start = time.time()\n","    print(f\"\\n[{i}/4] {case['name']}\")\n","    print(\"-\" * 60)\n","\n","    # 1. Build asset bundle\n","    print(\"  Step 1/6: Creating asset bundle...\", end=\"\", flush=True)\n","    asset_id = build_asset_bundle(\n","        asset_name=case[\"name\"],\n","        purpose=case[\"purpose\"],\n","        scope_boundary=case[\"scope_boundary\"],\n","        prohibited_uses=case[\"prohibited_uses\"]\n","    )\n","    print(\" âœ“\")\n","\n","    # 2. Generate eval cases (limit to NUM_EVAL_CASES)\n","    print(f\"  Step 2/6: Generating {NUM_EVAL_CASES} evaluation cases...\", end=\"\", flush=True)\n","    all_eval_cases = generate_eval_cases(case[\"name\"])\n","    eval_cases = all_eval_cases[:NUM_EVAL_CASES]\n","    print(\" âœ“\")\n","\n","    # 3. Run evaluation (this is the slow part - API calls)\n","    print(f\"  Step 3/6: Running evaluation ({NUM_EVAL_CASES} API calls)...\", end=\"\", flush=True)\n","    eval_start = time.time()\n","    eval_summary = run_eval(asset_id, eval_cases)\n","    eval_duration = time.time() - eval_start\n","    print(f\" âœ“ ({eval_duration:.1f}s)\")\n","\n","    # 4. Regression check\n","    print(\"  Step 4/6: Checking for regressions...\", end=\"\", flush=True)\n","    regression_ok, regression_msg = regression_check(asset_id, eval_summary[\"pass_rate\"])\n","    print(\" âœ“\")\n","\n","    # 5. Create release_candidate.json\n","    print(\"  Step 5/6: Creating release candidate...\", end=\"\", flush=True)\n","    asset_dir = base_dir / \"asset_bundles\" / asset_id\n","    release_candidate = {\n","        \"asset_id\": asset_id,\n","        \"asset_name\": case[\"name\"],\n","        \"version\": \"0.1-draft\",\n","        \"approval_state\": \"pending\",\n","        \"required_review_roles\": [\n","            \"Subject matter expert\",\n","            \"Quality assurance lead\",\n","            \"Governance officer\"\n","        ],\n","        \"known_limitations\": [\n","            f\"Evaluated with only {NUM_EVAL_CASES} synthetic cases (not comprehensive)\",\n","            \"No external validation or peer review\",\n","            \"Template may require customization for specific use cases\",\n","            \"Not verified against authoritative sources\"\n","        ],\n","        \"test_summary\": {\n","            \"total_cases\": eval_summary[\"total_cases\"],\n","            \"passed\": eval_summary[\"passed\"],\n","            \"pass_rate\": eval_summary[\"pass_rate\"],\n","            \"regression_ok\": regression_ok\n","        },\n","        \"version_bump_suggestion\": \"0.1-draft â†’ 0.1-rc1 (release candidate) after approval\",\n","        \"created_at\": now_iso()\n","    }\n","    write_json(asset_dir / \"release_candidate.json\", release_candidate)\n","    print(\" âœ“\")\n","\n","    # 6. Save human-readable summary\n","    print(\"  Step 6/6: Saving deliverable summary...\", end=\"\", flush=True)\n","    summary_text = f\"\"\"# {case['name']} - Asset Summary\n","\n","**Asset ID**: {asset_id}\n","**Version**: 0.1-draft\n","**Status**: Pending approval\n","\n","## Purpose\n","{case['purpose']}\n","\n","## Evaluation Results\n","- Total test cases: {eval_summary['total_cases']}\n","- Passed: {eval_summary['passed']}\n","- Pass rate: {eval_summary['pass_rate']:.2%}\n","- Regression check: {regression_msg}\n","\n","## Approval Status\n","- State: Pending\n","- Required reviews: Subject matter expert, QA lead, Governance officer\n","\n","## Known Limitations\n","- Evaluated with {NUM_EVAL_CASES} synthetic cases (not comprehensive)\n","- No external validation\n","- Requires customization for production use\n","- Not verified against authoritative sources\n","\n","## Next Steps\n","1. Human review of asset bundle components\n","2. Subject matter expert validation\n","3. Quality assurance sign-off\n","4. Governance approval for release\n","5. Version bump to 0.1-rc1 if approved\n","\n","---\n","Generated: {now_iso()}\n","\"\"\"\n","    with open(base_dir / \"deliverables\" / f\"{case['name'].replace(' ', '_').lower()}_summary.txt\", 'w') as f:\n","        f.write(summary_text)\n","    print(\" âœ“\")\n","\n","    # Update approvals_log\n","    approvals_log = read_json(base_dir / \"logs\" / \"approvals_log.json\")\n","    approvals_log[\"approvals\"].append({\n","        \"timestamp\": now_iso(),\n","        \"asset_id\": asset_id,\n","        \"asset_name\": case[\"name\"],\n","        \"approval_type\": \"release_candidate\",\n","        \"status\": \"pending\",\n","        \"required_approvers\": release_candidate[\"required_review_roles\"]\n","    })\n","    write_json(base_dir / \"logs\" / \"approvals_log.json\", approvals_log)\n","\n","    # Store for summary table\n","    results_summary.append({\n","        \"asset_name\": case[\"name\"],\n","        \"asset_id\": asset_id,\n","        \"pass_rate\": eval_summary[\"pass_rate\"],\n","        \"regression_ok\": regression_ok,\n","        \"approval_state\": \"pending\"\n","    })\n","\n","    case_duration = time.time() - case_start\n","    print(f\"\\n  Asset complete in {case_duration:.1f}s\")\n","\n","total_duration = time.time() - start_time\n","\n","# Print summary table\n","print(\"\\n\" + \"=\"*80)\n","print(\"ASSET CREATION SUMMARY\")\n","print(\"=\"*80)\n","print(f\"{'Asset Name':<35} {'Pass Rate':<12} {'Regression':<12} {'Approval'}\")\n","print(\"-\"*80)\n","for r in results_summary:\n","    regression_symbol = \"âœ“\" if r[\"regression_ok\"] else \"âœ—\"\n","    print(f\"{r['asset_name']:<35} {r['pass_rate']:>10.1%}  {regression_symbol:>10}  {r['approval_state']}\")\n","print(\"=\"*80)\n","\n","print(f\"\\nâœ“ All 4 Asset Bundles created and evaluated in {total_duration:.1f}s ({total_duration/60:.1f} min)\")\n","print(f\"âœ“ Total API calls made: {NUM_EVAL_CASES * 4}\")\n","print(f\"\\nðŸ“ Outputs:\")\n","print(f\"  - Asset bundles: {base_dir / 'asset_bundles'}\")\n","print(f\"  - Deliverables: {base_dir / 'deliverables'}\")\n","print(f\"  - Logs: {base_dir / 'logs'}\")\n","print(f\"\\nâš ï¸  Note: Each asset evaluated with {NUM_EVAL_CASES} cases\")\n","print(f\"   Change NUM_EVAL_CASES to 5 for comprehensive evaluation\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8R6HD06kJxP","executionInfo":{"status":"ok","timestamp":1768915488988,"user_tz":360,"elapsed":238880,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"57326e82-d7e8-4ff2-8e67-881b65ce3984"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","ASSET BUNDLE CREATION AND EVALUATION\n","================================================================================\n","Creating 4 assets with 3 evaluation cases each\n","Estimated time: 6.0-12.0 minutes\n","================================================================================\n","\n","\n","[1/4] Market Entry Memo Shell\n","------------------------------------------------------------\n","  Step 1/6: Creating asset bundle...âœ“ Asset Bundle created: asset_market_entry_memo_shell_f9b3c41f\n","  Location: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/asset_bundles/asset_market_entry_memo_shell_f9b3c41f\n","  Files created:\n","    - prompt_pack.txt\n","    - template.txt\n","    - asset_spec.json\n"," âœ“\n","  Step 2/6: Generating 3 evaluation cases... âœ“\n","  Step 3/6: Running evaluation (3 API calls)...  Evaluating 3 cases... âœ“\n"," âœ“ (50.2s)\n","  Step 4/6: Checking for regressions... âœ“\n","  Step 5/6: Creating release candidate... âœ“\n","  Step 6/6: Saving deliverable summary... âœ“\n","\n","  Asset complete in 50.3s\n","\n","[2/4] Cost Transformation Workplan\n","------------------------------------------------------------\n","  Step 1/6: Creating asset bundle...âœ“ Asset Bundle created: asset_cost_transformation_workplan_43158c5c\n","  Location: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/asset_bundles/asset_cost_transformation_workplan_43158c5c\n","  Files created:\n","    - prompt_pack.txt\n","    - template.txt\n","    - asset_spec.json\n"," âœ“\n","  Step 2/6: Generating 3 evaluation cases... âœ“\n","  Step 3/6: Running evaluation (3 API calls)...  Evaluating 3 cases... âœ“\n"," âœ“ (66.9s)\n","  Step 4/6: Checking for regressions... âœ“\n","  Step 5/6: Creating release candidate... âœ“\n","  Step 6/6: Saving deliverable summary... âœ“\n","\n","  Asset complete in 66.9s\n","\n","[3/4] IC Pre-read Shell\n","------------------------------------------------------------\n","  Step 1/6: Creating asset bundle...âœ“ Asset Bundle created: asset_ic_pre-read_shell_234bd4f7\n","  Location: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/asset_bundles/asset_ic_pre-read_shell_234bd4f7\n","  Files created:\n","    - prompt_pack.txt\n","    - template.txt\n","    - asset_spec.json\n"," âœ“\n","  Step 2/6: Generating 3 evaluation cases... âœ“\n","  Step 3/6: Running evaluation (3 API calls)...  Evaluating 3 cases.E.. âœ“\n"," âœ“ (61.2s)\n","  Step 4/6: Checking for regressions... âœ“\n","  Step 5/6: Creating release candidate... âœ“\n","  Step 6/6: Saving deliverable summary... âœ“\n","\n","  Asset complete in 61.2s\n","\n","[4/4] RACI + Cadence Shell\n","------------------------------------------------------------\n","  Step 1/6: Creating asset bundle...âœ“ Asset Bundle created: asset_raci_+_cadence_shell_3d702fde\n","  Location: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/asset_bundles/asset_raci_+_cadence_shell_3d702fde\n","  Files created:\n","    - prompt_pack.txt\n","    - template.txt\n","    - asset_spec.json\n"," âœ“\n","  Step 2/6: Generating 3 evaluation cases... âœ“\n","  Step 3/6: Running evaluation (3 API calls)...  Evaluating 3 cases... âœ“\n"," âœ“ (60.4s)\n","  Step 4/6: Checking for regressions... âœ“\n","  Step 5/6: Creating release candidate... âœ“\n","  Step 6/6: Saving deliverable summary... âœ“\n","\n","  Asset complete in 60.4s\n","\n","================================================================================\n","ASSET CREATION SUMMARY\n","================================================================================\n","Asset Name                          Pass Rate    Regression   Approval\n","--------------------------------------------------------------------------------\n","Market Entry Memo Shell                 100.0%           âœ“  pending\n","Cost Transformation Workplan            100.0%           âœ“  pending\n","IC Pre-read Shell                        66.7%           âœ“  pending\n","RACI + Cadence Shell                    100.0%           âœ“  pending\n","================================================================================\n","\n","âœ“ All 4 Asset Bundles created and evaluated in 238.8s (4.0 min)\n","âœ“ Total API calls made: 12\n","\n","ðŸ“ Outputs:\n","  - Asset bundles: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/asset_bundles\n","  - Deliverables: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/deliverables\n","  - Logs: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b/logs\n","\n","âš ï¸  Note: Each asset evaluated with 3 cases\n","   Change NUM_EVAL_CASES to 5 for comprehensive evaluation\n"]}]},{"cell_type":"markdown","source":["##10.AUDIT BUNDLE"],"metadata":{"id":"deALRI7DNc11"}},{"cell_type":"markdown","source":["###10.1.OVERVIEW"],"metadata":{"id":"hspK-tF8Nd9D"}},{"cell_type":"markdown","source":["**Cell 10: Creating the Audit Package**\n","\n","The final cell packages everything into a complete, auditable deliverable that you can archive, share with stakeholders, or present to governance committees. This isn't just zipping up files - we're creating comprehensive documentation that explains what was created, how to use it, and what happens next.\n","\n","**The audit readme document**\n","\n","We generate an extensive readme file that serves as the user manual for this entire package. It starts with run information - the unique identifier, timestamp, model configuration, and a fingerprint hash that proves these settings. If someone later questions whether results were produced with approved configurations, this provides definitive proof.\n","\n","The readme then walks through every artifact in the package, explaining what each file contains and why it matters. For someone unfamiliar with Level 4 processes, this documentation is essential. They can understand that prompts_log contains hashed records of API calls, that regression_baseline stores quality metrics, that release_candidate documents approval requirements.\n","\n","**The promotion workflow**\n","\n","Most importantly, the readme provides step-by-step instructions for promoting assets from draft status to production release. It defines five phases - subject matter expert review, quality assurance testing, governance approval, version management, and deployment. Each phase has specific deliverables and decision criteria.\n","\n","This is crucial because creating the asset is only the beginning. The real work happens in validation and approval. Without clear guidance, assets languish in draft status because nobody knows what's required to move them forward. The readme removes that ambiguity.\n","\n","**Prominent limitations**\n","\n","We're very explicit about what this run did not do. It used only synthetic test cases, not real scenarios. It evaluated just three to five cases per asset, not comprehensive coverage. No human experts reviewed the outputs. No external validation occurred. These limitations don't make the assets worthless, but they must inform how you use them.\n","\n","Being honest about limitations builds trust. If you claim assets are production-ready when they've only been tested synthetically, you're setting up stakeholders for disappointment. If you clearly state current limitations and what additional validation is needed, you enable informed decisions.\n","\n","**Package inventory and validation**\n","\n","We create a formal inventory listing total files, directories, assets created, and API calls made. This provides accountability - you can verify the package is complete and hasn't been tampered with. The validation checklist confirms all required governance artifacts exist.\n","\n","The directory tree visualization shows the package structure at a glance. Even non-technical stakeholders can see there are asset bundles, deliverables, and comprehensive logs. The file count and archive size help with storage planning and compliance documentation.\n","\n","**Actionable next steps**\n","\n","Rather than ending with a generic success message, we provide concrete guidance. Download the zip archive. Review the audit readme. Inspect individual asset bundles. Check the deliverables folder for summaries. If you want to promote assets to production, here's what you need to do differently - increase evaluation cases, run subject matter expert validation, obtain formal approvals.\n","\n","This transforms the notebook from a technical exercise into a business process with clear handoffs and accountability."],"metadata":{"id":"QktoO36gNwPN"}},{"cell_type":"markdown","source":["###10.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"-qPlPvQYNhIv"}},{"cell_type":"code","source":["# Cell 10: Bundle + AUDIT_README + Zip\n","\n","import shutil\n","\n","print(\"=\"*80)\n","print(\"FINAL PACKAGING AND AUDIT TRAIL\")\n","print(\"=\"*80 + \"\\n\")\n","\n","# Step 1: Create AUDIT_README.txt\n","print(\"Step 1/4: Creating AUDIT_README.txt...\", end=\"\", flush=True)\n","\n","audit_readme = f\"\"\"# AI-Assisted Consulting - Level 4 (Innovators) - Audit Package\n","\n","## Run Information\n","- Run ID: {run_name}\n","- Created: {now_iso()}\n","- Model: {MODEL}\n","- Temperature: {TEMPERATURE}\n","- Max Tokens: {MAX_TOKENS}\n","- Config Hash: {stable_config_hash()}\n","\n","## Purpose\n","This package contains a complete audit trail for a Level 4 (Innovators) AI-assisted\n","consulting session focused on creating reusable internal assets with evaluation\n","and controlled release processes.\n","\n","**Key principle**: Level 4 is NOT about \"smarter AI advice\" â€” it's about creating\n","governance-first, reusable assets that can be deployed at scale with appropriate\n","quality controls.\n","\n","## What's in This Package\n","\n","### Core Governance Artifacts\n","1. **run_manifest.json** - Session metadata, model config, environment fingerprint\n","2. **logs/prompts_log.jsonl** - Append-only log of all API calls (hashed prompts only)\n","3. **logs/risk_log.json** - All auto-detected and flagged risks\n","4. **logs/verification_register.json** - Items requiring human verification\n","5. **logs/change_log.json** - All asset creation and modification events\n","6. **logs/approvals_log.json** - Approval workflows and status\n","7. **logs/exception_log.json** - Parsing failures and policy violations\n","\n","### Level 4 Specific Artifacts\n","8. **asset_registry.json** - Registry of all created assets in this run\n","9. **asset_bundles/<asset_id>/** - Complete Asset Bundles (see structure below)\n","10. **logs/evaluation_harness_log.jsonl** - Evaluation run records\n","11. **regression_baseline.json** - Quality baseline for regression testing\n","\n","### Asset Bundle Structure\n","Each asset bundle folder contains:\n","- **asset_spec.json** - Metadata, scope, prohibited uses, version\n","- **prompt_pack.txt** - Reusable prompt template with guardrails\n","- **template.txt** - Deliverable shell with labeled sections\n","- **evaluation_plan.json** - Test definitions and thresholds\n","- **eval_cases.json** - {NUM_EVAL_CASES} synthetic test cases (non-confidential)\n","- **eval_results.json** - Pass/fail outcomes per test\n","- **release_candidate.json** - Approval checklist and known limitations\n","\n","### Deliverables\n","12. **deliverables/** - Human-readable summaries for each asset\n","\n","## Assets Created in This Run\n","1. **Market Entry Memo Shell** - Reusable market entry analysis template\n","2. **Cost Transformation Workplan** - Reusable cost reduction workplan template\n","3. **IC Pre-read Shell** - Reusable investment committee pre-read template\n","4. **RACI + Cadence Shell** - Reusable RACI matrix and meeting cadence template\n","\n","## Evaluation Summary\n","- Evaluation cases per asset: {NUM_EVAL_CASES}\n","- Total API calls: {NUM_EVAL_CASES * 4}\n","- All assets: DRAFT status (version 0.1-draft)\n","\n","## Reproducibility\n","To reproduce this run:\n","1. Use the same model and parameters from run_manifest.json\n","2. Use eval_cases.json for each asset (synthetic inputs)\n","3. Compare results against regression_baseline.json\n","4. All prompts are logged as SHA-256 hashes for verification\n","\n","## How to Promote Assets from Draft to Released\n","\n","### Approval Workflow (Required for Each Asset)\n","\n","**Phase 1: Subject Matter Expert Review**\n","- Validate asset purpose and scope alignment\n","- Confirm prohibited uses are appropriate\n","- Test with non-synthetic (real but sanitized) cases\n","- Document additional limitations found\n","\n","**Phase 2: Quality Assurance Review**\n","- Run evaluation harness with expanded test cases ({NUM_EVAL_CASES} â†’ 10+ cases)\n","- Verify pass rates meet all thresholds in evaluation_plan.json\n","- Check for regressions vs. regression_baseline.json\n","- Validate edge cases and error handling\n","\n","**Phase 3: Governance Review**\n","- Confirm confidentiality controls are adequate\n","- Validate \"Not verified\" disclaimers are prominent\n","- Approve change management plan\n","- Review risk log for any high-severity issues\n","- Verify traceability (all inputs/outputs logged)\n","\n","**Phase 4: Version Management**\n","- 0.1-draft â†’ 0.1-rc1 (release candidate) after all reviews\n","- 0.1-rc1 â†’ 0.1.0 (released) after production pilot\n","- Update asset_registry.json with new version\n","- Log in change_log.json with reviewer sign-offs\n","\n","**Phase 5: Deployment**\n","- Add to internal asset library/repository\n","- Create user documentation and training materials\n","- Define usage metrics and feedback mechanisms\n","- Establish monitoring for quality drift\n","- Plan periodic re-evaluation (quarterly recommended)\n","\n","### Critical Reminders\n","\n","âš ï¸ **Not Verified**: All outputs are drafts requiring human review\n","âš ï¸ **No Recommendations**: Assets must not generate rankings or \"best option\" language\n","âš ï¸ **Confidentiality**: Never use with client-confidential data without proper controls\n","âš ï¸ **Traceability**: All model calls are logged; all risks are flagged\n","âš ï¸ **Change Management**: Any asset modifications require re-evaluation\n","âš ï¸ **Regression Testing**: Compare new results vs. baseline before each release\n","âš ï¸ **Evaluation Scope**: Current evaluation used only {NUM_EVAL_CASES} synthetic cases\n","\n","### Known Limitations of This Run\n","- Synthetic evaluation cases only (not tested with real client scenarios)\n","- Limited test coverage ({NUM_EVAL_CASES} cases per asset)\n","- No external validation or peer review\n","- No human subject matter expert review yet\n","- Templates may require significant customization\n","- Not validated against authoritative sources or industry standards\n","\n","### Next Steps\n","1. Expand evaluation to 10+ cases per asset (including edge cases)\n","2. Conduct subject matter expert review for each asset\n","3. Test with sanitized real-world inputs (non-confidential)\n","4. Document additional limitations discovered\n","5. Obtain formal approvals before promoting to RC status\n","6. Establish monitoring and feedback loops post-deployment\n","\n","## Questions or Issues?\n","Review exception_log.json for any parsing failures or policy violations encountered\n","during the run.\n","\n","## File Integrity\n","- Run manifest hash: {stable_config_hash()}\n","- Total assets created: 4\n","- Total files in package: {sum(1 for _ in base_dir.rglob('*') if _.is_file())}\n","- Package created: {now_iso()}\n","\n","---\n","Generated by: AI-Assisted Consulting Level 4 (Innovators)\n","Model: {MODEL}\n","Chapter: 4 - Level 4 (Innovators)\n","\"\"\"\n","\n","audit_readme_path = base_dir / \"AUDIT_README.txt\"\n","with open(audit_readme_path, 'w') as f:\n","    f.write(audit_readme)\n","\n","print(\" âœ“\")\n","\n","# Step 2: Create package inventory\n","print(\"Step 2/4: Creating package inventory...\", end=\"\", flush=True)\n","\n","inventory = {\n","    \"run_id\": run_name,\n","    \"created_at\": now_iso(),\n","    \"total_files\": sum(1 for _ in base_dir.rglob('*') if _.is_file()),\n","    \"total_directories\": sum(1 for _ in base_dir.rglob('*') if _.is_dir()),\n","    \"assets_created\": len(results_summary),\n","    \"evaluation_cases_per_asset\": NUM_EVAL_CASES,\n","    \"total_api_calls\": NUM_EVAL_CASES * 4,\n","    \"config_hash\": stable_config_hash()\n","}\n","write_json(base_dir / \"package_inventory.json\", inventory)\n","print(\" âœ“\")\n","\n","# Step 3: Create zip bundle\n","print(\"Step 3/4: Creating zip archive...\", end=\"\", flush=True)\n","zip_name = f\"{run_name}_complete\"\n","zip_path = Path(f\"/content/{zip_name}\")\n","shutil.make_archive(str(zip_path), 'zip', base_dir)\n","zip_size_mb = zip_path.with_suffix('.zip').stat().st_size / (1024 * 1024)\n","print(f\" âœ“ ({zip_size_mb:.2f} MB)\")\n","\n","# Step 4: Generate final checklist\n","print(\"Step 4/4: Validating package contents...\", end=\"\", flush=True)\n","\n","checklist_items = [\n","    (\"run_manifest.json\", (base_dir / \"run_manifest.json\").exists()),\n","    (\"asset_registry.json\", (base_dir / \"asset_registry.json\").exists()),\n","    (\"AUDIT_README.txt\", audit_readme_path.exists()),\n","    (\"package_inventory.json\", (base_dir / \"package_inventory.json\").exists()),\n","    (f\"{len(results_summary)} Asset Bundles\", len(list((base_dir / \"asset_bundles\").iterdir())) >= len(results_summary)),\n","    (\"Evaluation harness log\", (base_dir / \"logs\" / \"evaluation_harness_log.jsonl\").exists()),\n","    (\"Regression baseline\", (base_dir / \"regression_baseline.json\").exists()),\n","    (\"Change log\", (base_dir / \"logs\" / \"change_log.json\").exists()),\n","    (\"Approvals log\", (base_dir / \"logs\" / \"approvals_log.json\").exists()),\n","    (\"Risk log\", (base_dir / \"logs\" / \"risk_log.json\").exists()),\n","    (\"Zip bundle\", zip_path.with_suffix('.zip').exists())\n","]\n","\n","all_passed = all(status for _, status in checklist_items)\n","print(\" âœ“\" if all_passed else \" âš \")\n","\n","# Print final summary\n","print(\"\\n\" + \"=\"*80)\n","print(\"PACKAGE CONTENTS\")\n","print(\"=\"*80 + \"\\n\")\n","\n","# Show directory tree (simplified)\n","def count_files_in_dir(path):\n","    return sum(1 for _ in path.rglob('*') if _.is_file())\n","\n","print(f\"ðŸ“¦ {base_dir.name}/\")\n","print(f\"   â”œâ”€â”€ AUDIT_README.txt\")\n","print(f\"   â”œâ”€â”€ run_manifest.json\")\n","print(f\"   â”œâ”€â”€ package_inventory.json\")\n","print(f\"   â”œâ”€â”€ asset_registry.json ({len(results_summary)} assets)\")\n","print(f\"   â”œâ”€â”€ regression_baseline.json\")\n","print(f\"   â”œâ”€â”€ asset_bundles/ ({len(results_summary)} bundles)\")\n","for i, result in enumerate(results_summary, 1):\n","    asset_id_short = result['asset_id'][:30] + \"...\"\n","    print(f\"   â”‚   â”œâ”€â”€ {asset_id_short}\")\n","print(f\"   â”œâ”€â”€ deliverables/ ({len(results_summary)} summaries)\")\n","print(f\"   â””â”€â”€ logs/\")\n","print(f\"       â”œâ”€â”€ prompts_log.jsonl\")\n","print(f\"       â”œâ”€â”€ evaluation_harness_log.jsonl\")\n","print(f\"       â”œâ”€â”€ risk_log.json\")\n","print(f\"       â”œâ”€â”€ change_log.json\")\n","print(f\"       â”œâ”€â”€ approvals_log.json\")\n","print(f\"       â”œâ”€â”€ verification_register.json\")\n","print(f\"       â””â”€â”€ exception_log.json\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"VALIDATION CHECKLIST\")\n","print(\"=\"*80 + \"\\n\")\n","\n","for item, status in checklist_items:\n","    symbol = \"âœ“\" if status else \"âœ—\"\n","    print(f\"  {symbol} {item}\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"PACKAGE SUMMARY\")\n","print(\"=\"*80 + \"\\n\")\n","\n","print(f\"ðŸ“Š Statistics:\")\n","print(f\"   â€¢ Total files: {inventory['total_files']}\")\n","print(f\"   â€¢ Total directories: {inventory['total_directories']}\")\n","print(f\"   â€¢ Assets created: {inventory['assets_created']}\")\n","print(f\"   â€¢ API calls made: {inventory['total_api_calls']}\")\n","print(f\"   â€¢ Evaluation cases per asset: {NUM_EVAL_CASES}\")\n","print(f\"   â€¢ Archive size: {zip_size_mb:.2f} MB\")\n","\n","print(f\"\\nðŸ“ Locations:\")\n","print(f\"   â€¢ Run directory: {base_dir}\")\n","print(f\"   â€¢ Zip archive: {zip_path}.zip\")\n","\n","print(f\"\\nâš ï¸  Critical Reminders:\")\n","print(f\"   â€¢ All assets are DRAFTS (version 0.1-draft)\")\n","print(f\"   â€¢ Human review and approval required before production use\")\n","print(f\"   â€¢ Each asset evaluated with only {NUM_EVAL_CASES} synthetic cases\")\n","print(f\"   â€¢ Expand to 10+ cases and real scenarios before release\")\n","print(f\"   â€¢ See AUDIT_README.txt for complete promotion workflow\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"âœ“ NOTEBOOK COMPLETE\")\n","print(\"=\"*80)\n","print(f\"\\nðŸŽ¯ Next Actions:\")\n","print(f\"   1. Download: {zip_path}.zip\")\n","print(f\"   2. Review: AUDIT_README.txt for promotion process\")\n","print(f\"   3. Inspect: asset_bundles/ for each asset's components\")\n","print(f\"   4. Check: deliverables/ for human-readable summaries\")\n","print(f\"   5. Validate: logs/ for complete audit trail\")\n","print(f\"\\nðŸ’¡ To promote assets to production:\")\n","print(f\"   â€¢ Increase NUM_EVAL_CASES to 10+ in Cell 9\")\n","print(f\"   â€¢ Run subject matter expert validation\")\n","print(f\"   â€¢ Obtain formal approvals per AUDIT_README\")\n","print(f\"   â€¢ Update version to 0.1-rc1, then 0.1.0\")"],"metadata":{"id":"STPTAD0YNi9b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768915775253,"user_tz":360,"elapsed":117,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"5643fe1b-8be0-4a21-a2af-63955b19f882"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","FINAL PACKAGING AND AUDIT TRAIL\n","================================================================================\n","\n","Step 1/4: Creating AUDIT_README.txt... âœ“\n","Step 2/4: Creating package inventory... âœ“\n","Step 3/4: Creating zip archive... âœ“ (0.06 MB)\n","Step 4/4: Validating package contents... âœ“\n","\n","================================================================================\n","PACKAGE CONTENTS\n","================================================================================\n","\n","ðŸ“¦ run_20260120_132001_54ba806b/\n","   â”œâ”€â”€ AUDIT_README.txt\n","   â”œâ”€â”€ run_manifest.json\n","   â”œâ”€â”€ package_inventory.json\n","   â”œâ”€â”€ asset_registry.json (4 assets)\n","   â”œâ”€â”€ regression_baseline.json\n","   â”œâ”€â”€ asset_bundles/ (4 bundles)\n","   â”‚   â”œâ”€â”€ asset_market_entry_memo_shell_...\n","   â”‚   â”œâ”€â”€ asset_cost_transformation_work...\n","   â”‚   â”œâ”€â”€ asset_ic_pre-read_shell_234bd4...\n","   â”‚   â”œâ”€â”€ asset_raci_+_cadence_shell_3d7...\n","   â”œâ”€â”€ deliverables/ (4 summaries)\n","   â””â”€â”€ logs/\n","       â”œâ”€â”€ prompts_log.jsonl\n","       â”œâ”€â”€ evaluation_harness_log.jsonl\n","       â”œâ”€â”€ risk_log.json\n","       â”œâ”€â”€ change_log.json\n","       â”œâ”€â”€ approvals_log.json\n","       â”œâ”€â”€ verification_register.json\n","       â””â”€â”€ exception_log.json\n","\n","================================================================================\n","VALIDATION CHECKLIST\n","================================================================================\n","\n","  âœ“ run_manifest.json\n","  âœ“ asset_registry.json\n","  âœ“ AUDIT_README.txt\n","  âœ“ package_inventory.json\n","  âœ“ 4 Asset Bundles\n","  âœ“ Evaluation harness log\n","  âœ“ Regression baseline\n","  âœ“ Change log\n","  âœ“ Approvals log\n","  âœ“ Risk log\n","  âœ“ Zip bundle\n","\n","================================================================================\n","PACKAGE SUMMARY\n","================================================================================\n","\n","ðŸ“Š Statistics:\n","   â€¢ Total files: 47\n","   â€¢ Total directories: 9\n","   â€¢ Assets created: 4\n","   â€¢ API calls made: 12\n","   â€¢ Evaluation cases per asset: 3\n","   â€¢ Archive size: 0.06 MB\n","\n","ðŸ“ Locations:\n","   â€¢ Run directory: /content/ai_consulting_ch4_runs/run_20260120_132001_54ba806b\n","   â€¢ Zip archive: /content/run_20260120_132001_54ba806b_complete.zip\n","\n","âš ï¸  Critical Reminders:\n","   â€¢ All assets are DRAFTS (version 0.1-draft)\n","   â€¢ Human review and approval required before production use\n","   â€¢ Each asset evaluated with only 3 synthetic cases\n","   â€¢ Expand to 10+ cases and real scenarios before release\n","   â€¢ See AUDIT_README.txt for complete promotion workflow\n","\n","================================================================================\n","âœ“ NOTEBOOK COMPLETE\n","================================================================================\n","\n","ðŸŽ¯ Next Actions:\n","   1. Download: /content/run_20260120_132001_54ba806b_complete.zip\n","   2. Review: AUDIT_README.txt for promotion process\n","   3. Inspect: asset_bundles/ for each asset's components\n","   4. Check: deliverables/ for human-readable summaries\n","   5. Validate: logs/ for complete audit trail\n","\n","ðŸ’¡ To promote assets to production:\n","   â€¢ Increase NUM_EVAL_CASES to 10+ in Cell 9\n","   â€¢ Run subject matter expert validation\n","   â€¢ Obtain formal approvals per AUDIT_README\n","   â€¢ Update version to 0.1-rc1, then 0.1.0\n"]}]},{"cell_type":"markdown","source":["##11.CONCLUSIONS"],"metadata":{"id":"7X2_AKt3NkAR"}},{"cell_type":"markdown","source":["**Conclusion: From Assets to Organizational Capability**\n","\n","You've now completed the full Level 4 journey. You've built four Asset Bundles from scratch, implemented comprehensive evaluation systems, created regression baselines, generated audit trails, and packaged everything into professional deliverables. More importantly, you've internalized a fundamentally different way of thinking about AI in professional services.\n","\n","**What You've Actually Built**\n","\n","Let's be precise about what exists in your deliverable package. You have four asset bundles, each containing seven components - a formal specification, a reusable prompt pack, a structured template, an evaluation plan, synthetic test cases, test results, and a release candidate checklist. You have governance logs tracking every API call, every risk identified, every change made, and every approval required. You have regression baselines establishing quality standards. You have comprehensive documentation explaining how everything works and what happens next.\n","\n","What you do not have is production-ready assets. I want to emphasize this clearly because the temptation to skip ahead is strong. These assets have been tested against only three to five synthetic cases each. No subject matter expert has validated them. No real user has attempted to apply them to actual work. No edge cases have been explored. The evaluation coverage is minimal by any professional standard.\n","\n","This is intentional. The notebook teaches you the methodology for creating production assets, not the shortcut to avoiding proper validation. If you deployed these assets broadly tomorrow, you would likely encounter failures, edge cases, and limitations that synthetic testing didn't reveal. Those failures would damage trust in AI-assisted work more than never creating the assets at all.\n","\n","**The Real Value Proposition**\n","\n","The true value of Level 4 isn't the specific assets you created today. It's the repeatable process you now understand. You can see what comprehensive asset development looks like - the testing required, the documentation needed, the approval workflows essential, the governance infrastructure demanded. You have working code that implements these patterns, which you can adapt to your specific context.\n","\n","Consider what happens next in your organization. Perhaps you identify a high-value use case - let's say standardizing how your firm analyzes competitive positioning for healthcare clients. You've done this analysis dozens of times. The pattern is consistent enough to systematize, but complex enough to benefit from AI assistance.\n","\n","You can now approach this systematically. You'll define the asset's scope and boundaries - what it does, what it explicitly avoids, who can use it, what inputs are required. You'll create the prompt pack encoding your firm's methodology. You'll generate comprehensive test cases covering common scenarios and known edge cases. You'll run evaluation harnesses measuring quality across multiple dimensions. You'll document limitations honestly. You'll establish approval workflows involving the right stakeholders.\n","\n","This is dramatically different from someone casually creating a ChatGPT prompt, getting a good result once, and encouraging everyone to use it. That approach fails at scale because it lacks testing, lacks governance, and lacks accountability. Your approach succeeds because it applies professional development standards to AI asset creation.\n","\n","**The Organizational Conversation**\n","\n","Level 4 also equips you for crucial conversations with leadership about AI adoption. Many executives oscillate between two extremes - either dismissing AI as unreliable hype or expecting it to magically solve complex problems. Both positions miss the nuance.\n","\n","You can now explain the middle ground. AI can create significant value through reusable assets, but only with appropriate investment in development and governance. You can show concrete examples of what proper asset development looks like. You can demonstrate evaluation methodologies that measure quality objectively. You can present audit trails proving compliance and accountability. You can articulate realistic timelines and resource requirements.\n","\n","When someone asks how long it takes to create a production asset, you have an informed answer. Initial development might take days - defining scope, creating prompts, building evaluation cases, running initial tests. Validation and refinement might take weeks - subject matter expert review, expanded testing, real-world piloting, iteration based on feedback. Deployment infrastructure might take months - training users, establishing monitoring, creating support processes, building feedback loops.\n","\n","These timelines seem long compared to \"I got a good ChatGPT response in five minutes.\" But they're short compared to traditional methodology development, which often takes years and relies entirely on expert judgment rather than systematic testing. Level 4 represents the realistic middle ground between casual experimentation and traditional approaches.\n","\n","**Common Pitfalls to Avoid**\n","\n","Having taught this material to hundreds of professionals, I can predict where you're likely to encounter challenges. The first pitfall is evaluation shortcuts. You'll be tempted to reduce test cases, accept lower pass rates, or skip regression testing. Resist this. Quality standards exist for a reason. If an asset only passes seventy percent of cases, it needs refinement before deployment, not deployment with fingers crossed.\n","\n","The second pitfall is scope creep. You'll want assets to do more - generate recommendations, make decisions, provide authoritative answers. But broader scope means higher risk and harder governance. Keep assets focused on specific, testable capabilities. Let humans handle judgment-intensive work. This constraint might feel limiting, but it's what makes assets deployable safely.\n","\n","The third pitfall is bypassing approval workflows. When you've invested significant effort creating an asset, you want to see it used immediately. But releasing unapproved assets undermines the entire governance framework. The approval process isn't bureaucratic obstruction - it's validation that the asset actually works, serves a real need, and can be supported properly.\n","\n","The fourth pitfall is treating this as a one-time exercise. Asset development requires ongoing maintenance. Models get updated. Use cases evolve. Edge cases emerge. Quality drifts. You need processes for monitoring asset performance, collecting user feedback, running periodic re-evaluation, and managing updates. Assets without maintenance become liabilities.\n","\n","**The Path to Level 5**\n","\n","Some of you will stop at Level 4, and that's entirely appropriate. You'll build small libraries of assets for your immediate teams, maintain them manually, and generate significant value without massive infrastructure investment. For mid-size teams working on consistent problem types, this is optimal.\n","\n","Others will recognize the need for Level 5 capabilities - automated deployment systems, continuous monitoring, feedback loops, A/B testing, gradual rollouts, automated regression detection, and sophisticated version control. These capabilities enable enterprise-scale deployment but require corresponding investment in technical infrastructure and operational processes.\n","\n","The transition from Level 4 to Level 5 isn't about AI sophistication. It's about operational maturity. You're moving from artisanal asset development to industrial-scale production. The same principles apply - governance, testing, documentation, approval - but automated and systematized for higher volume and velocity.\n","\n","**Final Reflections**\n","\n","Level 4 represents a maturity threshold in AI-assisted work. Below this level, you're experimenting with AI as a personal tool. At this level and above, you're building organizational capability. That shift requires different thinking, different standards, and different disciplines.\n","\n","The governance infrastructure might feel heavy initially. All the logging, validation, regression testing, approval workflows, and documentation seem like overhead. But this infrastructure is what separates sustainable value creation from hype cycles that promise transformation but deliver chaos.\n","\n","You now have both the conceptual framework and the practical tools to create AI assets responsibly. You understand what quality looks like, how to measure it, how to maintain it, and how to prove it. You can build assets that earn trust through demonstrated reliability rather than demanding trust through claimed sophistication.\n","\n","The organizations that will genuinely transform their operations with AI won't be those that adopt the most advanced models or create the most impressive demonstrations. They'll be those that systematically build libraries of reliable, well-governed, properly tested assets that solve real problems consistently. You're now equipped to build those organizations.\n","\n","The work continues, but you've crossed an essential threshold. Welcome to Level 4."],"metadata":{"id":"aV65a4QjNzQV"}}]}